{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D to 3D LSTM\n",
    "\n",
    "This is the first approach to try to estimate 3D points coordinates from 2D keypoints extracted with Openpose. Here I will build a simple LSTM to perform the task over the Panoptic Studio dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch utilities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Plotting utilities\n",
    "%matplotlib widget\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from timeit import default_timer as timer\n",
    "import pyprind\n",
    "\n",
    "# Directory and file utilities\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = { 0:'one video', 1:'one signer', 2:'all signers'}\n",
    "MODE =  modes[0]\n",
    "video = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition\n",
    "Now I will define some functions in order to parse and organise the data, and later convert it to pytorch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is structured as follows: in the dataset directory there are several folders, each folder corresponds to a recording; each of these folders contains a folder with the audio, folders with face, hands and body keypoints estimations for each frame, and a folder with the video recorded from different views.\n",
    "\n",
    "In this first approach I will be using the keypoints estimations. Every keypoint folder (face, hands or body) is organized the same way: it contains a json per frame of the video, which includes the 3D keypoints estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_keypoints` will go through each folder in the dataset directory and retrieve the face keypoints, the hands keypoints and the body keypoints. It will separate them into input (2D coordinates per joint per frame) and grountruth (third coordinate to estimate for each input 2D keypoint). \n",
    "The input will be of shape $([n videos, seq len, input size])$, where *seq_len* = number of frames, and *input_size* = face + hands + body keypoints, that is (70+(21+21)+26)x2 -multiplied by 2 because there are x and y coordinates-. The groundtruth (label) data will be of the same shape, except that the last dimension size will not be multiplied by 2 (there's only one coordinate to estimate).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints(data_path):\n",
    "    dataset = []\n",
    "    groundtruth = []\n",
    "    # Look over just the folders inside the directory\n",
    "    just_folders = filter(lambda x: isdir(join(data_path, x)), listdir(data_path))\n",
    "    for p in list(map(lambda x: join(data_path, x), just_folders)): \n",
    "        # Gets 2 list of n_frames lists, one for the 2D coordinates and one for the third coordinate.\n",
    "        # Each list of the n_frames lists contains, either the (x and y) or the z of each keypoint for the face(first line), hands(second), body(third).\n",
    "        # e.g. the first line will result in [[x1,y1,x2,y2...x70,y70]sub1...[x1,y1...x70,y70]subN], [[z1,z2...z70]sub1...[z1..z70]subN]\n",
    "        # Actually, as there will be two of each list above because there are two people en each video.\n",
    "        pose_2d, pose_3d = get_body(p)\n",
    "        \n",
    "        # Concatenates the coordinates for the face, hands and body on the last dimension, for each person.\n",
    "        vid_input_p1, vid_input_p2 = pose_2d\n",
    "        vid_labels_p1, vid_labels_p2 = pose_3d\n",
    "        \n",
    "        dataset.append(vid_input_p1)\n",
    "        dataset.append(vid_input_p2)\n",
    "        groundtruth.append(vid_labels_p1)\n",
    "        groundtruth.append(vid_labels_p2)\n",
    "        print(f'Completed folder {p}')\n",
    "    return dataset, groundtruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The body json is organised a bit differently, inside each person object contains the *joints26* field with a list of 3D coordinates. But this list is structured as follows: *[x1,y1,z1,acc1,x2,y2,z2,acc2...]*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_body(path):\n",
    "    body_2D_seq = ([], [])\n",
    "    body_3D_seq = ([], [])\n",
    "    paths = map(lambda x: join(path, 'hdPose3d_stage1_op25', x), sorted(listdir(join(path, 'hdPose3d_stage1_op25'))))\n",
    "    files = list(filter(lambda x: isfile(x), paths))\n",
    "    for f in files:\n",
    "        with open(f, 'r') as j:\n",
    "            json_array = json.load(j)\n",
    "            i = 0\n",
    "            for person in json_array['bodies']:\n",
    "                if person['id'] != -1:\n",
    "                    x = person['joints26'][::4]\n",
    "                    y = person['joints26'][1::4]\n",
    "                    two_coord = [l[item] for item in range(len(x)) for l in [x,y]]\n",
    "                    third_coord = person['joints26'][2::4]\n",
    "                    body_2D_seq[person['id']].append(two_coord)\n",
    "                    body_3D_seq[person['id']].append(third_coord)\n",
    "                    i += 1\n",
    "                    pid = person['id']\n",
    "            if i<2:\n",
    "                body_2D_seq[1-pid].append(body_2D_seq[1-pid][-1])\n",
    "                body_3D_seq[1-pid].append(body_3D_seq[1-pid][-1])\n",
    "    print('Body completed.')\n",
    "    return body_2D_seq, body_3D_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190419_asl2\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190419_asl4\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190419_asl5\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl1\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl2\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl3\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl5\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl7\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl9\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl91\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl1\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl2\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl3\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl4\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl5\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl6\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl7\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl8\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl910\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl911\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl912\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl913\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../../data/DB keypoints'\n",
    "dataset, groundtruth = get_keypoints(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_seq(dataset):\n",
    "    max_seq = max([len(x) for x in dataset])\n",
    "    seq_lengths = []\n",
    "    for seq in dataset:\n",
    "        seq_lengths.append(len(seq))\n",
    "        for i in range(max_seq-len(seq)):\n",
    "            seq.append([np.nan for j in range(len(seq[0]))])\n",
    "        \n",
    "    return max_seq, seq_lengths\n",
    "\n",
    "max_seq, seq_lengths = padding_seq(dataset)\n",
    "_, _ = padding_seq(groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 8752, 52) (44, 8752, 26) (44,)\n"
     ]
    }
   ],
   "source": [
    "# From python lists to numpy ndarray.\n",
    "dataset = np.asarray(dataset)\n",
    "groundtruth = np.asarray(groundtruth)\n",
    "lengths = np.asarray(seq_lengths)\n",
    "print(dataset.shape, groundtruth.shape, lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../pickles/body_data.npy', dataset)\n",
    "np.save('../../pickles/body_ground.npy', groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../pickles/body_lengths.npy', lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset structuring\n",
    "Now let's convert the lists obtained to Pytorch tensors and organise them in train, validation and test datasets. \n",
    "First, I will define a padding function in order to make all the sequences of video frames the same length, so I can train the LSTM in batches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load from pickle\n",
    "Load keypoints from pre-saved pickle files instead of directly reading the jsons, can be found in below cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once they are numpy ndarrays I save the keypoints into pickle files for faster loading in later executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8752\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load('../../pickles/body_data.npy', allow_pickle=True)\n",
    "groundtruth = np.load('../../pickles/body_ground.npy', allow_pickle=True)\n",
    "lengths = np.load('../../pickles/body_lengths.npy', allow_pickle=True)\n",
    "max_seq = dataset.shape[1]\n",
    "print(max_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8272 8272 8272 8272 8271 8271 8751 8751 5952 5952 7611 7611 6953 6953\n",
      " 6012 6012 8574 8574 5176 5176 8045 8045 7836 7836 7326 7326 7141 7141\n",
      " 8751 8751 5155 5155 8751 8751 8752 8752 8751 8751 8751 8751 8751 8751\n",
      " 7205 7205]\n"
     ]
    }
   ],
   "source": [
    "print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517,)\n"
     ]
    }
   ],
   "source": [
    "if MODE == 'one video':\n",
    "    dataset, groundtruth = dataset[video], groundtruth[video]\n",
    "    chunks_d = np.split(dataset, 547, axis=0)[:lengths[video]//16]\n",
    "    chunks_g = np.split(groundtruth, 547, axis=0)[:lengths[video]//16]\n",
    "    chunks_d = tuple(np.expand_dims(c, axis=0) for c in chunks_d)\n",
    "    chunks_g = tuple(np.expand_dims(c, axis=0) for c in chunks_g)\n",
    "    \n",
    "    dataset = np.concatenate(chunks_d, axis=0)\n",
    "    groundtruth = np.concatenate(chunks_g, axis=0)\n",
    "    lengths = np.asarray([16 for i in range(lengths[video]//16)])\n",
    "    print(lengths.shape)\n",
    "\n",
    "elif MODE == 'one signer':\n",
    "    dataset, groundtruth, lengths = dataset[20::2, :-2], groundtruth[20::2, :-2], lengths[20::2]\n",
    "    print(lengths)\n",
    "    lengths = np.asarray([min(8750, x) for x in lengths])\n",
    "    print(lengths)\n",
    "    chunks_d, chunks_g = np.split(dataset, 35, axis=1), np.split(groundtruth, 35, axis=1)\n",
    "    \n",
    "    dataset, groundtruth = np.concatenate(chunks_d, axis=0), np.concatenate(chunks_g, axis=0)\n",
    "    lengths = np.concatenate(tuple([250 if j*250<=lengths[i] \n",
    "                                    else (lengths[i]%250 if (j-1)*250<lengths[i] \n",
    "                                          else 0) for i in range(12)] for j in range(1,36)), axis=0)\n",
    "    print(dataset.shape, groundtruth.shape, lengths.shape)\n",
    "    \n",
    "elif MODE == 'all signers':\n",
    "    dataset, groundtruth = dataset[:, :-2], groundtruth[:, :-2]\n",
    "    lengths = np.asarray([min(8750, x) for x in lengths])\n",
    "    chunks_d, chunks_g = np.split(dataset, 25, axis=1), np.split(groundtruth, 25, axis=1)\n",
    "    \n",
    "    dataset, groundtruth = np.concatenate(chunks_d, axis=0), np.concatenate(chunks_g, axis=0)\n",
    "    lengths = np.concatenate(tuple([350 if j*350<=lengths[i]\n",
    "                                    else (lengths[i]%350 if (j-1)*350<lengths[i] \n",
    "                                          else 0) for i in range(44)] for j in range(1,26)), axis=0)\n",
    "    print(dataset.shape, groundtruth.shape, lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517, 16, 52) (517, 16, 26) (517,)\n"
     ]
    }
   ],
   "source": [
    "# Clean all NaN videos\n",
    "dataset = np.delete(dataset, np.where(lengths==0), axis=0)\n",
    "groundtruth = np.delete(groundtruth, np.where(lengths==0), axis=0)\n",
    "lengths = np.delete(lengths, np.where(lengths==0), axis=0)\n",
    "\n",
    "print(dataset.shape, groundtruth.shape, lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517, 16, 52) (517, 16, 26) (517,)\n"
     ]
    }
   ],
   "source": [
    "def align(tensor, coordinates=1):\n",
    "    for n_vid in range(tensor.shape[0]):\n",
    "        max_value = [np.nanmax(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        min_value = [np.nanmin(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        center = [(max_value[i]+min_value[i])/2 for i in range(coordinates)]\n",
    "        for j in range(coordinates):\n",
    "            subtensor = tensor[n_vid, :, j::coordinates]\n",
    "            subtensor[:] = np.subtract(subtensor, center[j])\n",
    "\n",
    "align(dataset,2)\n",
    "align(groundtruth)\n",
    "print(dataset.shape, groundtruth.shape, lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517, 16, 52) (517, 16, 26)\n"
     ]
    }
   ],
   "source": [
    "r = R.from_euler('y', 110, degrees=True)\n",
    "shapes = dataset.shape\n",
    "dataset = dataset.reshape(-1, 2)\n",
    "groundtruth = groundtruth.reshape(-1,1)\n",
    "xyz = np.concatenate((dataset, groundtruth), axis=1)\n",
    "xyz = r.apply(xyz)\n",
    "dataset, groundtruth = xyz[:, :2].reshape(shapes), xyz[:,2].reshape(shapes[0], shapes[1],26)\n",
    "\n",
    "print(dataset.shape, groundtruth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517, 2)\n"
     ]
    }
   ],
   "source": [
    "def norm_uniform(tensor, coordinates=1, factor=None):\n",
    "    scale = []\n",
    "    mean_ranges = []\n",
    "    for n_vid in range(tensor.shape[0]):\n",
    "        coord_scale = []\n",
    "        max_value = [np.nanmax(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        min_value = [np.nanmin(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        center = [(max_value[i]+min_value[i])/2 for i in range(coordinates)]\n",
    "        ranges = np.ndarray((tensor.shape[1],coordinates))\n",
    "        for n_frame in range(tensor.shape[1]):\n",
    "            rang = [np.nanmax(tensor[n_vid, n_frame,i::coordinates])-np.nanmin(tensor[n_vid, n_frame,i::coordinates]) for i in range(coordinates)]\n",
    "            ranges[n_frame] = np.asarray(rang)\n",
    "        mean_range = [np.nanmean(ranges[:,i]) for i in range(coordinates)]\n",
    "        for j in range(coordinates):\n",
    "            subtensor = tensor[n_vid, :, j::coordinates]\n",
    "            subtensor[:] = np.subtract(subtensor, center[j])\n",
    "            if factor is not None:\n",
    "                subtensor[:] = np.divide(subtensor, factor[n_vid])\n",
    "            else:\n",
    "                subtensor[:] = np.divide(subtensor, max_value[j]-center[j])\n",
    "            coord_scale.append((max_value[j]-center[j] if factor is None else factor[n_vid]))\n",
    "        scale.append(coord_scale)\n",
    "        mean_ranges.append(mean_range)\n",
    "    return mean_ranges\n",
    "input_scale = np.asarray(norm_uniform(dataset,2)).squeeze()\n",
    "print(input_scale.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517, 2) (517, 2)\n"
     ]
    }
   ],
   "source": [
    "def normalize(tensor, coordinates=1, std=None):\n",
    "    moments = []\n",
    "    std_centroids = []\n",
    "    for n_vid in range(tensor.shape[0]):\n",
    "        coord_moments = []\n",
    "        mean_value = [np.nanmean(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        std_value = [np.nanstd(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        centroids = np.ndarray((tensor.shape[1],coordinates))\n",
    "        for n_frame in range(tensor.shape[1]):\n",
    "            centroid = [np.nanmean(tensor[n_vid, n_frame, i::coordinates]) for i in range(coordinates)]\n",
    "            centroids[n_frame] = np.asarray(centroid)\n",
    "        std_centroid = [np.nanstd(centroids[:,i]) for i in range(coordinates)]\n",
    "        if std is not None:\n",
    "            std_value = [std[n_vid]]\n",
    "        for j in range(coordinates):\n",
    "            subtensor = tensor[:, :, j::coordinates]\n",
    "            subtensor[:] = np.subtract(subtensor, mean_value[j])\n",
    "            subtensor[:] = np.divide(subtensor, std_value[j])\n",
    "            coord_moments.append((mean_value[j], std_value[j]))\n",
    "        moments.append(coord_moments)\n",
    "        std_centroids.append(std_centroid)\n",
    "    return moments, std_centroids\n",
    "\n",
    "moments, std_centroids = normalize(dataset, 2)\n",
    "mom_x, mom_y = np.asarray(moments)[:, 0], np.asarray(moments)[:, 1]\n",
    "std_centroids = np.asarray(std_centroids)\n",
    "print(mom_x.shape, mom_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517, 16, 52) (517, 16, 26) (517,)\n",
      "(517, 2) (517, 2) (517, 2)\n"
     ]
    }
   ],
   "source": [
    "# Randomly shuffle videos\n",
    "permutation = np.random.permutation(dataset.shape[0])\n",
    "dataset, groundtruth, lengths = dataset[permutation], groundtruth[permutation], lengths[permutation]\n",
    "input_scale, mom_x, mom_y = input_scale[permutation], mom_x[permutation], mom_y[permutation]\n",
    "std_centroids = std_centroids[permutation]\n",
    "\n",
    "print(dataset.shape, groundtruth.shape, lengths.shape)\n",
    "print(input_scale.shape, mom_x.shape, mom_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(414, 16, 52) (51, 16, 52) (52, 16, 52)\n",
      "(414, 16, 26) (51, 16, 26) (52, 16, 26)\n",
      "(414,) (51,) (52,)\n"
     ]
    }
   ],
   "source": [
    "l1, l2 = len(dataset), len(groundtruth)\n",
    "p1, p2 = 0.8, 0.9\n",
    "# Split in train, validation and test\n",
    "training_kp, val_kp, test_kp = dataset[:round(p1*l1)], dataset[round(p1*l1):round(p2*l1)], dataset[round(p2*l1):]\n",
    "training_lbl, val_lbl, test_lbl = groundtruth[:round(p1*l2)], groundtruth[round(p1*l2):round(p2*l2)], groundtruth[round(p2*l2):]\n",
    "training_lengths, val_lengths, test_lengths = lengths[:round(p1*l1)], lengths[round(p1*l1):round(p2*l1)], lengths[round(p2*l1):]\n",
    "training_inpscale, val_inpscale, test_inpscale = input_scale[:round(p1*l1)], input_scale[round(p1*l1):round(p2*l1)], input_scale[round(p2*l1):]\n",
    "training_mom_x, val_mom_x, test_mom_x = mom_x[:round(p1*l1)], mom_x[round(p1*l1):round(p2*l1)], mom_x[round(p2*l1):]\n",
    "training_mom_y, val_mom_y, test_mom_y = mom_y[:round(p1*l1)], mom_y[round(p1*l1):round(p2*l1)], mom_y[round(p2*l1):]\n",
    "\n",
    "training_std_centroids, val_std_centroids, test_std_centroids = std_centroids[:round(p1*l1)], std_centroids[round(p1*l1):round(p2*l1)], std_centroids[round(p2*l1):]\n",
    "\n",
    "print(training_kp.shape, val_kp.shape, test_kp.shape)\n",
    "print(training_lbl.shape, val_lbl.shape, test_lbl.shape)\n",
    "print(training_lengths.shape, val_lengths.shape, test_lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(414,) (51,)\n",
      "(414,) (51,)\n"
     ]
    }
   ],
   "source": [
    "training_outscale = np.asarray(norm_uniform(training_lbl)).squeeze()\n",
    "val_outscale_t = np.asarray(norm_uniform(val_lbl)).squeeze()\n",
    "test_outscale_t = np.asarray(norm_uniform(test_lbl)).squeeze()\n",
    "print(training_outscale.shape, val_outscale_t.shape)\n",
    "\n",
    "training_mom_z = np.asarray(normalize(training_lbl)[0])[:, 0, 1]\n",
    "val_mom_z_t = np.asarray(normalize(val_lbl)[0])[:, 0, 1]\n",
    "test_mom_z_t = np.asarray(normalize(test_lbl)[0])[:, 0, 1]\n",
    "print(training_mom_z.shape, val_mom_z_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51,) (52,)\n",
      "(51,) (52,)\n"
     ]
    }
   ],
   "source": [
    "alpha = LinearRegression(fit_intercept=True)#make_pipeline(PolynomialFeatures(9), LinearRegression(fit_intercept=True))\n",
    "alpha.fit(training_inpscale, training_outscale)\n",
    "val_outscale = alpha.predict(val_inpscale)\n",
    "test_outscale = alpha.predict(test_inpscale)\n",
    "print(val_outscale.shape, test_outscale.shape)\n",
    "\n",
    "sigma = LinearRegression(fit_intercept=True)#make_pipeline(PolynomialFeatures(9), LinearRegression(fit_intercept=True))\n",
    "train_X = np.concatenate((training_mom_x[:,1, np.newaxis], training_mom_y[:,1, np.newaxis]), axis=1)\n",
    "sigma.fit(train_X, training_mom_z)\n",
    "val_X = np.concatenate((val_mom_x[:,1,np.newaxis], val_mom_y[:,1,np.newaxis]), axis=1)\n",
    "test_X = np.concatenate((test_mom_x[:,1,np.newaxis], test_mom_y[:,1,np.newaxis]), axis=1)\n",
    "val_mom_z = sigma.predict(val_X)\n",
    "test_mom_z = sigma.predict(test_X)\n",
    "print(val_mom_z.shape, test_mom_z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84661e8a269f48288f7d2a0598fde9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7efcd3146a10>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(training_inpscale[:,0], training_inpscale[:,1], training_outscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abf1fbc1e5043a99017c7e1b2510701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f84c582b4d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(training_std_centroids[:,0], training_std_centroids[:,1], training_mom_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d83a69d2ea46c99008a1848fc0a8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f84c56c7d90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(training_std_centroids[:,0], training_mom_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc6da98ef7a4ef3ad306b1086c6b678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f84c563f750>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(training_std_centroids[:,1], training_mom_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29.80188876 30.87737658 27.73357044 29.79071392 36.67716098 25.8405184\n",
      " 32.00619114 35.16466133 32.90460962 22.75207336 29.94964563 30.19744454\n",
      " 33.34520212 37.32608939 28.76491192 29.04352957 23.2870003  30.07300141\n",
      " 30.83953629 28.77152732 27.41993708 24.42374717 31.04599363 27.33953214\n",
      " 23.28470164 28.87252261 31.5200786  28.14355776 31.9769403  25.35102135\n",
      " 32.97517944 30.31193222 35.06591523 30.47254267 28.90978656 29.93571114\n",
      " 32.29299605 35.32736569 34.23340654 30.08737681 31.02673421 25.09057892\n",
      " 26.59544178 35.58875244 31.1194973  29.16558719 30.30690271 34.82035643\n",
      " 33.05286596 39.71442627 31.16105294] [37.28179001 24.56885892 32.90954753 22.72342256 43.20176538 35.87970353\n",
      " 29.10572711 30.38360922 32.10740787 24.90781529 27.68248156 28.99905634\n",
      " 32.15746513 32.57179806 29.62720424 23.49983232 34.80197463 28.56092208\n",
      " 40.91188764 35.08291809 26.10495045 28.3373283  28.81819214 35.58789506\n",
      " 29.989667   32.36643809 32.32597531 35.28926446 32.96732646 24.31549686\n",
      " 22.97319613 27.65068086 32.62329355 32.53466377 28.00264018 28.60944349\n",
      " 28.57541745 31.68571532 31.01312852 27.67487235 28.90213426 24.84159973\n",
      " 29.58546912 30.03992791 28.4759499  32.47803692 23.75034549 23.21076516\n",
      " 33.45635872 28.61778427 37.5550102  30.97467861]\n",
      "[0.471508   1.08917452 0.87297475 1.0830484  0.94551695 1.07739698\n",
      " 0.89143897 1.15023447 0.98587676 1.1059704  0.90407585 1.00014255\n",
      " 1.01377224 0.98851762 0.89874761 0.86020901 1.27622851 0.98987056\n",
      " 1.02942957 1.05047942 0.82681715 1.20082028 0.91840761 1.00035678\n",
      " 1.17638744 0.88346776 0.96778271 0.98029788 1.03137974 0.94735513\n",
      " 1.04321176 0.98841957 1.03578011 1.02200875 0.90366718 1.0595811\n",
      " 0.97008529 1.00396451 1.05562919 0.89857759 1.07488827 0.97980412\n",
      " 1.13839252 0.79616054 1.2334133  0.95224662 1.1117696  0.88105638\n",
      " 1.02853214 0.97601296 1.01602774] [0.50506673 0.87353688 1.08416693 1.13863553 0.83460747 0.92036923\n",
      " 1.21400189 0.91048725 1.06028537 1.05182011 1.0196664  0.74250969\n",
      " 1.2450897  0.96762758 1.17999142 0.9673408  0.91173145 0.99089782\n",
      " 0.90652377 1.14921251 0.92440363 1.08713419 0.99313493 0.95659103\n",
      " 1.04857322 0.93460669 1.07869888 0.95961293 0.90786773 1.17182836\n",
      " 1.0790981  0.96161025 0.88481491 0.94402749 1.1044022  0.93507938\n",
      " 0.98187324 0.98013297 1.08722927 1.13132453 0.83476094 1.11020955\n",
      " 0.94631687 1.10837865 0.85806963 1.01934004 1.13055486 1.02620844\n",
      " 0.77781449 1.14001767 0.93908076 1.01754526]\n",
      "12.022763305401709 17.392523391086993\n",
      "0.015475763935459472 0.017379872876527737\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(val_outscale_t, test_outscale_t)\n",
    "print(val_mom_z_t, test_mom_z_t)\n",
    "print(mean_squared_error(val_outscale, val_outscale_t), mean_squared_error(test_outscale, test_outscale_t))\n",
    "print(mean_squared_error(val_mom_z, val_mom_z_t), mean_squared_error(test_mom_z, test_mom_z_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 2) (52, 2)\n"
     ]
    }
   ],
   "source": [
    "val_outscale = np.concatenate((val_outscale[:,np.newaxis], val_outscale_t[:,np.newaxis]), axis=1)\n",
    "test_outscale = np.concatenate((test_outscale[:,np.newaxis], test_outscale_t[:,np.newaxis]), axis=1)\n",
    "\n",
    "val_mom_z = np.concatenate((val_mom_z[:,np.newaxis], val_mom_z_t[:,np.newaxis]), axis=1)\n",
    "test_mom_z = np.concatenate((test_mom_z[:,np.newaxis], test_mom_z_t[:,np.newaxis]), axis=1)\n",
    "print(val_outscale.shape, test_mom_z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([414, 16, 52]) torch.Size([51, 16, 26]) torch.Size([52])\n",
      "torch.Size([414, 2]) torch.Size([414])\n"
     ]
    }
   ],
   "source": [
    "# From python lists to pytorch tensors.\n",
    "training_kp, val_kp, test_kp = torch.tensor(np.nan_to_num(training_kp), dtype=torch.float32), torch.tensor(np.nan_to_num(val_kp), dtype=torch.float32), torch.tensor(np.nan_to_num(test_kp), dtype=torch.float32)\n",
    "training_lbl, val_lbl, test_lbl = torch.tensor(np.nan_to_num(training_lbl), dtype=torch.float32), torch.tensor(np.nan_to_num(val_lbl), dtype=torch.float32), torch.tensor(np.nan_to_num(test_lbl), dtype=torch.float32)\n",
    "training_lengths, val_lengths, test_lengths = torch.tensor(np.nan_to_num(training_lengths), dtype=torch.float32), torch.tensor(np.nan_to_num(val_lengths), dtype=torch.float32), torch.tensor(np.nan_to_num(test_lengths), dtype=torch.float32)\n",
    "training_inpscale, val_inpscale, test_inpscale = torch.tensor(np.nan_to_num(training_inpscale), dtype=torch.float32), torch.tensor(np.nan_to_num(val_inpscale), dtype=torch.float32), torch.tensor(np.nan_to_num(test_inpscale), dtype=torch.float32)\n",
    "training_outscale, val_outscale, test_outscale = torch.tensor(np.nan_to_num(training_outscale), dtype=torch.float32), torch.tensor(np.nan_to_num(val_outscale), dtype=torch.float32), torch.tensor(np.nan_to_num(test_outscale), dtype=torch.float32)\n",
    "training_mom_x, val_mom_x, test_mom_x = torch.tensor(np.nan_to_num(training_mom_x), dtype=torch.float32), torch.tensor(np.nan_to_num(val_mom_x), dtype=torch.float32), torch.tensor(np.nan_to_num(test_mom_x), dtype=torch.float32)\n",
    "training_mom_y, val_mom_y, test_mom_y = torch.tensor(np.nan_to_num(training_mom_y), dtype=torch.float32), torch.tensor(np.nan_to_num(val_mom_y), dtype=torch.float32), torch.tensor(np.nan_to_num(test_mom_y), dtype=torch.float32)\n",
    "training_mom_z, val_mom_z, test_mom_z = torch.tensor(np.nan_to_num(training_mom_z), dtype=torch.float32), torch.tensor(np.nan_to_num(val_mom_z), dtype=torch.float32), torch.tensor(np.nan_to_num(test_mom_z), dtype=torch.float32)\n",
    "\n",
    "print(training_kp.shape, val_lbl.shape, test_lengths.shape)\n",
    "print(training_inpscale.shape, training_outscale.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we define the batch_size and put the datasets in DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7efcd302d050>\n"
     ]
    }
   ],
   "source": [
    "train_data = TensorDataset(training_kp, training_lbl, training_lengths,\n",
    "                          training_inpscale, training_outscale,\n",
    "                          training_mom_x, training_mom_y, training_mom_z)\n",
    "val_data = TensorDataset(val_kp, val_lbl, val_lengths,\n",
    "                        val_inpscale, val_outscale,\n",
    "                        val_mom_x, val_mom_y, val_mom_z)\n",
    "test_data = TensorDataset(test_kp, test_lbl, test_lengths,\n",
    "                         test_inpscale, test_outscale,\n",
    "                         test_mom_x, test_mom_y, test_mom_z)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a GPU available we set our device to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print some examples to see whether it is loaded correctly or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16, 52]) torch.Size([32, 16, 26]) torch.Size([32]) torch.Size([32, 2]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y, sample_len, iscale, oscale, momx, momy, momz = dataiter.next()\n",
    "\n",
    "print(sample_x.shape, sample_y.shape, sample_len.shape, iscale.shape, momz.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building\n",
    "It is time to build the model for this approach. It will consist on a single/double layer LSTM followed by a Linear layer with output size the number of keypoints we want to estimate. I also define a method to initialize the hidden_state of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_2D3D(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, bidirectional, dropout=0.):\n",
    "        super().__init__()\n",
    "        # Save the model parameters\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bi = bidirectional\n",
    "        \n",
    "        # Define the architecture\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*(2 if self.bi else 1), output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, state, lengths):\n",
    "        # Describe the forward step\n",
    "        batch_size, seq_len = x.size(0), x.size(1) # We save the batch size and the (maximum) sequence length\n",
    "        \n",
    "        # Need to pack a tensor containing padded sequences of variable length\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths=lengths, batch_first=True, enforce_sorted=False)\n",
    "        ht, hidden_state = self.lstm(packed, state) # ht will be a PackedSequence\n",
    "\n",
    "        # Need to flatten and reshape the output to feed it to the Linear layer\n",
    "        ht = ht.data.contiguous() # ht will be of shape [sum(lengths), hidden_dim]\n",
    "        ot = self.fc(ht) # ot will be of shape [sum(lengths), ouput_size]\n",
    "\n",
    "        l_ot = [ot[:int(length)] for length in lengths] # list of batch elements, each shape [lengths[i], output_size]\n",
    "        packed_ot = nn.utils.rnn.pack_sequence(l_ot, enforce_sorted=False) # PackedSequence\n",
    "        # Finally return to shape [batch_size, seq_len, output_size]\n",
    "        ot, _ = nn.utils.rnn.pad_packed_sequence(packed_ot, batch_first=True, total_length=seq_len)\n",
    "        \n",
    "        return ot, hidden_state\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers*(2 if self.bi else 1), batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers*(2 if self.bi else 1), batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_2D3D(\n",
      "  (lstm): LSTM(52, 512, num_layers=2, batch_first=True)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=26, bias=True)\n",
      "  )\n",
      ")\n",
      "3273754\n"
     ]
    }
   ],
   "source": [
    "# Define some model parameters\n",
    "INPUT_SIZE = sample_x.size(2)\n",
    "OUTPUT_SIZE = sample_y.size(2)\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = False\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTM_2D3D(INPUT_SIZE, OUTPUT_SIZE, HIDDEN_DIM, N_LAYERS, BIDIRECTIONAL, dropout=0.)\n",
    "model.to(device)\n",
    "print(model)\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now we will proceed with the training. The first cell will define the learning rate, the loss function and the selected optimizer for the training process. Then we will proceed with a training over a number of epochs in which we will print it's training loss and validation loss. I also will be using Tensorboard to have a much nicer view of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substract_root_PJPE(output):\n",
    "    jb = torch.chunk(output, max_seq, dim=1)\n",
    "    root = 8\n",
    "    n_joints = []\n",
    "    for chunk in jb:\n",
    "        n_joints.append(chunk.sub(chunk[:,:,root].unsqueeze(2)))\n",
    "    joints_merged = torch.cat(tuple(n_joints), dim=1)\n",
    "    return joints_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpjpe(rooted_o, rooted_l, seq_lens):\n",
    "    MPJPE = []\n",
    "    for i in range(len(seq_lens)):\n",
    "        MPJPE.append(rooted_o[i,:int(seq_lens[i])].sub(rooted_l[i,:int(seq_lens[i])]).abs().mean().item())\n",
    "    \n",
    "    return np.mean(MPJPE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_keypoints(output, labels, seq_lens, alpha=0.1):\n",
    "    total_correct = 0\n",
    "    for i in range(len(seq_lens)):\n",
    "        for j in range(int(seq_lens[i])):\n",
    "            rang = (labels[i,j].max()-labels[i,j].min())\n",
    "            for o, l in zip(output[i,j], labels[i,j]):\n",
    "                if o < l+alpha*rang and o > l-alpha*rang:\n",
    "                    total_correct += 1\n",
    "    return total_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 30\n",
    "lr = 1e-3\n",
    "loss_function = nn.MSELoss()\n",
    "one_cycle = True\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.)\n",
    "if one_cycle:\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, \n",
    "                                                  steps_per_epoch=len(train_loader), epochs=NUM_EPOCHS,\n",
    "                                                  div_factor=25.0, final_div_factor=10000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "name = 'body_whole'\n",
    "writer = SummaryWriter(log_dir=f'/deeplearning/logs/{name}{datetime.now()}_lr-{lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 1/30 in 16.47s.\n",
      " Loss: 1.1332  Val Loss: 1.0023\n",
      " Train MPJPE: 1.1678  Val MPJPE: 1.1187\n",
      " Train PCK: 23.277  Val PCK: 22.641\n",
      "Finished epoch 2/30 in 16.45s.\n",
      " Loss: 0.9704  Val Loss: 0.9434\n",
      " Train MPJPE: 1.0474  Val MPJPE: 1.0675\n",
      " Train PCK: 25.444  Val PCK: 24.166\n",
      "Finished epoch 3/30 in 17.52s.\n",
      " Loss: 0.4973  Val Loss: 0.8380\n",
      " Train MPJPE: 0.6641  Val MPJPE: 1.0126\n",
      " Train PCK: 43.865  Val PCK: 26.307\n",
      "Finished epoch 4/30 in 17.61s.\n",
      " Loss: 0.3771  Val Loss: 0.7761\n",
      " Train MPJPE: 0.5011  Val MPJPE: 0.9329\n",
      " Train PCK: 47.119  Val PCK: 27.201\n",
      "Finished epoch 5/30 in 17.82s.\n",
      " Loss: 0.3418  Val Loss: 0.6347\n",
      " Train MPJPE: 0.4147  Val MPJPE: 0.8194\n",
      " Train PCK: 53.208  Val PCK: 30.138\n",
      "Finished epoch 6/30 in 17.69s.\n",
      " Loss: 0.3267  Val Loss: 0.4851\n",
      " Train MPJPE: 0.3959  Val MPJPE: 0.6485\n",
      " Train PCK: 52.173  Val PCK: 35.074\n",
      "Finished epoch 7/30 in 17.97s.\n",
      " Loss: 0.3128  Val Loss: 0.3488\n",
      " Train MPJPE: 0.3860  Val MPJPE: 0.4803\n",
      " Train PCK: 53.119  Val PCK: 41.339\n",
      "Finished epoch 8/30 in 17.86s.\n",
      " Loss: 0.2940  Val Loss: 0.2571\n",
      " Train MPJPE: 0.3641  Val MPJPE: 0.3122\n",
      " Train PCK: 54.328  Val PCK: 50.473\n",
      "Finished epoch 9/30 in 17.87s.\n",
      " Loss: 0.2872  Val Loss: 0.2582\n",
      " Train MPJPE: 0.3522  Val MPJPE: 0.3082\n",
      " Train PCK: 54.758  Val PCK: 48.821\n",
      "Finished epoch 10/30 in 17.98s.\n",
      " Loss: 0.2819  Val Loss: 0.2752\n",
      " Train MPJPE: 0.3471  Val MPJPE: 0.3259\n",
      " Train PCK: 55.678  Val PCK: 52.171\n",
      "Finished epoch 11/30 in 18.00s.\n",
      " Loss: 0.2784  Val Loss: 0.2750\n",
      " Train MPJPE: 0.3468  Val MPJPE: 0.3286\n",
      " Train PCK: 55.335  Val PCK: 51.983\n",
      "Finished epoch 12/30 in 17.86s.\n",
      " Loss: 0.2825  Val Loss: 0.2643\n",
      " Train MPJPE: 0.3455  Val MPJPE: 0.3043\n",
      " Train PCK: 55.793  Val PCK: 50.105\n",
      "Finished epoch 13/30 in 17.90s.\n",
      " Loss: 0.2830  Val Loss: 0.2617\n",
      " Train MPJPE: 0.3461  Val MPJPE: 0.3043\n",
      " Train PCK: 55.174  Val PCK: 52.554\n",
      "Finished epoch 14/30 in 17.75s.\n",
      " Loss: 0.2842  Val Loss: 0.2800\n",
      " Train MPJPE: 0.3503  Val MPJPE: 0.3263\n",
      " Train PCK: 55.756  Val PCK: 50.090\n",
      "Finished epoch 15/30 in 17.85s.\n",
      " Loss: 0.2868  Val Loss: 0.3007\n",
      " Train MPJPE: 0.3480  Val MPJPE: 0.3431\n",
      " Train PCK: 55.318  Val PCK: 48.280\n",
      "Finished epoch 16/30 in 18.05s.\n",
      " Loss: 0.2842  Val Loss: 0.2699\n",
      " Train MPJPE: 0.3501  Val MPJPE: 0.2809\n",
      " Train PCK: 56.104  Val PCK: 49.219\n",
      "Finished epoch 17/30 in 18.10s.\n",
      " Loss: 0.2866  Val Loss: 0.2730\n",
      " Train MPJPE: 0.3448  Val MPJPE: 0.3046\n",
      " Train PCK: 53.782  Val PCK: 52.787\n",
      "Finished epoch 18/30 in 17.92s.\n",
      " Loss: 0.2827  Val Loss: 0.3121\n",
      " Train MPJPE: 0.3488  Val MPJPE: 0.3474\n",
      " Train PCK: 55.694  Val PCK: 49.023\n",
      "Finished epoch 19/30 in 18.12s.\n",
      " Loss: 0.2848  Val Loss: 0.2283\n",
      " Train MPJPE: 0.3470  Val MPJPE: 0.2747\n",
      " Train PCK: 54.654  Val PCK: 51.878\n",
      "Finished epoch 20/30 in 17.83s.\n",
      " Loss: 0.2867  Val Loss: 0.2857\n",
      " Train MPJPE: 0.3522  Val MPJPE: 0.3360\n",
      " Train PCK: 54.791  Val PCK: 51.908\n",
      "Finished epoch 21/30 in 18.09s.\n",
      " Loss: 0.2830  Val Loss: 0.2820\n",
      " Train MPJPE: 0.3449  Val MPJPE: 0.3170\n",
      " Train PCK: 55.374  Val PCK: 50.563\n",
      "Finished epoch 22/30 in 18.01s.\n",
      " Loss: 0.2800  Val Loss: 0.2762\n",
      " Train MPJPE: 0.3436  Val MPJPE: 0.3223\n",
      " Train PCK: 55.652  Val PCK: 50.744\n",
      "Finished epoch 23/30 in 17.93s.\n",
      " Loss: 0.2824  Val Loss: 0.2509\n",
      " Train MPJPE: 0.3467  Val MPJPE: 0.3121\n",
      " Train PCK: 55.311  Val PCK: 52.254\n",
      "Finished epoch 24/30 in 18.04s.\n",
      " Loss: 0.2802  Val Loss: 0.2815\n",
      " Train MPJPE: 0.3443  Val MPJPE: 0.3314\n",
      " Train PCK: 55.545  Val PCK: 55.011\n",
      "Finished epoch 25/30 in 17.98s.\n",
      " Loss: 0.2758  Val Loss: 0.2934\n",
      " Train MPJPE: 0.3402  Val MPJPE: 0.3181\n",
      " Train PCK: 55.745  Val PCK: 48.362\n",
      "Finished epoch 26/30 in 17.99s.\n",
      " Loss: 0.2731  Val Loss: 0.2869\n",
      " Train MPJPE: 0.3421  Val MPJPE: 0.3167\n",
      " Train PCK: 55.941  Val PCK: 49.947\n",
      "Finished epoch 27/30 in 17.96s.\n",
      " Loss: 0.2845  Val Loss: 0.2717\n",
      " Train MPJPE: 0.3446  Val MPJPE: 0.3092\n",
      " Train PCK: 55.213  Val PCK: 52.111\n",
      "Finished epoch 28/30 in 17.96s.\n",
      " Loss: 0.2759  Val Loss: 0.2871\n",
      " Train MPJPE: 0.3421  Val MPJPE: 0.3261\n",
      " Train PCK: 55.875  Val PCK: 47.476\n",
      "Finished epoch 29/30 in 18.01s.\n",
      " Loss: 0.2773  Val Loss: 0.2620\n",
      " Train MPJPE: 0.3406  Val MPJPE: 0.3071\n",
      " Train PCK: 55.707  Val PCK: 51.382\n",
      "Finished epoch 30/30 in 18.01s.\n",
      " Loss: 0.2795  Val Loss: 0.2699\n",
      " Train MPJPE: 0.3432  Val MPJPE: 0.3166\n",
      " Train PCK: 55.349  Val PCK: 49.241\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d134dace81455a8d6d50bc6df4ac11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7efca904ef90>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer_beg = timer()\n",
    "\n",
    "tr_losses = []\n",
    "val_losses = []\n",
    "\n",
    "model.train()\n",
    "for i in range(NUM_EPOCHS):\n",
    "    # Init the hidden state (ht, ct)\n",
    "    h = model.init_hidden(batch_size)\n",
    "    batch_losses = []\n",
    "    train_MPJPE = []\n",
    "    train_CK = [0, 0]\n",
    "    \n",
    "    if i+1 == NUM_EPOCHS:\n",
    "        preds, inps, labls, lens = [], [], [], []\n",
    "        val_preds, val_inps, val_labls, val_lens = [], [], [], []\n",
    "        iscale, oscale, val_iscale, val_oscale = [], [], [], []\n",
    "        momx, momy, momz = [], [], []\n",
    "        val_momx, val_momy, val_momz = [], [], []\n",
    "        \n",
    "    for inputs, labels, lengths, i_s, o_s, mx, my, mz in train_loader:\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels, lengths = inputs.to(device), labels.to(device), lengths.to(device)\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward step\n",
    "        output, h = model(inputs, h, lengths)\n",
    "        \n",
    "        if i+1 == NUM_EPOCHS:\n",
    "            e = [preds, inps, labls, lens, iscale, oscale, momx, momy, momz]\n",
    "            b = [output, inputs, labels, lengths, i_s, o_s, mx, my, mz]\n",
    "            for k in range(len(e)):\n",
    "                e[k].append(b[k])            \n",
    "\n",
    "        # Loss calculation and backward step\n",
    "        loss = loss_function(nn.utils.rnn.pack_padded_sequence(output, lengths=lengths, batch_first=True, \n",
    "                                                               enforce_sorted=False).data,\n",
    "                             nn.utils.rnn.pack_padded_sequence(labels, lengths=lengths, batch_first=True,\n",
    "                                                               enforce_sorted=False).data)\n",
    "        loss.backward()\n",
    "        # Weight update\n",
    "        optimizer.step()\n",
    "        # One cycle policy step\n",
    "        if one_cycle:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Output data collection for showing\n",
    "        batch_losses.append(loss.item())\n",
    "        rooted_o, rooted_l = substract_root_PJPE(output), substract_root_PJPE(labels)\n",
    "        train_MPJPE.append(mpjpe(rooted_o, rooted_l, lengths))\n",
    "        \n",
    "        train_CK[0] += correct_keypoints(output, labels, lengths)\n",
    "        train_CK[1] += lengths.sum()*labels.shape[2]\n",
    "    \n",
    "    timer_end = timer()\n",
    "    tr_losses.append(np.mean(batch_losses))\n",
    "    writer.add_scalar('Loss/train', tr_losses[-1], i)   \n",
    "    train_MPJPE_total = np.mean(train_MPJPE)\n",
    "    train_PCK = train_CK[0]/train_CK[1]\n",
    "    \n",
    "    # Validation at the end of an epoch\n",
    "    val_h = model.init_hidden(batch_size)\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    val_MPJPE = []\n",
    "    val_CK = [0,0]\n",
    "    \n",
    "    for inp, lab, lns, vis, vos, vmx, vmy, vmz in val_loader:\n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "        inp, lab, lns = inp.to(device), lab.to(device), lns.to(device)\n",
    "        out, val_h = model(inp, val_h, lns)\n",
    "        \n",
    "        if i+1 == NUM_EPOCHS:\n",
    "            e = [val_preds, val_inps, val_labls, val_lens, val_iscale, val_oscale, val_momx, val_momy, val_momz]\n",
    "            b = [out, inp, lab, lns, vis, vos, vmx, vmy, vmz]\n",
    "            for k in range(len(e)):\n",
    "                e[k].append(b[k])  \n",
    "        \n",
    "        loss = loss_function(nn.utils.rnn.pack_padded_sequence(out, lengths=lns, batch_first=True,\n",
    "                                                               enforce_sorted=False).data,\n",
    "                             nn.utils.rnn.pack_padded_sequence(lab, lengths=lns, batch_first=True,\n",
    "                                                               enforce_sorted=False).data)\n",
    "        val_loss.append(loss.item())\n",
    "        rooted_o, rooted_l = substract_root_PJPE(out), substract_root_PJPE(lab)\n",
    "        val_MPJPE.append(mpjpe(rooted_o, rooted_l, lns))\n",
    "        val_CK[0] += correct_keypoints(out, lab, lns)\n",
    "        val_CK[1] += lns.sum()*lab.shape[2]\n",
    "        \n",
    "    val_losses.append(np.mean(val_loss))\n",
    "    writer.add_scalar('Loss/validation', val_losses[-1], i)  \n",
    "    val_MPJPE_total = np.mean(val_MPJPE)\n",
    "    val_PCK = val_CK[0]/val_CK[1] \n",
    "    model.train()\n",
    "    \n",
    "    # Output loss and training time.\n",
    "    print(f\"Finished epoch {i+1}/{NUM_EPOCHS} in {(timer_end-timer_beg):.2f}s.\\n\",\n",
    "             f\"Loss: {np.mean(tr_losses[-1]):.4f}\", f\" Val Loss: {val_losses[-1]:.4f}\\n\",\n",
    "             f\"Train MPJPE: {train_MPJPE_total:.4f}\", f\" Val MPJPE: {val_MPJPE_total:.4f}\\n\",\n",
    "             f\"Train PCK: {train_PCK*100:.3f}\", f\" Val PCK: {val_PCK*100:.3f}\")\n",
    "    timer_beg = timer()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(tr_losses, label='train')\n",
    "plt.plot(val_losses, label='validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSELoss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the predictions for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([384, 2]) torch.Size([384, 16, 52])\n"
     ]
    }
   ],
   "source": [
    "tr_predictions = torch.cat(tuple(preds), dim=0)\n",
    "tr_inputs = torch.cat(tuple(inps), dim=0)\n",
    "tr_groundtruth = torch.cat(tuple(labls), dim=0)\n",
    "tr_lengths = torch.cat(tuple(lens), dim=0)\n",
    "tr_inp_scale, tr_out_scale = torch.cat(tuple(iscale), dim=0), torch.cat(tuple(oscale), dim=0)\n",
    "tr_mx, tr_my, tr_mz = torch.cat(tuple(momx), dim=0), torch.cat(tuple(momy), dim=0), torch.cat(tuple(momz), dim=0)\n",
    "print(tr_inp_scale.shape, tr_inputs.shape)\n",
    "\n",
    "val_predictions = torch.cat(tuple(val_preds), dim=0)\n",
    "val_inputs = torch.cat(tuple(val_inps), dim=0)\n",
    "val_groundtruth = torch.cat(tuple(val_labls), dim=0)\n",
    "val_length = torch.cat(tuple(val_lens), dim=0)\n",
    "val_inp_scale, val_out_scale = torch.cat(tuple(val_iscale), dim=0), torch.cat(tuple(val_oscale), dim=0)\n",
    "val_mx, val_my, val_mz = torch.cat(tuple(val_momx), dim=0), torch.cat(tuple(val_momy), dim=0), torch.cat(tuple(val_momz), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = torch.zeros((138), device='cuda:0')\n",
    "\n",
    "for i in range(len(tr_predictions)):\n",
    "    count=0\n",
    "    for frame in tr_predictions[i, :int(tr_lengths[i])]:\n",
    "        if torch.all(frame.eq(zeros)):\n",
    "            count +=1\n",
    "    print(f'{count}/{int(tr_lengths[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = torch.zeros((138), device='cuda:0')\n",
    "\n",
    "for i in range(len(val_predictions)):\n",
    "    count=0\n",
    "    for frame in tr_predictions[i, :int(val_length[i])]:\n",
    "        if torch.all(frame.eq(zeros)):\n",
    "            count +=1\n",
    "    print(f'{count}/{int(val_length[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'./{name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'./{name}.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "After the training, we shall proceed with the performance test. This will go through the test batches and perform the inference, then it will show the test loss, as well as the performance metric. In this case, as we are working with human body keypoints, we will use the Mean Per Joint Position Error (MPJPE) metric, which outputs the mean euclidean distance between the joints (keypoints) positions estimated and the ones in the groundtruth.\n",
    "\n",
    "The formula for MPJPE is the following:\n",
    "\n",
    "$\\text{MPJPE} = \\frac1T\\frac1N\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{i=1}^{N}\\|(J_{i}^{(t)}-J_{root}^{(t)})-(Ĵ_{i}^{(t)}-Ĵ_{root}^{(t)})\\|$\n",
    "\n",
    "Where N is the number of joints, and T the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the formula above, we need to align the root joints of the labels and the network output. In order to do that, I have defined a function (`substract_root_PJPE`) that substracts the root joint of each keypoint set (face, hands, body) in the corresponding keypoint set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "MPJPE = []\n",
    "h = model.init_hidden(batch_size)\n",
    "preds, inps, labls, lengs = [], [], [], []\n",
    "iscal, oscal = [], []\n",
    "mx, my, mz = [], [], []\n",
    "\n",
    "model.eval()\n",
    "for inputs_test, labels_test, lengths_test, is_test, os_test, mx_test, my_test, mz_test in test_loader:\n",
    "    \n",
    "    h = tuple([each.data for each in h])\n",
    "    inputs_test, labels_test, lengths_test = inputs_test.to(device), labels_test.to(device), lengths_test.to(device)\n",
    "    \n",
    "    output_test, h = model(inputs_test, h, lengths_test)\n",
    "    \n",
    "    e = [preds, inps, labls, lengs, iscal, oscal, mx, my, mz]\n",
    "    b = [output_test, inputs_test, labels_test, lengths_test, is_test, os_test, mx_test, my_test, mz_test]\n",
    "    for k in range(len(e)):\n",
    "        e[k].append(b[k])      \n",
    "    \n",
    "    test_loss = loss_function(nn.utils.rnn.pack_padded_sequence(output_test, lengths=lengths_test, batch_first=True,\n",
    "                                                               enforce_sorted=False).data,\n",
    "                             nn.utils.rnn.pack_padded_sequence(labels_test, lengths=lengths_test, batch_first=True,\n",
    "                                                               enforce_sorted=False).data)\n",
    "    test_losses.append(test_loss.item())\n",
    "    rooted_o, rooted_l = substract_root_PJPE(output_test), substract_root_PJPE(labels_test)\n",
    "    MPJPE.append(mpjpe(rooted_o, rooted_l, lengths_test))\n",
    "\n",
    "test_predictions = torch.cat(tuple(preds), dim=0)\n",
    "test_inputs = torch.cat(tuple(inps), dim=0)\n",
    "test_groundtruth = torch.cat(tuple(labls), dim=0)\n",
    "test_inp_scale, test_out_scale = torch.cat(tuple(iscal), dim=0), torch.cat(tuple(oscal), dim=0)\n",
    "test_mx, test_my, test_mz = torch.cat(tuple(mx), dim=0), torch.cat(tuple(my), dim=0), torch.cat(tuple(mz), dim=0)\n",
    "MPJPE_total = np.mean(MPJPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 0.3779 \n",
      "Test loss: 0.3738\n"
     ]
    }
   ],
   "source": [
    "print(f\"MPJPE: {MPJPE_total:.4f}\", f\"\\nTest loss: {np.mean(test_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results into a json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'train':{'inputs':tr_inputs.tolist(), 'predictions':tr_predictions.tolist(), 'labels':tr_groundtruth.tolist(), 'lengths':tr_lengths.tolist()},\n",
    "          'validation':{'inputs':val_inputs.tolist(), 'predictions':val_predictions.tolist(), 'labels':val_groundtruth.tolist(), 'lengths':val_length.tolist()},\n",
    "          'test':{'inputs':test_inputs.tolist(), 'predictions':test_predictions.tolist(), 'labels':test_groundtruth.tolist(), 'lengths':test_lengths.tolist()}}\n",
    "with open('../../results/small_body.json', 'w') as fp:\n",
    "    json.dump(results, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load results from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../results/small_body.json', 'r') as j:\n",
    "    jd = json.load(j)\n",
    "    tr, val, test = jd['train'], jd['validation'], jd['test']\n",
    "    tr_inputs, tr_predictions, tr_groundtruth, tr_lengths = tuple(torch.tensor(tr[n]) for n in ['inputs', 'predictions',\n",
    "                                                                                              'labels', 'lengths'])\n",
    "    val_inputs, val_predictions, val_groundtruth, val_length = tuple(torch.tensor(val[n]) for n in ['inputs', 'predictions',\n",
    "                                                                                              'labels', 'lengths'])\n",
    "    test_inputs, test_predictions, test_groundtruth, test_lengths = tuple(torch.tensor(test[n]) for n in ['inputs', 'predictions',\n",
    "                                                                                              'labels', 'lengths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([288, 250, 52]), torch.Size([32, 250, 26]), torch.Size([38]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_inputs.shape, val_predictions.shape, test_lengths.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to better understanding of the results, I will plot some of the frames from the last batches on the training and validation, and also from testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_rotate(c_inputs, c_z, frames, frame):\n",
    "    c_inputs[:,::2].mul_(mom_x[1])\n",
    "    c_inputs[:,1::2].mul_(mom_y[1])\n",
    "    c_z.mul_(stdz)\n",
    "\n",
    "    bodiesXY = torch.chunk(c_inputs[frames, :], len(frames), dim=0)\n",
    "    bodiesZ = torch.chunk(c_z[frames, :], len(frames), dim=0)\n",
    "    \n",
    "    x = bodiesXY[frame].squeeze()[::2]\n",
    "    y = bodiesXY[frame].squeeze()[1::2]\n",
    "    z = bodiesZ[frame].squeeze()\n",
    "\n",
    "    l_arm = [[c[i] for i in [1, 0, 9, 10, 11]] for c in [x,y,z]]\n",
    "    r_arm = [[c[i] for i in [0, 3, 4, 5]] for c in [x,y,z]]\n",
    "    l_leg = [[c[i] for i in [0, 2, 12, 13, 14, 22, 23, 24]] for c in [x,y,z]]\n",
    "    r_leg = [[c[i] for i in [2, 6, 7, 8, 19, 20, 21]] for c in [x,y,z]]\n",
    "    head = [[c[i] for i in [18, 17, 1, 15, 16]] for c in [x,y,z]]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    def init():\n",
    "        ax.plot(r_arm[0], r_arm[2], r_arm[1])\n",
    "        ax.plot(l_arm[0], l_arm[2], l_arm[1])\n",
    "        ax.plot(r_leg[0], r_leg[2], r_leg[1])\n",
    "        ax.plot(l_leg[0], l_leg[2], l_leg[1])\n",
    "        ax.plot(head[0], head[2], head[1])\n",
    "        \n",
    "        lims = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()\n",
    "        spans = lims[0][1]-lims[0][0], lims[1][1]-lims[1][0], lims[2][1]-lims[2][0]\n",
    "        span = max(spans)\n",
    "        margins = [(span-s)/2 for  s in spans]\n",
    "        ax.set_xlim(lims[0][0]-margins[0], lims[0][1]+margins[0])\n",
    "        ax.set_ylim(lims[1][0]-margins[1], lims[1][1]+margins[1])\n",
    "        ax.set_zlim(lims[2][0]-margins[2], lims[2][1]+margins[2])\n",
    "        \n",
    "        return fig,\n",
    "\n",
    "    def animate(i):\n",
    "        ax.view_init(elev=220., azim=3.6*i)\n",
    "        return fig,\n",
    "\n",
    "    # Animate\n",
    "    ani = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=100, interval=100, blit=True)    \n",
    "\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frames(predictions, groundtruth, inputs, video_n, frames, rot, train=False):\n",
    "    \n",
    "    inp = inputs.clone()\n",
    "    preds = predictions.clone()\n",
    "    grtr = groundtruth.clone()\n",
    "\n",
    "    bodiesXY = torch.chunk(inp[video_n, frames, :], len(frames), dim=0)\n",
    "    pred_bodiesZ = torch.chunk(preds[video_n, frames, :], len(frames), dim=0)\n",
    "    true_bodiesZ = torch.chunk(grtr[video_n, frames, :], len(frames), dim=0)\n",
    "   \n",
    "    nrows = np.ceil(len(frames)/2)\n",
    "    fig = plt.figure(figsize=(15, 6*nrows))\n",
    "    fig2 = plt.figure(figsize=(15, 6*nrows))\n",
    "    for frame in range(len(frames)):\n",
    "        x = bodiesXY[frame].squeeze()[::2].tolist()\n",
    "        y = bodiesXY[frame].squeeze()[1::2].tolist()\n",
    "        pred_z = pred_bodiesZ[frame].squeeze().tolist()\n",
    "        true_z = true_bodiesZ[frame].squeeze().tolist()\n",
    "        if not train:\n",
    "            print((x2.max()-x2.min())/(x1.max()-x1.min()))\n",
    "            x1 = x1*((x2.max()-x2.min())/(x1.max()-x1.min()))\n",
    "        \n",
    "        r = R.from_euler('y', rot, degrees=True)\n",
    "        \n",
    "        xyz1, xyz2 = np.asarray([c for c in zip(x, y, pred_z)]), np.asarray([c for c in zip(x, y, true_z)])\n",
    "        xyz1, xyz2 = r.apply(xyz1), r.apply(xyz2)\n",
    "        x1, x2 = xyz1[:,0], xyz2[:,0]\n",
    "        y1, y2 = xyz1[:,1], xyz2[:,1]\n",
    "        pred_z, true_z = xyz1[:,2], xyz2[:,2]\n",
    "        \n",
    "        r_arm = tuple([[c[i] for i in [1, 0, 9, 10, 11]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        l_arm = tuple([[c[i] for i in [0, 3, 4, 5]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        r_leg = tuple([[c[i] for i in [0, 2, 12, 13, 14, 22, 23, 24]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        l_leg = tuple([[c[i] for i in [2, 6, 7, 8, 19, 20, 21]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        head = tuple([[c[i] for i in [18, 17, 1, 15, 16]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "\n",
    "        ax = fig.add_subplot(nrows, 2, frame+1, projection='3d')\n",
    "\n",
    "        ax.plot(r_arm[0][0], r_arm[0][1], r_arm[0][2])\n",
    "        ax.plot(l_arm[0][0], l_arm[0][1], l_arm[0][2])\n",
    "        ax.plot(r_leg[0][0], r_leg[0][1], r_leg[0][2])\n",
    "        ax.plot(l_leg[0][0], l_leg[0][1], l_leg[0][2])\n",
    "        ax.plot(head[0][0], head[0][1], head[0][2])\n",
    "        \n",
    "        ax2 = fig2.add_subplot(nrows, 2, frame+1, projection='3d')\n",
    "        ax2.plot(r_arm[1][0], r_arm[1][1], r_arm[1][2])\n",
    "        ax2.plot(l_arm[1][0], l_arm[1][1], l_arm[1][2])\n",
    "        ax2.plot(r_leg[1][0], r_leg[1][1], r_leg[1][2])\n",
    "        ax2.plot(l_leg[1][0], l_leg[1][1], l_leg[1][2])\n",
    "        ax2.plot(head[1][0], head[1][1], head[1][2])\n",
    "        \n",
    "        lims = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()\n",
    "        spans = lims[0][1]-lims[0][0], lims[1][1]-lims[1][0], lims[2][1]-lims[2][0]\n",
    "        span = max(spans)\n",
    "        margins = [(span-s)/2 for  s in spans]\n",
    "        ax.set_xlim(lims[0][0]-margins[0], lims[0][1]+margins[0])\n",
    "        ax.set_ylim(lims[1][0]-margins[1], lims[1][1]+margins[1])\n",
    "        ax.set_zlim(lims[2][0]-margins[2], lims[2][1]+margins[2])\n",
    "        \n",
    "        lims2 = ax2.get_xlim(), ax2.get_ylim(), ax2.get_zlim()\n",
    "        spans2 = lims2[0][1]-lims2[0][0], lims2[1][1]-lims2[1][0], lims2[2][1]-lims2[2][0]\n",
    "        span2 = max(spans2)\n",
    "        margins2 = [(span2-s)/2 for  s in spans2]\n",
    "        ax2.set_xlim(lims2[0][0]-margins2[0], lims2[0][1]+margins2[0])\n",
    "        ax2.set_ylim(lims2[1][0]-margins2[1], lims2[1][1]+margins2[1])\n",
    "        ax2.set_zlim(lims2[2][0]-margins2[2], lims2[2][1]+margins2[2])\n",
    "\n",
    "        ax.view_init(elev=-65., azim=-90.)\n",
    "        ax2.view_init(elev=-65., azim=-90.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single frame\n",
    "On the first cell you can select which frames you want to plot and from which video of the batch. On the second you select which frame of the previosly selected you want to plot, specifying its index on the declared \"frames\" list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last batches of training -output, inputs, labels-.\n",
    "vid = 1\n",
    "frames = [100]\n",
    "\n",
    "c_inputs = training_kp[vid].clone()\n",
    "c_labels = training_lbl[vid].clone()\n",
    "c_inputs[:,::2].mul_(training_inpscale[vid, 0])\n",
    "c_inputs[:,1::2].mul_(training_inpscale[vid, 1])\n",
    "c_labels.mul_(training_outscale[vid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(plot_and_rotate(c_inputs, c_output, frames, 0).to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_inputs = tr_inputs[vid].clone()\n",
    "HTML(plot_and_rotate(c_inputs, c_labels, frames, 0).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice of frames\n",
    "Now let's plot a sequence of frames of the selected video. We will plot both the groundtruth and the predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [i for i in range(1,9,2)]\n",
    "video_n = 34\n",
    "\n",
    "c_inputs = tr_inputs.clone()\n",
    "c_output = tr_predictions.clone()\n",
    "c_labels = tr_groundtruth.clone()\n",
    "\n",
    "for vid in range(c_labels.shape[0]): \n",
    "    c_inputs[vid,:,::2].mul_(tr_inp_scale[vid, 0])\n",
    "    c_inputs[vid,:,1::2].mul_(tr_inp_scale[vid, 1])\n",
    "    c_output[vid].mul_(tr_out_scale[vid])\n",
    "    c_labels[vid].mul_(tr_out_scale[vid])\n",
    "    \n",
    "    c_inputs[vid,:,::2].mul_(tr_mx[vid, 1])\n",
    "    c_inputs[vid,:,1::2].mul_(tr_my[vid, 1])\n",
    "    c_output[vid].mul_(tr_mz[vid])\n",
    "    c_labels[vid].mul_(tr_mz[vid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbab8ccc90a43439a7e7a3b1c160f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5b285ecdd941dd9e9a231791755f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_frames(c_output, c_labels, c_inputs, video_n, frames, -90, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [i for i in range(1,9,2)]\n",
    "video_n = 17\n",
    "\n",
    "c_inputs = test_inputs.clone()\n",
    "c_output = test_predictions.clone()\n",
    "c_labels = test_groundtruth.clone()\n",
    "\n",
    "for vid in range(c_labels.shape[0]): \n",
    "    c_inputs[vid,:,::2].mul_(test_inp_scale[vid, 0])\n",
    "    c_inputs[vid,:,1::2].mul_(test_inp_scale[vid, 1])\n",
    "    c_output[vid].mul_(test_out_scale[vid,0])\n",
    "    c_labels[vid].mul_(test_out_scale[vid,1])\n",
    "    \n",
    "    c_inputs[vid,:,::2].mul_(test_mx[vid, 1])\n",
    "    c_inputs[:,:,1::2].mul_(test_my[vid, 1])\n",
    "    c_output[vid].mul_(test_mz[vid,0])\n",
    "    c_labels[vid].mul_(test_mz[vid,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ccf6df3585413985af122e23042664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02a483d56354f9896641857c63ffc0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_frames(c_output, c_labels, c_inputs, video_n, frames, -90, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [i for i in range(1,9,2)]\n",
    "video_n = 17\n",
    "\n",
    "c_inputs = val_inputs.clone()\n",
    "c_output = val_predictions.clone()\n",
    "c_labels = val_groundtruth.clone()\n",
    "\n",
    "for vid in range(c_labels.shape[0]): \n",
    "    c_inputs[vid,:,::2].mul_(val_inp_scale[vid, 0])\n",
    "    c_inputs[vid,:,1::2].mul_(val_inp_scale[vid, 1])\n",
    "    c_output[vid].mul_(val_out_scale[vid])\n",
    "    c_labels[vid].mul_(val_out_scale[vid])\n",
    "    \n",
    "    c_inputs[vid,:,::2].mul_(val_mom_x[vid, 1])\n",
    "    c_inputs[:,:,1::2].mul_(val_mom_y[vid, 1])\n",
    "    c_output[vid].mul_(val_mom_z[vid])\n",
    "    c_labels[vid].mul_(val_mom_z[vid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3d97c68a8b4afb91c9f19a1565a9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adca4f266a3b4f1e8745c4ae18086975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_frames(c_output, c_labels, c_inputs, video_n, frames, -90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
