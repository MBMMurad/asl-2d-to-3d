{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Plotting utilities\n",
    "import pyprind\n",
    "\n",
    "# Directory and file utilities\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition\n",
    "Now I will define some functions in order to parse and organise the data, and later convert it to pytorch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is structured as follows: in the dataset directory there are several folders, each folder corresponds to a recording; each of these folders contains a folder with the audio, folders with face, hands and body keypoints estimations for each frame, and a folder with the video recorded from different views.\n",
    "\n",
    "In this first approach I will be using the keypoints estimations. Every keypoint folder (face, hands or body) is organized the same way: it contains a json per frame of the video, which includes the 3D keypoints estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_keypoints` will go through each folder in the dataset directory and retrieve the face keypoints, the hands keypoints and the body keypoints. It will separate them into input (2D coordinates per joint per frame) and grountruth (third coordinate to estimate for each input 2D keypoint). \n",
    "The input will be of shape $([n videos, seq len, input size])$, where *seq_len* = number of frames, and *input_size* = face + hands + body keypoints, that is (70+(21+21)+26)x2 -multiplied by 2 because there are x and y coordinates-. The groundtruth (label) data will be of the same shape, except that the last dimension size will not be multiplied by 2 (there's only one coordinate to estimate).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints(data_path):\n",
    "    dataset = []\n",
    "    groundtruth = []\n",
    "    # Look over just the folders inside the directory\n",
    "    just_folders = filter(lambda x: isdir(join(data_path, x)), listdir(data_path))\n",
    "    for p in list(map(lambda x: join(data_path, x), just_folders)): \n",
    "        # Gets 2 list of n_frames lists, one for the 2D coordinates and one for the third coordinate.\n",
    "        # Each list of the n_frames lists contains, either the (x and y) or the z of each keypoint for the face(first line), hands(second), body(third).\n",
    "        # e.g. the first line will result in [[x1,y1,x2,y2...x70,y70]sub1...[x1,y1...x70,y70]subN], [[z1,z2...z70]sub1...[z1..z70]subN]\n",
    "        # Actually, as there will be two of each list above because there are two people en each video.\n",
    "        face_2d, face_3d = get_face(p)\n",
    "        hands_2d, hands_3d = get_hands(p)\n",
    "        pose_2d, pose_3d = get_body(p)\n",
    "        \n",
    "        # Concatenates the coordinates for the face, hands and body on the last dimension, for each person.\n",
    "        vid_input_p1, vid_input_p2 = ([fa+ha+po for fa, ha, po in zip(face_2d[i], hands_2d[i], pose_2d[i])] for i in range(2))\n",
    "        vid_labels_p1, vid_labels_p2 = ([fa+ha+po for fa, ha, po in zip(face_3d[i], hands_3d[i], pose_3d[i])] for i in range(2))\n",
    "        \n",
    "        dataset.append(vid_input_p1)\n",
    "        dataset.append(vid_input_p2)\n",
    "        groundtruth.append(vid_labels_p1)\n",
    "        groundtruth.append(vid_labels_p2)\n",
    "        print(f'Completed folder {p}')\n",
    "    return dataset, groundtruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are in charge of retrieving the keypoints from each json. The json face json has a key *people* with a list of person objects. Each person object has *id* field and *landmarks* field, the latter containing a list of 3D coordinates for each keypoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face(path):\n",
    "    face_2D_seq = ([], [])\n",
    "    face_3D_seq = ([], [])\n",
    "    # List only the files (json), for there might be folders containing invalid frames.\n",
    "    paths = map(lambda x: join(path, 'hdFace3d', x), sorted(listdir(join(path, 'hdFace3d'))))\n",
    "    files = list(filter(lambda x: isfile(x), paths))\n",
    "    for f in files[1:]: # The first frame of face keypoints estimation it's blank\n",
    "        with open(f, 'r') as j:\n",
    "            json_array = json.load(j)\n",
    "            i = 0\n",
    "            for person in json_array['people']:\n",
    "                if person['id'] != -1: # If the id is -1, it means there's no person\n",
    "                    x = person['face70']['landmarks'][::3]\n",
    "                    y = person['face70']['landmarks'][1::3]\n",
    "                    two_coord = [l[item] for item in range(len(x)) for l in [x,y]]\n",
    "                    third_coord = person['face70']['landmarks'][2::3]\n",
    "                    face_2D_seq[i].append(two_coord)\n",
    "                    face_3D_seq[i].append(third_coord)\n",
    "                    i+=1\n",
    "            if i<2: # In case there was only one person detected on a frame\n",
    "                face_2D_seq[i].append([0. for i in range(140)])\n",
    "                face_3D_seq[i].append([0. for i in range(70)])\n",
    "    print('Face completed.')\n",
    "    # Each return var being a tuple with the list of n_frames list of coordinates for each person\n",
    "    return face_2D_seq, face_3D_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hands json contains the *landmarks* field inside both *left_hand* and *right_hand* field. As there are some frames that may not have one of the hands estimated, I've had to put some exception handling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hands(path):\n",
    "    hand_2D_seq = ([], [])\n",
    "    hand_3D_seq = ([], [])\n",
    "    paths = map(lambda x: join(path, 'hdHand3d', x), sorted(listdir(join(path, 'hdHand3d'))))\n",
    "    files = list(filter(lambda x: isfile(x), paths))\n",
    "    for f in files[1:-1]: # The first and the last frames of these folders are blank.\n",
    "        with open(f, 'r') as j:\n",
    "            json_array = json.load(j)\n",
    "            i = 0\n",
    "            for person in json_array['people']:\n",
    "                if person['id'] != -1:\n",
    "                    try:\n",
    "                        # Separate x,y from z\n",
    "                        hands= [[person[hand]['landmarks'][c] \n",
    "                                 for c in range(len(person['right_hand']['landmarks'])) if (c+1)%3!=0] \n",
    "                                 for hand in ['left_hand', 'right_hand']]\n",
    "                        hand_2D_seq[i].append(hands[0]+hands[1])\n",
    "                        \n",
    "                        hands_3d = [person[hand]['landmarks'][2::3] \n",
    "                                    for hand in ['left_hand', 'right_hand']]\n",
    "                        hand_3D_seq[i].append(hands_3d[0]+hands_3d[1])\n",
    "                    \n",
    "                    except Exception as e: # In case left_hand or right_hand keys don't exist.\n",
    "                        if 'left_hand' in str(e): \n",
    "                            try: # Just put a 0., 0., 0. estimation for each keypoint of the left_hand\n",
    "                                hands = [0. for i in range(42)]+[person['right_hand']['landmarks'][c] \n",
    "                                                                 for c in range(len(person['right_hand']['landmarks'])) if (c+1)%3!=0]\n",
    "                                hands_3d = [0. for i in range(21)]+person['right_hand']['landmarks'][2::3]\n",
    "                            except: # In case neither left_hand nor right_hand exist\n",
    "                                hands = [0. for i in range(84)]\n",
    "                                hands_3d = [0. for i in range(42)]\n",
    "                        elif 'right_hand' in str(e): # Just put a 0., 0., 0. estimation for each keypoint of the right_hand\n",
    "                            hands = [person['left_hand']['landmarks'][c] \n",
    "                                     for c in range(len(person['left_hand']['landmarks'])) if (c+1)%3!=0]+[0. for i in range(42)]\n",
    "                            hands_3d = person['left_hand']['landmarks'][2::3]+[0. for i in range(21)]\n",
    "\n",
    "                        hand_2D_seq[i].append(hands)\n",
    "                        hand_3D_seq[i].append(hands_3d)\n",
    "                    i+=1\n",
    "            if i<2:\n",
    "                hand_2D_seq[i].append([0. for i in range(84)])\n",
    "                hand_3D_seq[i].append([0. for i in range(42)])\n",
    "    print('Hands completed.')\n",
    "    return hand_2D_seq, hand_3D_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The body json is organised a bit differently, inside each person object contains the *joints26* field with a list of 3D coordinates. But this list is structured as follows: *[x1,y1,z1,acc1,x2,y2,z2,acc2...]*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_body(path):\n",
    "    body_2D_seq = ([], [])\n",
    "    body_3D_seq = ([], [])\n",
    "    paths = map(lambda x: join(path, 'hdPose3d_stage1_op25', x), sorted(listdir(join(path, 'hdPose3d_stage1_op25'))))\n",
    "    files = list(filter(lambda x: isfile(x), paths))\n",
    "    for f in files[:-1]:\n",
    "        with open(f, 'r') as j:\n",
    "            json_array = json.load(j)\n",
    "            i = 0\n",
    "            for person in json_array['bodies']:\n",
    "                if person['id'] != -1:\n",
    "                    x = person['joints26'][::4]\n",
    "                    y = person['joints26'][1::4]\n",
    "                    two_coord = [l[item] for item in range(len(x)) for l in [x,y]]\n",
    "                    third_coord = person['joints26'][2::4]\n",
    "                    body_2D_seq[i].append(two_coord)\n",
    "                    body_3D_seq[i].append(third_coord)\n",
    "                    i += 1\n",
    "            if i<2:\n",
    "                body_2D_seq[i].append([0. for i in range(52)])\n",
    "                body_3D_seq[i].append([0. for i in range(26)])\n",
    "    print('Body completed.')\n",
    "    return body_2D_seq, body_3D_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face completed.\n",
      "Hands completed.\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190419_asl2\n",
      "Face completed.\n",
      "Hands completed.\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190419_asl4\n",
      "Face completed.\n",
      "Hands completed.\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190419_asl5\n",
      "Face completed.\n",
      "Hands completed.\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl1\n",
      "Face completed.\n",
      "Hands completed.\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl2\n",
      "Face completed.\n",
      "Hands completed.\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl3\n",
      "Face completed.\n",
      "Hands completed.\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl5\n",
      "Face completed.\n",
      "Hands completed.\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl7\n",
      "Face completed.\n",
      "Hands completed.\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl9\n",
      "Face completed.\n",
      "Hands completed.\n",
      "Body completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl91\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../../data/DB keypoints'\n",
    "dataset, groundtruth = get_keypoints(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset structuring\n",
    "Now let's convert the lists obtained to Pytorch tensors and organise them in train, validation and test datasets. \n",
    "First, I will define a padding function in order to make all the sequences of video frames the same length, so I can train the LSTM in batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8751"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def padding_seq(dataset):\n",
    "    max_seq = max([len(x) for x in dataset])\n",
    "    for seq in dataset:\n",
    "        for i in range(max_seq-len(seq)):\n",
    "            seq.append([np.nan for j in range(len(seq[0]))])\n",
    "    return max_seq\n",
    "\n",
    "max_seq = padding_seq(dataset)\n",
    "padding_seq(groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 8751, 276) (20, 8751, 138)\n"
     ]
    }
   ],
   "source": [
    "# From python lists to numpy ndarray.\n",
    "dataset = np.asarray(dataset)\n",
    "groundtruth = np.asarray(groundtruth)\n",
    "print(dataset.shape, groundtruth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing videos\n",
      "Current video\n",
      "0% [############################  ] 100% | ETA: 00:00:09/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:23: RuntimeWarning: Mean of empty slice\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:33: RuntimeWarning: Mean of empty slice\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:43: RuntimeWarning: Mean of empty slice\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:21\n",
      "0% [#                   ] 100% | ETA: 00:44:50Current video\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:20\n",
      "0% [##                  ] 100% | ETA: 00:42:19Current video\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:24\n",
      "0% [###                 ] 100% | ETA: 00:40:19Current video\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:25\n",
      "0% [####                ] 100% | ETA: 00:38:09Current video\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:18\n",
      "0% [#####               ] 100% | ETA: 00:35:32Current video\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:23\n",
      "0% [######              ] 100% | ETA: 00:33:14Current video\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:26\n",
      "0% [#######             ] 100% | ETA: 00:30:58Current video\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:25\n",
      "0% [########            ] 100% | ETA: 00:28:40Current video\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:30\n",
      "0% [#########           ] 100% | ETA: 00:26:25Current video\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:24\n",
      "0% [##########          ] 100% | ETA: 00:24:01Current video\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:22\n",
      "0% [###########         ] 100% | ETA: 00:21:35Current video\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:24\n",
      "0% [############        ] 100% | ETA: 00:19:11Current video\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:26\n",
      "0% [#############       ] 100% | ETA: 00:16:49Current video\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:24\n",
      "0% [##############      ] 100% | ETA: 00:14:25Current video\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:27\n",
      "0% [###############     ] 100% | ETA: 00:12:02Current video\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:26\n",
      "0% [################    ] 100% | ETA: 00:09:38Current video\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:24\n",
      "0% [#################   ] 100% | ETA: 00:07:13Current video\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:22\n",
      "0% [##################  ] 100% | ETA: 00:04:48Current video\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:23\n",
      "0% [################### ] 100% | ETA: 00:02:24Current video\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:23\n",
      "0% [####################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:48:05\n"
     ]
    }
   ],
   "source": [
    "def stats(tensor):\n",
    "    pbar = pyprind.ProgBar(len(tensor), title='Analyzing videos')\n",
    "    d_min = []\n",
    "    d_max = []\n",
    "    d_mean = []\n",
    "    for vid in tensor:\n",
    "        vid_min = [np.inf, np.inf, np.inf]\n",
    "        vid_max = [0., 0., 0.]\n",
    "        frame_means = [[], [], []]\n",
    "        pbar2 = pyprind.ProgBar(len(vid), title='Current video')\n",
    "        for frame in vid:\n",
    "            frame_min = [np.inf, np.inf, np.inf]\n",
    "            frame_max = [0., 0., 0.]\n",
    "            frame_diff = [[], [], []]\n",
    "            for i in range(70):\n",
    "                for j in range(i+1, 70):\n",
    "                    m = np.abs(np.subtract(frame[i], frame[j]))\n",
    "                    frame_min[0] = m if m<frame_min[0] else frame_min[0]\n",
    "                    frame_max[0] = m if m>frame_max[0] else frame_max[0]\n",
    "                    frame_diff[0].append(m)\n",
    "            vid_min[0] = frame_min[0] if frame_min[0] < vid_min[0] else vid_min[0]\n",
    "            vid_max[0] = frame_max[0] if frame_max[0] > vid_max[0] else vid_max[0]\n",
    "            frame_means[0].append(np.nanmean(frame_diff[0]))\n",
    "            \n",
    "            for i in range(70, 112):\n",
    "                for j in range(i+1, 112):\n",
    "                    m = np.abs(np.subtract(frame[i], frame[j]))\n",
    "                    frame_min[1] = m if m<frame_min[1] else frame_min[1]\n",
    "                    frame_max[1] = m if m>frame_max[1] else frame_max[1]\n",
    "                    frame_diff[1].append(m)\n",
    "            vid_min[1] = frame_min[1] if frame_min[1] < vid_min[1] else vid_min[1]\n",
    "            vid_max[1] = frame_max[1] if frame_max[1] > vid_max[1] else vid_max[1]\n",
    "            frame_means[1].append(np.nanmean(frame_diff[1]))\n",
    "            \n",
    "            for i in range(112, len(frame)):\n",
    "                for j in range(i+1, len(frame)):\n",
    "                    m = np.abs(np.subtract(frame[i], frame[j]))\n",
    "                    frame_min[2] = m if m<frame_min[2] else frame_min[2]\n",
    "                    frame_max[2] = m if m>frame_max[2] else frame_max[2]\n",
    "                    frame_diff[2].append(m)\n",
    "            vid_min[2] = frame_min[2] if frame_min[2] < vid_min[2] else vid_min[2]\n",
    "            vid_max[2] = frame_max[2] if frame_max[2] > vid_max[2] else vid_max[2]\n",
    "            frame_means[2].append(np.nanmean(frame_diff[2]))\n",
    "            pbar2.update()\n",
    "        d_min.append(vid_min)\n",
    "        d_max.append(vid_max)\n",
    "        d_mean.append([np.nanmean(frame_means[0]), np.nanmean(frame_means[1]), np.nanmean(frame_means[2])])\n",
    "        pbar.update()\n",
    "    return d_min, d_max, d_mean\n",
    "\n",
    "mins, maxs, means = stats(groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-cf8fab3dcdb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Video {i}\\n-Face min: {mins[i][0]} max: {maxs[i][0]:.2f} mean: {means[i][0]:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"-Hands min: {mins[i][1]} max: {maxs[i][1]:.2f} mean: {means[i][1]:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"-Body min: {mins[i][2]} max: {maxs[i][2]:.2f} mean: {means[i][2]:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "for i in range(len(mins)):\n",
    "    print(f\"Video {i}\\n-Face min: {mins[i][0]} max: {maxs[i][0]:.2f} mean: {means[i][0]:.2f}\")\n",
    "    print(f\"-Hands min: {mins[i][1]} max: {maxs[i][1]:.2f} mean: {means[i][1]:.2f}\")\n",
    "    print(f\"-Body min: {mins[i][2]} max: {maxs[i][2]:.2f} mean: {means[i][2]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.657289328643808\n"
     ]
    }
   ],
   "source": [
    "print(means[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
