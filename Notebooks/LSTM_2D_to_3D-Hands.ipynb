{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D to 3D LSTM\n",
    "\n",
    "This is the first approach to try to estimate 3D points coordinates from 2D keypoints extracted with Openpose. Here I will build a simple LSTM to perform the task over the Panoptic Studio dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch utilities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Plotting utilities\n",
    "%matplotlib widget\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from timeit import default_timer as timer\n",
    "import pyprind\n",
    "\n",
    "# Directory and file utilities\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = { 0:'one video', 1:'one signer', 2:'all signers'}\n",
    "MODE =  modes[0]\n",
    "video = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition\n",
    "Now I will define some functions in order to parse and organise the data, and later convert it to pytorch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is structured as follows: in the dataset directory there are several folders, each folder corresponds to a recording; each of these folders contains a folder with the audio, folders with face, hands and body keypoints estimations for each frame, and a folder with the video recorded from different views.\n",
    "\n",
    "In this first approach I will be using the keypoints estimations. Every keypoint folder (face, hands or body) is organized the same way: it contains a json per frame of the video, which includes the 3D keypoints estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_keypoints` will go through each folder in the dataset directory and retrieve the face keypoints, the hands keypoints and the body keypoints. It will separate them into input (2D coordinates per joint per frame) and grountruth (third coordinate to estimate for each input 2D keypoint). \n",
    "The input will be of shape $([n videos, seq len, input size])$, where *seq_len* = number of frames, and *input_size* = face + hands + body keypoints, that is (70+(21+21)+26)x2 -multiplied by 2 because there are x and y coordinates-. The groundtruth (label) data will be of the same shape, except that the last dimension size will not be multiplied by 2 (there's only one coordinate to estimate).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints(data_path):\n",
    "    dataset = []\n",
    "    groundtruth = []\n",
    "    # Look over just the folders inside the directory\n",
    "    just_folders = filter(lambda x: isdir(join(data_path, x)), listdir(data_path))\n",
    "    for p in list(map(lambda x: join(data_path, x), just_folders)): \n",
    "        # Gets 2 list of n_frames lists, one for the 2D coordinates and one for the third coordinate.\n",
    "        # Each list of the n_frames lists contains, either the (x and y) or the z of each keypoint for the face(first line), hands(second), body(third).\n",
    "        # e.g. the first line will result in [[x1,y1,x2,y2...x70,y70]sub1...[x1,y1...x70,y70]subN], [[z1,z2...z70]sub1...[z1..z70]subN]\n",
    "        # Actually, as there will be two of each list above because there are two people en each video.\n",
    "        hands_2d, hands_3d = get_hands(p)\n",
    "        \n",
    "        # Concatenates the coordinates for the face, hands and body on the last dimension, for each person.\n",
    "        vid_input_p1, vid_input_p2 = hands_2d\n",
    "        vid_labels_p1, vid_labels_p2 = hands_3d\n",
    "        \n",
    "        dataset.append(vid_input_p1)\n",
    "        dataset.append(vid_input_p2)\n",
    "        groundtruth.append(vid_labels_p1)\n",
    "        groundtruth.append(vid_labels_p2)\n",
    "        print(f'Completed folder {p}')\n",
    "    return dataset, groundtruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hands json contains the *landmarks* field inside both *left_hand* and *right_hand* field. As there are some frames that may not have one of the hands estimated, I've had to put some exception handling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hands(path):\n",
    "    hand_2D_seq = ([], [])\n",
    "    hand_3D_seq = ([], [])\n",
    "    paths = map(lambda x: join(path, 'hdHand3d', x), sorted(listdir(join(path, 'hdHand3d'))))\n",
    "    files = list(filter(lambda x: isfile(x), paths))\n",
    "    for f in files: # The first and the last frames of these folders are blank.\n",
    "        with open(f, 'r') as j:\n",
    "            json_array = json.load(j)\n",
    "            i = 0\n",
    "            for person in json_array['people']:\n",
    "                if person['id'] != -1:\n",
    "                    try:\n",
    "                        # Separate x,y from z\n",
    "                        hands= [[person[hand]['landmarks'][c] \n",
    "                                 for c in range(len(person['right_hand']['landmarks'])) if (c+1)%3!=0] \n",
    "                                 for hand in ['left_hand', 'right_hand']]\n",
    "                        hand_2D_seq[person['id']].append(hands[0]+hands[1])\n",
    "                        \n",
    "                        hands_3d = [person[hand]['landmarks'][2::3] \n",
    "                                    for hand in ['left_hand', 'right_hand']]\n",
    "                        hand_3D_seq[person['id']].append(hands_3d[0]+hands_3d[1])\n",
    "                    \n",
    "                    except Exception as e: # In case left_hand or right_hand keys don't exist.\n",
    "                        if 'left_hand' in str(e): \n",
    "                            try: # Just put a 0., 0., 0. estimation for each keypoint of the left_hand\n",
    "                                hands = hand_2D_seq[person['id']][-1][:42]+[person['right_hand']['landmarks'][c] \n",
    "                                                                 for c in range(len(person['right_hand']['landmarks'])) if (c+1)%3!=0]\n",
    "                                hands_3d = hand_3D_seq[person['id']][-1][:21]+person['right_hand']['landmarks'][2::3]\n",
    "                            except: # In case neither left_hand nor right_hand exist\n",
    "                                hands = hand_2D_seq[person['id']][-1]\n",
    "                                hands_3d = hand_3D_seq[person['id']][-1]\n",
    "                        elif 'right_hand' in str(e): # Just put a 0., 0., 0. estimation for each keypoint of the right_hand\n",
    "                            hands = [person['left_hand']['landmarks'][c] \n",
    "                                     for c in range(len(person['left_hand']['landmarks'])) if (c+1)%3!=0]+hand_2D_seq[person['id']][-1][42:]\n",
    "                            hands_3d = person['left_hand']['landmarks'][2::3]+hand_3D_seq[person['id']][-1][21:]\n",
    "\n",
    "                        hand_2D_seq[person['id']].append(hands)\n",
    "                        hand_3D_seq[person['id']].append(hands_3d)\n",
    "                    i+=1\n",
    "                    pid = person['id']\n",
    "            if i<2:\n",
    "                hand_2D_seq[1-pid].append(hand_2D_seq[1-pid][-1])\n",
    "                hand_3D_seq[1-pid].append(hand_3D_seq[1-pid][-1])\n",
    "    print('Hands completed.')\n",
    "    return hand_2D_seq, hand_3D_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190419_asl2\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190419_asl4\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190419_asl5\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl1\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl2\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl3\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl5\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl7\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl9\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl91\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl1\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl2\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl3\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl4\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl5\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl6\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl7\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl8\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl910\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl911\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl912\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl913\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../../data/DB keypoints'\n",
    "dataset, groundtruth = get_keypoints(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_seq(dataset):\n",
    "    max_seq = max([len(x) for x in dataset])\n",
    "    seq_lengths = []\n",
    "    for seq in dataset:\n",
    "        seq_lengths.append(len(seq))\n",
    "        for i in range(max_seq-len(seq)):\n",
    "            seq.append([np.nan for j in range(len(seq[0]))])\n",
    "        \n",
    "    return max_seq, seq_lengths\n",
    "\n",
    "max_seq, seq_lengths = padding_seq(dataset)\n",
    "_, _ = padding_seq(groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 8752, 84) (44, 8752, 42) (44,)\n"
     ]
    }
   ],
   "source": [
    "# From python lists to numpy ndarray.\n",
    "dataset = np.asarray(dataset)\n",
    "groundtruth = np.asarray(groundtruth)\n",
    "lengths = np.asarray(seq_lengths)\n",
    "print(dataset.shape, groundtruth.shape, lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../pickles/hands_data.npy', dataset)\n",
    "np.save('../../pickles/hands_ground.npy', groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../pickles/hands_lengths.npy', lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset structuring\n",
    "Now let's convert the lists obtained to Pytorch tensors and organise them in train, validation and test datasets. \n",
    "First, I will define a padding function in order to make all the sequences of video frames the same length, so I can train the LSTM in batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8752\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load('../../pickles/hands_data.npy', allow_pickle=True)\n",
    "groundtruth = np.load('../../pickles/hands_ground.npy', allow_pickle=True)\n",
    "lengths = np.load('../../pickles/hands_lengths.npy', allow_pickle=True)\n",
    "max_seq = dataset.shape[1]\n",
    "print(max_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546,)\n"
     ]
    }
   ],
   "source": [
    "if MODE == 'one video':\n",
    "    dataset, groundtruth = dataset[video], groundtruth[video]\n",
    "    chunks_d = np.split(dataset, 547, axis=0)[:lengths[video]//16]\n",
    "    chunks_g = np.split(groundtruth, 547, axis=0)[:lengths[video]//16]\n",
    "    chunks_d = tuple(np.expand_dims(c, axis=0) for c in chunks_d)\n",
    "    chunks_g = tuple(np.expand_dims(c, axis=0) for c in chunks_g)\n",
    "    \n",
    "    dataset = np.concatenate(chunks_d, axis=0)\n",
    "    groundtruth = np.concatenate(chunks_g, axis=0)\n",
    "    lengths = np.asarray([16 for i in range(lengths[video]//16)])\n",
    "    print(lengths.shape)\n",
    "\n",
    "elif MODE == 'one signer':\n",
    "    dataset, groundtruth, lengths = dataset[20::2, :-2], groundtruth[20::2, :-2], lengths[20::2]\n",
    "    print(lengths)\n",
    "    lengths = np.asarray([min(8750, x) for x in lengths])\n",
    "    print(lengths)\n",
    "    chunks_d, chunks_g = np.split(dataset, 35, axis=1), np.split(groundtruth, 35, axis=1)\n",
    "    \n",
    "    dataset, groundtruth = np.concatenate(chunks_d, axis=0), np.concatenate(chunks_g, axis=0)\n",
    "    lengths = np.concatenate(tuple([250 if j*250<=lengths[i] \n",
    "                                    else (lengths[i]%250 if (j-1)*250<lengths[i] \n",
    "                                          else 0) for i in range(12)] for j in range(1,36)), axis=0)\n",
    "    print(dataset.shape, groundtruth.shape, lengths.shape)\n",
    "    \n",
    "elif MODE == 'all signers':\n",
    "    dataset, groundtruth = dataset[:, :-2], groundtruth[:, :-2]\n",
    "    lengths = np.asarray([min(8750, x) for x in lengths])\n",
    "    chunks_d, chunks_g = np.split(dataset, 25, axis=1), np.split(groundtruth, 25, axis=1)\n",
    "    \n",
    "    dataset, groundtruth = np.concatenate(chunks_d, axis=0), np.concatenate(chunks_g, axis=0)\n",
    "    lengths = np.concatenate(tuple([350 if j*350<=lengths[i]\n",
    "                                    else (lengths[i]%350 if (j-1)*350<lengths[i] \n",
    "                                          else 0) for i in range(44)] for j in range(1,26)), axis=0)\n",
    "    print(dataset.shape, groundtruth.shape, lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546, 16, 84) (546, 16, 42) (546,)\n"
     ]
    }
   ],
   "source": [
    "# Clean all NaN videos\n",
    "dataset = np.delete(dataset, np.where(lengths==0), axis=0)\n",
    "groundtruth = np.delete(groundtruth, np.where(lengths==0), axis=0)\n",
    "lengths = np.delete(lengths, np.where(lengths==0), axis=0)\n",
    "\n",
    "print(dataset.shape, groundtruth.shape, lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546, 16, 84) (546, 16, 42) (546,)\n"
     ]
    }
   ],
   "source": [
    "def align(tensor, coordinates=1):\n",
    "    for n_vid in range(tensor.shape[0]):\n",
    "        max_value = [np.nanmax(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        min_value = [np.nanmin(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        center = [(max_value[i]+min_value[i])/2 for i in range(coordinates)]\n",
    "        for j in range(coordinates):\n",
    "            subtensor = tensor[n_vid, :, j::coordinates]\n",
    "            subtensor[:] = np.subtract(subtensor, center[j])\n",
    "\n",
    "align(dataset,2)\n",
    "align(groundtruth)\n",
    "print(dataset.shape, groundtruth.shape, lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546, 16, 84) (546, 16, 42)\n"
     ]
    }
   ],
   "source": [
    "r = R.from_euler('y', 110, degrees=True)\n",
    "shapes = dataset.shape\n",
    "dataset = dataset.reshape(-1, 2)\n",
    "groundtruth = groundtruth.reshape(-1,1)\n",
    "xyz = np.concatenate((dataset, groundtruth), axis=1)\n",
    "xyz = r.apply(xyz)\n",
    "dataset, groundtruth = xyz[:, :2].reshape(shapes), xyz[:,2].reshape(shapes[0], shapes[1],42)\n",
    "\n",
    "print(dataset.shape, groundtruth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546, 2)\n"
     ]
    }
   ],
   "source": [
    "def norm_uniform(tensor, coordinates=1, factor=None):\n",
    "    scale = []\n",
    "    mean_ranges = []\n",
    "    for n_vid in range(tensor.shape[0]):\n",
    "        coord_scale = []\n",
    "        max_value = [np.nanmax(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        min_value = [np.nanmin(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        center = [(max_value[i]+min_value[i])/2 for i in range(coordinates)]\n",
    "        ranges = np.ndarray((tensor.shape[1],coordinates))\n",
    "        for n_frame in range(tensor.shape[1]):\n",
    "            rang = [np.nanmax(tensor[n_vid, n_frame,i::coordinates])-np.nanmin(tensor[n_vid, n_frame,i::coordinates]) for i in range(coordinates)]\n",
    "            ranges[n_frame] = np.asarray(rang)\n",
    "        mean_range = [np.nanmean(ranges[:,i]) for i in range(coordinates)]\n",
    "        for j in range(coordinates):\n",
    "            subtensor = tensor[n_vid, :, j::coordinates]\n",
    "            subtensor[:] = np.subtract(subtensor, center[j])\n",
    "            if factor is not None:\n",
    "                subtensor[:] = np.divide(subtensor, factor[n_vid])\n",
    "            else:\n",
    "                subtensor[:] = np.divide(subtensor, max_value[j]-center[j])\n",
    "            coord_scale.append((max_value[j]-center[j] if factor is None else factor[n_vid]))\n",
    "        scale.append(coord_scale)\n",
    "        mean_ranges.append(mean_range)\n",
    "    return mean_ranges\n",
    "input_scale = np.asarray(norm_uniform(dataset,2)).squeeze()\n",
    "print(input_scale.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546, 2) (546, 2)\n"
     ]
    }
   ],
   "source": [
    "def normalize(tensor, coordinates=1, std=None):\n",
    "    moments = []\n",
    "    std_centroids = []\n",
    "    for n_vid in range(tensor.shape[0]):\n",
    "        coord_moments = []\n",
    "        mean_value = [np.nanmean(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        std_value = [np.nanstd(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        centroids = np.ndarray((tensor.shape[1],coordinates))\n",
    "        for n_frame in range(tensor.shape[1]):\n",
    "            centroid = [np.nanmean(tensor[n_vid, n_frame, i::coordinates]) for i in range(coordinates)]\n",
    "            centroids[n_frame] = np.asarray(centroid)\n",
    "        std_centroid = [np.nanstd(centroids[:,i]) for i in range(coordinates)]\n",
    "        if std is not None:\n",
    "            std_value = [std[n_vid]]\n",
    "        for j in range(coordinates):\n",
    "            subtensor = tensor[:, :, j::coordinates]\n",
    "            subtensor[:] = np.subtract(subtensor, mean_value[j])\n",
    "            subtensor[:] = np.divide(subtensor, std_value[j])\n",
    "            coord_moments.append((mean_value[j], std_value[j]))\n",
    "        moments.append(coord_moments)\n",
    "        std_centroids.append(std_centroid)\n",
    "    return moments, std_centroids\n",
    "\n",
    "moments, std_centroids = normalize(dataset, 2)\n",
    "mom_x, mom_y = np.asarray(moments)[:, 0], np.asarray(moments)[:, 1]\n",
    "std_centroids = np.asarray(std_centroids)\n",
    "print(mom_x.shape, mom_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546, 16, 84) (546, 16, 42) (546,)\n",
      "(546, 2) (546, 2) (546, 2)\n"
     ]
    }
   ],
   "source": [
    "# Randomly shuffle videos\n",
    "permutation = np.random.permutation(dataset.shape[0])\n",
    "dataset, groundtruth, lengths = dataset[permutation], groundtruth[permutation], lengths[permutation]\n",
    "input_scale, mom_x, mom_y = input_scale[permutation], mom_x[permutation], mom_y[permutation]\n",
    "std_centroids = std_centroids[permutation]\n",
    "\n",
    "print(dataset.shape, groundtruth.shape, lengths.shape)\n",
    "print(input_scale.shape, mom_x.shape, mom_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(437, 16, 84) (54, 16, 84) (55, 16, 84)\n",
      "(437, 16, 42) (54, 16, 42) (55, 16, 42)\n",
      "(437,) (54,) (55,)\n"
     ]
    }
   ],
   "source": [
    "l1, l2 = len(dataset), len(groundtruth)\n",
    "p1, p2 = 0.8, 0.9\n",
    "# Split in train, validation and test\n",
    "training_kp, val_kp, test_kp = dataset[:round(p1*l1)], dataset[round(p1*l1):round(p2*l1)], dataset[round(p2*l1):]\n",
    "training_lbl, val_lbl, test_lbl = groundtruth[:round(p1*l2)], groundtruth[round(p1*l2):round(p2*l2)], groundtruth[round(p2*l2):]\n",
    "training_lengths, val_lengths, test_lengths = lengths[:round(p1*l1)], lengths[round(p1*l1):round(p2*l1)], lengths[round(p2*l1):]\n",
    "training_inpscale, val_inpscale, test_inpscale = input_scale[:round(p1*l1)], input_scale[round(p1*l1):round(p2*l1)], input_scale[round(p2*l1):]\n",
    "training_mom_x, val_mom_x, test_mom_x = mom_x[:round(p1*l1)], mom_x[round(p1*l1):round(p2*l1)], mom_x[round(p2*l1):]\n",
    "training_mom_y, val_mom_y, test_mom_y = mom_y[:round(p1*l1)], mom_y[round(p1*l1):round(p2*l1)], mom_y[round(p2*l1):]\n",
    "\n",
    "training_std_centroids, val_std_centroids, test_std_centroids = std_centroids[:round(p1*l1)], std_centroids[round(p1*l1):round(p2*l1)], std_centroids[round(p2*l1):]\n",
    "\n",
    "print(training_kp.shape, val_kp.shape, test_kp.shape)\n",
    "print(training_lbl.shape, val_lbl.shape, test_lbl.shape)\n",
    "print(training_lengths.shape, val_lengths.shape, test_lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(437,) (54,)\n",
      "(437,) (54,)\n"
     ]
    }
   ],
   "source": [
    "training_outscale = np.asarray(norm_uniform(training_lbl)).squeeze()\n",
    "val_outscale_t = np.asarray(norm_uniform(val_lbl)).squeeze()\n",
    "test_outscale_t = np.asarray(norm_uniform(test_lbl)).squeeze()\n",
    "print(training_outscale.shape, val_outscale_t.shape)\n",
    "\n",
    "training_mom_z = np.asarray(normalize(training_lbl)[0])[:, 0, 1]\n",
    "val_mom_z_t = np.asarray(normalize(val_lbl)[0])[:, 0, 1]\n",
    "test_mom_z_t = np.asarray(normalize(test_lbl)[0])[:, 0, 1]\n",
    "print(training_mom_z.shape, val_mom_z_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54,) (55,)\n",
      "(54,) (55,)\n"
     ]
    }
   ],
   "source": [
    "alpha = LinearRegression(fit_intercept=True)#make_pipeline(PolynomialFeatures(9), LinearRegression(fit_intercept=True))\n",
    "alpha.fit(training_inpscale, training_outscale)\n",
    "val_outscale = alpha.predict(val_inpscale)\n",
    "test_outscale = alpha.predict(test_inpscale)\n",
    "print(val_outscale.shape, test_outscale.shape)\n",
    "\n",
    "sigma = LinearRegression(fit_intercept=True)#make_pipeline(PolynomialFeatures(9), LinearRegression(fit_intercept=True))\n",
    "train_X = np.concatenate((training_mom_x[:,1, np.newaxis], training_mom_y[:,1, np.newaxis]), axis=1)\n",
    "sigma.fit(train_X, training_mom_z)\n",
    "val_X = np.concatenate((val_mom_x[:,1,np.newaxis], val_mom_y[:,1,np.newaxis]), axis=1)\n",
    "test_X = np.concatenate((test_mom_x[:,1,np.newaxis], test_mom_y[:,1,np.newaxis]), axis=1)\n",
    "val_mom_z = sigma.predict(val_X)\n",
    "test_mom_z = sigma.predict(test_X)\n",
    "print(val_mom_z.shape, test_mom_z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356b1826ba8e4e929d06bcddf3508533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f84c30ab450>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(training_inpscale[:,0], training_inpscale[:,1], training_outscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abf1fbc1e5043a99017c7e1b2510701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f84c582b4d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(training_std_centroids[:,0], training_std_centroids[:,1], training_mom_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d83a69d2ea46c99008a1848fc0a8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f84c56c7d90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(training_std_centroids[:,0], training_mom_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc6da98ef7a4ef3ad306b1086c6b678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f84c563f750>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(training_std_centroids[:,1], training_mom_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.62384328 26.08538707 18.5024789  14.63275937 15.17040225 12.61684342\n",
      " 28.32054826 33.18518587  7.45790339 16.08633951 13.47029407 24.54818245\n",
      " 18.05205021 35.58875122 22.1165948  24.16025418 12.23181917  8.17372776\n",
      " 16.78427015 21.27701073 11.21396596 10.55478739  8.48003442  8.76737949\n",
      "  7.37904394 12.34551908 17.13794177 11.0348657   7.61679276 19.30972979\n",
      " 23.20268102 15.05980711 24.87636407 15.08491216 10.84961533 18.69314493\n",
      " 25.24884071 24.50221987 10.59788944 13.35574911  7.15747334 11.08227538\n",
      " 25.82168262 11.24886873 22.21573887 12.24963959 24.17715013 14.98801861\n",
      " 23.48074623 17.77313133 24.30745926 13.12753734 10.7803098  14.2239586 ] [19.24075769 21.0865633  10.51266415 20.53859643 12.69762307 17.78786856\n",
      " 21.44638365 11.20541857 12.57880625 14.35516222 12.54869054 23.396771\n",
      " 12.41302235 10.87776973 12.41076713 18.35837224 16.78480256 16.74440396\n",
      " 15.0272793  16.86119019 17.54361432 39.01528193 19.35060923 13.16720597\n",
      "  8.0185736  17.91809464 15.47676804 22.62052017 16.72814386 12.76572259\n",
      " 16.66727925 15.27762108  9.20123269  8.83975844 18.94667009 11.64810686\n",
      " 15.41057945 17.26431918 15.74763653 38.30149502 20.66512369 14.90036758\n",
      " 15.33175659 13.52480761 17.9892532  15.85695743 11.33822668 10.57194747\n",
      " 22.27412715 16.05367896 15.15751161 18.28291155 10.00512298 17.94711892\n",
      " 10.8179405 ]\n",
      "[0.4723732  1.54204976 0.59657423 0.9148634  0.99890928 1.22548283\n",
      " 1.15422429 1.32770354 0.63024694 0.71459974 1.20995201 1.80635095\n",
      " 0.66942731 1.23725844 0.9738509  0.72542149 1.11527425 0.76037494\n",
      " 1.24134014 1.05548991 0.71606132 1.30930475 0.7794647  0.98638868\n",
      " 1.08206669 0.8912449  1.20505911 0.79860304 1.28524307 0.90455415\n",
      " 1.2406735  0.9383087  1.75836134 0.61009056 0.88782627 1.10942463\n",
      " 1.02140059 1.65455946 0.41338212 1.43277315 0.98695496 0.73666626\n",
      " 2.01298988 0.52759696 1.11915205 1.15853784 1.6235698  0.60991965\n",
      " 1.50833436 0.62777751 0.98313531 1.00309304 0.87113113 1.11540995] [0.47980792 1.0491568  0.81395087 1.04598833 1.13701238 0.94987878\n",
      " 1.55418233 0.49348367 0.92023416 1.32116955 1.0181709  0.98737166\n",
      " 1.01723281 0.85357294 0.98755484 1.20410785 1.03225214 1.40622862\n",
      " 0.5483837  0.97860412 1.76419707 1.07901883 0.59620928 0.91536698\n",
      " 0.88386684 1.22960247 1.04761822 1.24103832 0.83163559 1.00150782\n",
      " 1.1099582  0.83717395 1.17122082 0.96876727 1.16569625 0.91728485\n",
      " 1.04937732 0.93704675 0.86489642 1.4482194  0.68956755 1.20171023\n",
      " 1.08441994 0.75815412 0.97300799 0.97795424 0.79784008 1.15455858\n",
      " 1.58166116 0.68094293 1.19314637 1.12302832 0.57740247 1.23799862\n",
      " 1.07912459]\n",
      "42.93993769475547 27.134923392539324\n",
      "0.12486275697607606 0.06946505598833914\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(val_outscale_t, test_outscale_t)\n",
    "print(val_mom_z_t, test_mom_z_t)\n",
    "print(mean_squared_error(val_outscale, val_outscale_t), mean_squared_error(test_outscale, test_outscale_t))\n",
    "print(mean_squared_error(val_mom_z, val_mom_z_t), mean_squared_error(test_mom_z, test_mom_z_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54, 2) (55, 2)\n"
     ]
    }
   ],
   "source": [
    "val_outscale = np.concatenate((val_outscale[:,np.newaxis], val_outscale_t[:,np.newaxis]), axis=1)\n",
    "test_outscale = np.concatenate((test_outscale[:,np.newaxis], test_outscale_t[:,np.newaxis]), axis=1)\n",
    "\n",
    "val_mom_z = np.concatenate((val_mom_z[:,np.newaxis], val_mom_z_t[:,np.newaxis]), axis=1)\n",
    "test_mom_z = np.concatenate((test_mom_z[:,np.newaxis], test_mom_z_t[:,np.newaxis]), axis=1)\n",
    "print(val_outscale.shape, test_mom_z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([437, 16, 84]) torch.Size([54, 16, 42]) torch.Size([55])\n",
      "torch.Size([437, 2]) torch.Size([437])\n"
     ]
    }
   ],
   "source": [
    "# From python lists to pytorch tensors.\n",
    "training_kp, val_kp, test_kp = torch.tensor(np.nan_to_num(training_kp), dtype=torch.float32), torch.tensor(np.nan_to_num(val_kp), dtype=torch.float32), torch.tensor(np.nan_to_num(test_kp), dtype=torch.float32)\n",
    "training_lbl, val_lbl, test_lbl = torch.tensor(np.nan_to_num(training_lbl), dtype=torch.float32), torch.tensor(np.nan_to_num(val_lbl), dtype=torch.float32), torch.tensor(np.nan_to_num(test_lbl), dtype=torch.float32)\n",
    "training_lengths, val_lengths, test_lengths = torch.tensor(np.nan_to_num(training_lengths), dtype=torch.float32), torch.tensor(np.nan_to_num(val_lengths), dtype=torch.float32), torch.tensor(np.nan_to_num(test_lengths), dtype=torch.float32)\n",
    "training_inpscale, val_inpscale, test_inpscale = torch.tensor(np.nan_to_num(training_inpscale), dtype=torch.float32), torch.tensor(np.nan_to_num(val_inpscale), dtype=torch.float32), torch.tensor(np.nan_to_num(test_inpscale), dtype=torch.float32)\n",
    "training_outscale, val_outscale, test_outscale = torch.tensor(np.nan_to_num(training_outscale), dtype=torch.float32), torch.tensor(np.nan_to_num(val_outscale), dtype=torch.float32), torch.tensor(np.nan_to_num(test_outscale), dtype=torch.float32)\n",
    "training_mom_x, val_mom_x, test_mom_x = torch.tensor(np.nan_to_num(training_mom_x), dtype=torch.float32), torch.tensor(np.nan_to_num(val_mom_x), dtype=torch.float32), torch.tensor(np.nan_to_num(test_mom_x), dtype=torch.float32)\n",
    "training_mom_y, val_mom_y, test_mom_y = torch.tensor(np.nan_to_num(training_mom_y), dtype=torch.float32), torch.tensor(np.nan_to_num(val_mom_y), dtype=torch.float32), torch.tensor(np.nan_to_num(test_mom_y), dtype=torch.float32)\n",
    "training_mom_z, val_mom_z, test_mom_z = torch.tensor(np.nan_to_num(training_mom_z), dtype=torch.float32), torch.tensor(np.nan_to_num(val_mom_z), dtype=torch.float32), torch.tensor(np.nan_to_num(test_mom_z), dtype=torch.float32)\n",
    "\n",
    "print(training_kp.shape, val_lbl.shape, test_lengths.shape)\n",
    "print(training_inpscale.shape, training_outscale.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we define the batch_size and put the datasets in DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f90cd1ab3d0>\n"
     ]
    }
   ],
   "source": [
    "train_data = TensorDataset(training_kp, training_lbl, training_lengths,\n",
    "                          training_inpscale, training_outscale,\n",
    "                          training_mom_x, training_mom_y, training_mom_z)\n",
    "val_data = TensorDataset(val_kp, val_lbl, val_lengths,\n",
    "                        val_inpscale, val_outscale,\n",
    "                        val_mom_x, val_mom_y, val_mom_z)\n",
    "test_data = TensorDataset(test_kp, test_lbl, test_lengths,\n",
    "                         test_inpscale, test_outscale,\n",
    "                         test_mom_x, test_mom_y, test_mom_z)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a GPU available we set our device to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print some examples to see whether it is loaded correctly or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16, 84]) torch.Size([32, 16, 42]) torch.Size([32]) torch.Size([32, 2]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y, sample_len, iscale, oscale, momx, momy, momz = dataiter.next()\n",
    "\n",
    "print(sample_x.shape, sample_y.shape, sample_len.shape, iscale.shape, momz.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building\n",
    "It is time to build the model for this approach. It will consist on a single/double layer LSTM followed by a Linear layer with output size the number of keypoints we want to estimate. I also define a method to initialize the hidden_state of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_2D3D(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, bidirectional, dropout=0.):\n",
    "        super().__init__()\n",
    "        # Save the model parameters\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bi = bidirectional\n",
    "        \n",
    "        # Define the architecture\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*(2 if self.bi else 1), output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, state, lengths):\n",
    "        # Describe the forward step\n",
    "        batch_size, seq_len = x.size(0), x.size(1) # We save the batch size and the (maximum) sequence length\n",
    "        \n",
    "        # Need to pack a tensor containing padded sequences of variable length\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths=lengths, batch_first=True, enforce_sorted=False)\n",
    "        ht, hidden_state = self.lstm(packed, state) # ht will be a PackedSequence\n",
    "\n",
    "        # Need to flatten and reshape the output to feed it to the Linear layer\n",
    "        ht = ht.data.contiguous() # ht will be of shape [sum(lengths), hidden_dim]\n",
    "        ot = self.fc(ht) # ot will be of shape [sum(lengths), ouput_size]\n",
    "\n",
    "        l_ot = [ot[:int(length)] for length in lengths] # list of batch elements, each shape [lengths[i], output_size]\n",
    "        packed_ot = nn.utils.rnn.pack_sequence(l_ot, enforce_sorted=False) # PackedSequence\n",
    "        # Finally return to shape [batch_size, seq_len, output_size]\n",
    "        ot, _ = nn.utils.rnn.pad_packed_sequence(packed_ot, batch_first=True, total_length=seq_len)\n",
    "        \n",
    "        return ot, hidden_state\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers*(2 if self.bi else 1), batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers*(2 if self.bi else 1), batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_2D3D(\n",
      "  (lstm): LSTM(84, 512, num_layers=2, batch_first=True)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=42, bias=True)\n",
      "  )\n",
      ")\n",
      "3347498\n"
     ]
    }
   ],
   "source": [
    "# Define some model parameters\n",
    "INPUT_SIZE = sample_x.size(2)\n",
    "OUTPUT_SIZE = sample_y.size(2)\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = False\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTM_2D3D(INPUT_SIZE, OUTPUT_SIZE, HIDDEN_DIM, N_LAYERS, BIDIRECTIONAL, dropout=0.)\n",
    "model.to(device)\n",
    "print(model)\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now we will proceed with the training. The first cell will define the learning rate, the loss function and the selected optimizer for the training process. Then we will proceed with a training over a number of epochs in which we will print it's training loss and validation loss. I also will be using Tensorboard to have a much nicer view of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substract_root_PJPE(output):\n",
    "    jhl, jhr = torch.chunk(output[:, :, :21], max_seq, dim=1), torch.chunk(output[:, :, 21:], max_seq, dim=1)\n",
    "    joints_merged = []\n",
    "    roots = [0, 0]\n",
    "    for i, joints in enumerate((jhl, jhr)):\n",
    "        n_joints = []\n",
    "        for chunk in joints:\n",
    "            n_joints.append(chunk.sub(chunk[:,:,roots[i]].unsqueeze(2)))\n",
    "        joints_merged.append(torch.cat(tuple(n_joints), dim=1))\n",
    "    joints_merged = torch.cat(tuple(joints_merged), dim=2)\n",
    "    return joints_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpjpe(rooted_o, rooted_l, seq_lens):\n",
    "    MPJPE = []\n",
    "    for i in range(len(seq_lens)):\n",
    "        MPJPE.append(rooted_o[i,:int(seq_lens[i])].sub(rooted_l[i,:int(seq_lens[i])]).abs().mean().item())\n",
    "    \n",
    "    return np.mean(MPJPE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_keypoints(output, labels, seq_lens, alpha=0.1):\n",
    "    total_correct = 0\n",
    "    for i in range(len(seq_lens)):\n",
    "        for j in range(int(seq_lens[i])):\n",
    "            rang = (labels[i,j].max()-labels[i,j].min())\n",
    "            for o, l in zip(output[i,j], labels[i,j]):\n",
    "                if o < l+alpha*rang and o > l-alpha*rang:\n",
    "                    total_correct += 1\n",
    "    return total_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 80\n",
    "lr = 5e-6\n",
    "loss_function = nn.MSELoss()\n",
    "one_cycle = True\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "if one_cycle:\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, \n",
    "                                                  steps_per_epoch=len(train_loader), epochs=NUM_EPOCHS,\n",
    "                                                  div_factor=25.0, final_div_factor=10000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "name = 'hands_whole'\n",
    "writer = SummaryWriter(log_dir=f'/deeplearning/logs/{name}{datetime.now()}_lr-{lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 1/80 in 25.68s.\n",
      " Loss: 1.5967  Val Loss: 1.7918\n",
      " Train MPJPE: 0.8104  Val MPJPE: 0.7221\n",
      " Train PCK: 16.084  Val PCK: 14.407\n",
      "Finished epoch 2/80 in 25.85s.\n",
      " Loss: 1.5892  Val Loss: 1.7719\n",
      " Train MPJPE: 0.8121  Val MPJPE: 0.7559\n",
      " Train PCK: 16.372  Val PCK: 15.183\n",
      "Finished epoch 3/80 in 25.66s.\n",
      " Loss: 1.5929  Val Loss: 1.7041\n",
      " Train MPJPE: 0.8044  Val MPJPE: 0.7724\n",
      " Train PCK: 16.122  Val PCK: 15.485\n",
      "Finished epoch 4/80 in 25.71s.\n",
      " Loss: 1.5983  Val Loss: 1.7692\n",
      " Train MPJPE: 0.8022  Val MPJPE: 0.7432\n",
      " Train PCK: 16.179  Val PCK: 15.216\n",
      "Finished epoch 5/80 in 25.59s.\n",
      " Loss: 1.5921  Val Loss: 1.6901\n",
      " Train MPJPE: 0.8078  Val MPJPE: 0.7619\n",
      " Train PCK: 16.201  Val PCK: 15.392\n",
      "Finished epoch 6/80 in 25.57s.\n",
      " Loss: 1.6029  Val Loss: 1.7044\n",
      " Train MPJPE: 0.8043  Val MPJPE: 0.7984\n",
      " Train PCK: 16.007  Val PCK: 16.574\n",
      "Finished epoch 7/80 in 25.71s.\n",
      " Loss: 1.5949  Val Loss: 1.8828\n",
      " Train MPJPE: 0.8020  Val MPJPE: 0.7687\n",
      " Train PCK: 16.182  Val PCK: 14.862\n",
      "Finished epoch 8/80 in 25.59s.\n",
      " Loss: 1.5960  Val Loss: 1.7449\n",
      " Train MPJPE: 0.8003  Val MPJPE: 0.6980\n",
      " Train PCK: 16.128  Val PCK: 15.025\n",
      "Finished epoch 9/80 in 25.65s.\n",
      " Loss: 1.5739  Val Loss: 1.9206\n",
      " Train MPJPE: 0.8107  Val MPJPE: 0.6974\n",
      " Train PCK: 16.334  Val PCK: 14.565\n",
      "Finished epoch 10/80 in 25.90s.\n",
      " Loss: 1.5957  Val Loss: 1.6189\n",
      " Train MPJPE: 0.8046  Val MPJPE: 0.7752\n",
      " Train PCK: 16.179  Val PCK: 16.360\n",
      "Finished epoch 11/80 in 25.95s.\n",
      " Loss: 1.5803  Val Loss: 1.7475\n",
      " Train MPJPE: 0.8037  Val MPJPE: 0.8222\n",
      " Train PCK: 16.277  Val PCK: 16.564\n",
      "Finished epoch 12/80 in 25.90s.\n",
      " Loss: 1.5910  Val Loss: 1.9622\n",
      " Train MPJPE: 0.7991  Val MPJPE: 0.7194\n",
      " Train PCK: 16.206  Val PCK: 14.616\n",
      "Finished epoch 13/80 in 25.78s.\n",
      " Loss: 1.5701  Val Loss: 1.6971\n",
      " Train MPJPE: 0.8016  Val MPJPE: 0.8195\n",
      " Train PCK: 16.345  Val PCK: 16.109\n",
      "Finished epoch 14/80 in 25.88s.\n",
      " Loss: 1.5936  Val Loss: 1.7647\n",
      " Train MPJPE: 0.8005  Val MPJPE: 0.7030\n",
      " Train PCK: 16.124  Val PCK: 14.337\n",
      "Finished epoch 15/80 in 25.99s.\n",
      " Loss: 1.5762  Val Loss: 1.8605\n",
      " Train MPJPE: 0.8002  Val MPJPE: 0.7753\n",
      " Train PCK: 16.388  Val PCK: 15.253\n",
      "Finished epoch 16/80 in 25.72s.\n",
      " Loss: 1.5765  Val Loss: 1.7422\n",
      " Train MPJPE: 0.8010  Val MPJPE: 0.8357\n",
      " Train PCK: 16.211  Val PCK: 15.337\n",
      "Finished epoch 17/80 in 25.88s.\n",
      " Loss: 1.5867  Val Loss: 1.7686\n",
      " Train MPJPE: 0.7893  Val MPJPE: 0.7414\n",
      " Train PCK: 16.159  Val PCK: 15.072\n",
      "Finished epoch 18/80 in 25.87s.\n",
      " Loss: 1.5859  Val Loss: 1.7859\n",
      " Train MPJPE: 0.7885  Val MPJPE: 0.7407\n",
      " Train PCK: 16.119  Val PCK: 14.634\n",
      "Finished epoch 19/80 in 25.98s.\n",
      " Loss: 1.5756  Val Loss: 1.8447\n",
      " Train MPJPE: 0.7839  Val MPJPE: 0.7431\n",
      " Train PCK: 16.098  Val PCK: 14.793\n",
      "Finished epoch 20/80 in 25.76s.\n",
      " Loss: 1.5701  Val Loss: 1.8594\n",
      " Train MPJPE: 0.7861  Val MPJPE: 0.7580\n",
      " Train PCK: 16.276  Val PCK: 14.779\n",
      "Finished epoch 21/80 in 26.40s.\n",
      " Loss: 1.5623  Val Loss: 1.8679\n",
      " Train MPJPE: 0.7852  Val MPJPE: 0.6845\n",
      " Train PCK: 16.329  Val PCK: 13.444\n",
      "Finished epoch 22/80 in 25.68s.\n",
      " Loss: 1.5690  Val Loss: 1.8874\n",
      " Train MPJPE: 0.7781  Val MPJPE: 0.7478\n",
      " Train PCK: 16.309  Val PCK: 15.365\n",
      "Finished epoch 23/80 in 25.99s.\n",
      " Loss: 1.5582  Val Loss: 1.7521\n",
      " Train MPJPE: 0.7774  Val MPJPE: 0.7845\n",
      " Train PCK: 16.191  Val PCK: 15.918\n",
      "Finished epoch 24/80 in 25.80s.\n",
      " Loss: 1.5508  Val Loss: 1.7385\n",
      " Train MPJPE: 0.7695  Val MPJPE: 0.7474\n",
      " Train PCK: 16.392  Val PCK: 15.225\n",
      "Finished epoch 25/80 in 25.83s.\n",
      " Loss: 1.5412  Val Loss: 1.8055\n",
      " Train MPJPE: 0.7656  Val MPJPE: 0.7703\n",
      " Train PCK: 16.369  Val PCK: 15.737\n",
      "Finished epoch 26/80 in 26.07s.\n",
      " Loss: 1.5255  Val Loss: 1.7743\n",
      " Train MPJPE: 0.7616  Val MPJPE: 0.6973\n",
      " Train PCK: 16.413  Val PCK: 15.113\n",
      "Finished epoch 27/80 in 25.86s.\n",
      " Loss: 1.5251  Val Loss: 1.6687\n",
      " Train MPJPE: 0.7541  Val MPJPE: 0.8033\n",
      " Train PCK: 16.540  Val PCK: 16.969\n",
      "Finished epoch 28/80 in 26.08s.\n",
      " Loss: 1.5242  Val Loss: 1.7649\n",
      " Train MPJPE: 0.7472  Val MPJPE: 0.7432\n",
      " Train PCK: 16.543  Val PCK: 14.518\n",
      "Finished epoch 29/80 in 26.03s.\n",
      " Loss: 1.5108  Val Loss: 1.7442\n",
      " Train MPJPE: 0.7353  Val MPJPE: 0.7517\n",
      " Train PCK: 16.588  Val PCK: 14.820\n",
      "Finished epoch 30/80 in 26.27s.\n",
      " Loss: 1.5063  Val Loss: 2.0246\n",
      " Train MPJPE: 0.7197  Val MPJPE: 0.6709\n",
      " Train PCK: 16.394  Val PCK: 13.783\n",
      "Finished epoch 31/80 in 26.29s.\n",
      " Loss: 1.4798  Val Loss: 1.8054\n",
      " Train MPJPE: 0.7094  Val MPJPE: 0.7275\n",
      " Train PCK: 16.593  Val PCK: 13.923\n",
      "Finished epoch 32/80 in 26.45s.\n",
      " Loss: 1.4692  Val Loss: 1.7041\n",
      " Train MPJPE: 0.7071  Val MPJPE: 0.7845\n",
      " Train PCK: 16.974  Val PCK: 14.820\n",
      "Finished epoch 33/80 in 26.26s.\n",
      " Loss: 1.4756  Val Loss: 1.8993\n",
      " Train MPJPE: 0.6981  Val MPJPE: 0.7478\n",
      " Train PCK: 16.778  Val PCK: 14.021\n",
      "Finished epoch 34/80 in 26.63s.\n",
      " Loss: 1.4689  Val Loss: 1.6672\n",
      " Train MPJPE: 0.6903  Val MPJPE: 0.7926\n",
      " Train PCK: 16.753  Val PCK: 17.555\n",
      "Finished epoch 35/80 in 26.51s.\n",
      " Loss: 1.4496  Val Loss: 1.6991\n",
      " Train MPJPE: 0.6955  Val MPJPE: 0.8265\n",
      " Train PCK: 16.937  Val PCK: 16.495\n",
      "Finished epoch 36/80 in 26.60s.\n",
      " Loss: 1.4440  Val Loss: 1.8169\n",
      " Train MPJPE: 0.6870  Val MPJPE: 0.6826\n",
      " Train PCK: 17.043  Val PCK: 14.095\n",
      "Finished epoch 37/80 in 26.75s.\n",
      " Loss: 1.4518  Val Loss: 1.7278\n",
      " Train MPJPE: 0.6886  Val MPJPE: 0.7538\n",
      " Train PCK: 16.928  Val PCK: 15.876\n",
      "Finished epoch 38/80 in 26.64s.\n",
      " Loss: 1.4334  Val Loss: 1.7795\n",
      " Train MPJPE: 0.6899  Val MPJPE: 0.6982\n",
      " Train PCK: 17.152  Val PCK: 15.100\n",
      "Finished epoch 39/80 in 26.72s.\n",
      " Loss: 1.4659  Val Loss: 1.8926\n",
      " Train MPJPE: 0.6894  Val MPJPE: 0.7391\n",
      " Train PCK: 16.901  Val PCK: 13.644\n",
      "Finished epoch 40/80 in 26.62s.\n",
      " Loss: 1.4383  Val Loss: 1.7824\n",
      " Train MPJPE: 0.6800  Val MPJPE: 0.6914\n",
      " Train PCK: 17.039  Val PCK: 15.183\n",
      "Finished epoch 41/80 in 26.67s.\n",
      " Loss: 1.4450  Val Loss: 1.7909\n",
      " Train MPJPE: 0.6777  Val MPJPE: 0.7309\n",
      " Train PCK: 17.177  Val PCK: 14.946\n",
      "Finished epoch 42/80 in 26.90s.\n",
      " Loss: 1.4373  Val Loss: 1.6695\n",
      " Train MPJPE: 0.6832  Val MPJPE: 0.8154\n",
      " Train PCK: 17.200  Val PCK: 16.323\n",
      "Finished epoch 43/80 in 26.65s.\n",
      " Loss: 1.4400  Val Loss: 1.8235\n",
      " Train MPJPE: 0.6805  Val MPJPE: 0.7374\n",
      " Train PCK: 16.891  Val PCK: 15.369\n",
      "Finished epoch 44/80 in 27.09s.\n",
      " Loss: 1.4403  Val Loss: 1.8449\n",
      " Train MPJPE: 0.6980  Val MPJPE: 0.7801\n",
      " Train PCK: 17.218  Val PCK: 15.509\n",
      "Finished epoch 45/80 in 26.71s.\n",
      " Loss: 1.4511  Val Loss: 1.8102\n",
      " Train MPJPE: 0.6827  Val MPJPE: 0.7583\n",
      " Train PCK: 16.938  Val PCK: 15.062\n",
      "Finished epoch 46/80 in 26.83s.\n",
      " Loss: 1.4540  Val Loss: 1.9013\n",
      " Train MPJPE: 0.6901  Val MPJPE: 0.7155\n",
      " Train PCK: 16.937  Val PCK: 14.411\n",
      "Finished epoch 47/80 in 26.89s.\n",
      " Loss: 1.4255  Val Loss: 1.8566\n",
      " Train MPJPE: 0.6964  Val MPJPE: 0.7472\n",
      " Train PCK: 16.998  Val PCK: 14.448\n",
      "Finished epoch 48/80 in 26.88s.\n",
      " Loss: 1.4589  Val Loss: 1.8353\n",
      " Train MPJPE: 0.6800  Val MPJPE: 0.7469\n",
      " Train PCK: 16.832  Val PCK: 14.653\n",
      "Finished epoch 49/80 in 26.69s.\n",
      " Loss: 1.4316  Val Loss: 1.7015\n",
      " Train MPJPE: 0.6896  Val MPJPE: 0.7234\n",
      " Train PCK: 17.068  Val PCK: 15.699\n",
      "Finished epoch 50/80 in 26.83s.\n",
      " Loss: 1.4154  Val Loss: 1.6674\n",
      " Train MPJPE: 0.6826  Val MPJPE: 0.7926\n",
      " Train PCK: 17.296  Val PCK: 17.043\n",
      "Finished epoch 51/80 in 26.80s.\n",
      " Loss: 1.4340  Val Loss: 1.7798\n",
      " Train MPJPE: 0.6817  Val MPJPE: 0.7471\n",
      " Train PCK: 17.244  Val PCK: 14.411\n",
      "Finished epoch 52/80 in 26.97s.\n",
      " Loss: 1.4294  Val Loss: 1.8069\n",
      " Train MPJPE: 0.6803  Val MPJPE: 0.7870\n",
      " Train PCK: 17.174  Val PCK: 15.258\n",
      "Finished epoch 53/80 in 26.73s.\n",
      " Loss: 1.4282  Val Loss: 1.7865\n",
      " Train MPJPE: 0.6856  Val MPJPE: 0.7920\n",
      " Train PCK: 17.370  Val PCK: 16.881\n",
      "Finished epoch 54/80 in 26.89s.\n",
      " Loss: 1.4315  Val Loss: 1.8739\n",
      " Train MPJPE: 0.6817  Val MPJPE: 0.7232\n",
      " Train PCK: 17.099  Val PCK: 15.058\n",
      "Finished epoch 55/80 in 26.69s.\n",
      " Loss: 1.4546  Val Loss: 1.8411\n",
      " Train MPJPE: 0.6861  Val MPJPE: 0.6851\n",
      " Train PCK: 17.072  Val PCK: 14.351\n",
      "Finished epoch 56/80 in 26.82s.\n",
      " Loss: 1.4339  Val Loss: 1.9154\n",
      " Train MPJPE: 0.6900  Val MPJPE: 0.6315\n",
      " Train PCK: 17.164  Val PCK: 14.207\n",
      "Finished epoch 57/80 in 27.11s.\n",
      " Loss: 1.4312  Val Loss: 1.8026\n",
      " Train MPJPE: 0.6773  Val MPJPE: 0.7202\n",
      " Train PCK: 17.037  Val PCK: 15.365\n",
      "Finished epoch 58/80 in 27.06s.\n",
      " Loss: 1.4416  Val Loss: 1.7721\n",
      " Train MPJPE: 0.6868  Val MPJPE: 0.7934\n",
      " Train PCK: 17.002  Val PCK: 16.071\n",
      "Finished epoch 59/80 in 26.90s.\n",
      " Loss: 1.4370  Val Loss: 1.7092\n",
      " Train MPJPE: 0.6826  Val MPJPE: 0.7311\n",
      " Train PCK: 17.106  Val PCK: 14.886\n",
      "Finished epoch 60/80 in 26.65s.\n",
      " Loss: 1.4345  Val Loss: 1.8454\n",
      " Train MPJPE: 0.6852  Val MPJPE: 0.7353\n",
      " Train PCK: 17.203  Val PCK: 14.718\n",
      "Finished epoch 61/80 in 26.78s.\n",
      " Loss: 1.4532  Val Loss: 1.6659\n",
      " Train MPJPE: 0.6789  Val MPJPE: 0.8164\n",
      " Train PCK: 16.973  Val PCK: 15.937\n",
      "Finished epoch 62/80 in 26.79s.\n",
      " Loss: 1.4362  Val Loss: 1.8030\n",
      " Train MPJPE: 0.6802  Val MPJPE: 0.7421\n",
      " Train PCK: 17.070  Val PCK: 16.416\n",
      "Finished epoch 63/80 in 26.85s.\n",
      " Loss: 1.4554  Val Loss: 1.9147\n",
      " Train MPJPE: 0.6841  Val MPJPE: 0.7125\n",
      " Train PCK: 17.023  Val PCK: 14.844\n",
      "Finished epoch 64/80 in 26.65s.\n",
      " Loss: 1.4447  Val Loss: 1.8755\n",
      " Train MPJPE: 0.6791  Val MPJPE: 0.7428\n",
      " Train PCK: 17.010  Val PCK: 15.067\n",
      "Finished epoch 65/80 in 26.74s.\n",
      " Loss: 1.4417  Val Loss: 1.9615\n",
      " Train MPJPE: 0.6835  Val MPJPE: 0.6870\n",
      " Train PCK: 17.002  Val PCK: 13.439\n",
      "Finished epoch 66/80 in 26.62s.\n",
      " Loss: 1.4369  Val Loss: 1.9215\n",
      " Train MPJPE: 0.6805  Val MPJPE: 0.6832\n",
      " Train PCK: 17.142  Val PCK: 14.397\n",
      "Finished epoch 67/80 in 26.70s.\n",
      " Loss: 1.4654  Val Loss: 1.8351\n",
      " Train MPJPE: 0.6836  Val MPJPE: 0.7433\n",
      " Train PCK: 16.868  Val PCK: 15.458\n",
      "Finished epoch 68/80 in 27.06s.\n",
      " Loss: 1.4219  Val Loss: 1.7099\n",
      " Train MPJPE: 0.6723  Val MPJPE: 0.7604\n",
      " Train PCK: 17.272  Val PCK: 16.416\n",
      "Finished epoch 69/80 in 27.04s.\n",
      " Loss: 1.4242  Val Loss: 1.6446\n",
      " Train MPJPE: 0.6849  Val MPJPE: 0.7908\n",
      " Train PCK: 16.904  Val PCK: 17.150\n",
      "Finished epoch 70/80 in 27.44s.\n",
      " Loss: 1.4345  Val Loss: 1.6818\n",
      " Train MPJPE: 0.6863  Val MPJPE: 0.7038\n",
      " Train PCK: 16.986  Val PCK: 16.062\n",
      "Finished epoch 71/80 in 26.78s.\n",
      " Loss: 1.4373  Val Loss: 1.7029\n",
      " Train MPJPE: 0.6810  Val MPJPE: 0.7920\n",
      " Train PCK: 17.132  Val PCK: 15.327\n",
      "Finished epoch 72/80 in 26.99s.\n",
      " Loss: 1.4164  Val Loss: 1.9044\n",
      " Train MPJPE: 0.6753  Val MPJPE: 0.7192\n",
      " Train PCK: 17.261  Val PCK: 14.346\n",
      "Finished epoch 73/80 in 26.86s.\n",
      " Loss: 1.4404  Val Loss: 1.7432\n",
      " Train MPJPE: 0.6861  Val MPJPE: 0.7723\n",
      " Train PCK: 17.248  Val PCK: 14.844\n",
      "Finished epoch 74/80 in 27.00s.\n",
      " Loss: 1.4277  Val Loss: 1.6536\n",
      " Train MPJPE: 0.6794  Val MPJPE: 0.8199\n",
      " Train PCK: 17.434  Val PCK: 17.322\n",
      "Finished epoch 75/80 in 26.71s.\n",
      " Loss: 1.4373  Val Loss: 1.8303\n",
      " Train MPJPE: 0.6807  Val MPJPE: 0.7275\n",
      " Train PCK: 17.053  Val PCK: 15.946\n",
      "Finished epoch 76/80 in 26.91s.\n",
      " Loss: 1.4300  Val Loss: 1.7990\n",
      " Train MPJPE: 0.6799  Val MPJPE: 0.7401\n",
      " Train PCK: 17.188  Val PCK: 14.169\n",
      "Finished epoch 77/80 in 26.74s.\n",
      " Loss: 1.4483  Val Loss: 1.7706\n",
      " Train MPJPE: 0.6862  Val MPJPE: 0.6644\n",
      " Train PCK: 17.061  Val PCK: 15.016\n",
      "Finished epoch 78/80 in 26.92s.\n",
      " Loss: 1.4402  Val Loss: 1.6647\n",
      " Train MPJPE: 0.6806  Val MPJPE: 0.7136\n",
      " Train PCK: 17.029  Val PCK: 15.485\n",
      "Finished epoch 79/80 in 28.02s.\n",
      " Loss: 1.4454  Val Loss: 1.8600\n",
      " Train MPJPE: 0.6855  Val MPJPE: 0.7219\n",
      " Train PCK: 16.900  Val PCK: 14.574\n",
      "Finished epoch 80/80 in 29.63s.\n",
      " Loss: 1.4388  Val Loss: 1.7135\n",
      " Train MPJPE: 0.6885  Val MPJPE: 0.7438\n",
      " Train PCK: 17.135  Val PCK: 15.811\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdec460fec8046678ecfaaa5c12340c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f90cc6f7b90>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer_beg = timer()\n",
    "\n",
    "tr_losses = []\n",
    "val_losses = []\n",
    "\n",
    "model.train()\n",
    "for i in range(NUM_EPOCHS):\n",
    "    # Init the hidden state (ht, ct)\n",
    "    h = model.init_hidden(batch_size)\n",
    "    batch_losses = []\n",
    "    train_MPJPE = []\n",
    "    train_CK = [0, 0]\n",
    "    \n",
    "    if i+1 == NUM_EPOCHS:\n",
    "        preds, inps, labls, lens = [], [], [], []\n",
    "        val_preds, val_inps, val_labls, val_lens = [], [], [], []\n",
    "        iscale, oscale, val_iscale, val_oscale = [], [], [], []\n",
    "        momx, momy, momz = [], [], []\n",
    "        val_momx, val_momy, val_momz = [], [], []\n",
    "        \n",
    "    for inputs, labels, lengths, i_s, o_s, mx, my, mz in train_loader:\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels, lengths = inputs.to(device), labels.to(device), lengths.to(device)\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward step\n",
    "        output, h = model(inputs, h, lengths)\n",
    "        \n",
    "        if i+1 == NUM_EPOCHS:\n",
    "            e = [preds, inps, labls, lens, iscale, oscale, momx, momy, momz]\n",
    "            b = [output, inputs, labels, lengths, i_s, o_s, mx, my, mz]\n",
    "            for k in range(len(e)):\n",
    "                e[k].append(b[k])            \n",
    "\n",
    "        # Loss calculation and backward step\n",
    "        loss = loss_function(nn.utils.rnn.pack_padded_sequence(output, lengths=lengths, batch_first=True, \n",
    "                                                               enforce_sorted=False).data,\n",
    "                             nn.utils.rnn.pack_padded_sequence(labels, lengths=lengths, batch_first=True,\n",
    "                                                               enforce_sorted=False).data)\n",
    "        loss.backward()\n",
    "        # Weight update\n",
    "        optimizer.step()\n",
    "        # One cycle policy step\n",
    "        if one_cycle:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Output data collection for showing\n",
    "        batch_losses.append(loss.item())\n",
    "        rooted_o, rooted_l = substract_root_PJPE(output), substract_root_PJPE(labels)\n",
    "        train_MPJPE.append(mpjpe(rooted_o, rooted_l, lengths))\n",
    "        \n",
    "        train_CK[0] += correct_keypoints(output, labels, lengths)\n",
    "        train_CK[1] += lengths.sum()*labels.shape[2]\n",
    "    \n",
    "    timer_end = timer()\n",
    "    tr_losses.append(np.mean(batch_losses))\n",
    "    writer.add_scalar('Loss/train', tr_losses[-1], i)   \n",
    "    train_MPJPE_total = np.mean(train_MPJPE)\n",
    "    train_PCK = train_CK[0]/train_CK[1]\n",
    "    \n",
    "    # Validation at the end of an epoch\n",
    "    val_h = model.init_hidden(batch_size)\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    val_MPJPE = []\n",
    "    val_CK = [0,0]\n",
    "    \n",
    "    for inp, lab, lns, vis, vos, vmx, vmy, vmz in val_loader:\n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "        inp, lab, lns = inp.to(device), lab.to(device), lns.to(device)\n",
    "        out, val_h = model(inp, val_h, lns)\n",
    "        \n",
    "        if i+1 == NUM_EPOCHS:\n",
    "            e = [val_preds, val_inps, val_labls, val_lens, val_iscale, val_oscale, val_momx, val_momy, val_momz]\n",
    "            b = [out, inp, lab, lns, vis, vos, vmx, vmy, vmz]\n",
    "            for k in range(len(e)):\n",
    "                e[k].append(b[k])  \n",
    "        \n",
    "        loss = loss_function(nn.utils.rnn.pack_padded_sequence(out, lengths=lns, batch_first=True,\n",
    "                                                               enforce_sorted=False).data,\n",
    "                             nn.utils.rnn.pack_padded_sequence(lab, lengths=lns, batch_first=True,\n",
    "                                                               enforce_sorted=False).data)\n",
    "        val_loss.append(loss.item())\n",
    "        rooted_o, rooted_l = substract_root_PJPE(out), substract_root_PJPE(lab)\n",
    "        val_MPJPE.append(mpjpe(rooted_o, rooted_l, lns))\n",
    "        val_CK[0] += correct_keypoints(out, lab, lns)\n",
    "        val_CK[1] += lns.sum()*lab.shape[2]\n",
    "        \n",
    "    val_losses.append(np.mean(val_loss))\n",
    "    writer.add_scalar('Loss/validation', val_losses[-1], i)  \n",
    "    val_MPJPE_total = np.mean(val_MPJPE)\n",
    "    val_PCK = val_CK[0]/val_CK[1] \n",
    "    model.train()\n",
    "    \n",
    "    # Output loss and training time.\n",
    "    print(f\"Finished epoch {i+1}/{NUM_EPOCHS} in {(timer_end-timer_beg):.2f}s.\\n\",\n",
    "             f\"Loss: {np.mean(tr_losses[-1]):.4f}\", f\" Val Loss: {val_losses[-1]:.4f}\\n\",\n",
    "             f\"Train MPJPE: {train_MPJPE_total:.4f}\", f\" Val MPJPE: {val_MPJPE_total:.4f}\\n\",\n",
    "             f\"Train PCK: {train_PCK*100:.3f}\", f\" Val PCK: {val_PCK*100:.3f}\")\n",
    "    timer_beg = timer()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(tr_losses, label='train')\n",
    "plt.plot(val_losses, label='validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSELoss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the predictions for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([384, 2]) torch.Size([384, 16, 84])\n"
     ]
    }
   ],
   "source": [
    "tr_predictions = torch.cat(tuple(preds), dim=0)\n",
    "tr_inputs = torch.cat(tuple(inps), dim=0)\n",
    "tr_groundtruth = torch.cat(tuple(labls), dim=0)\n",
    "tr_lengths = torch.cat(tuple(lens), dim=0)\n",
    "tr_inp_scale, tr_out_scale = torch.cat(tuple(iscale), dim=0), torch.cat(tuple(oscale), dim=0)\n",
    "tr_mx, tr_my, tr_mz = torch.cat(tuple(momx), dim=0), torch.cat(tuple(momy), dim=0), torch.cat(tuple(momz), dim=0)\n",
    "print(tr_inp_scale.shape, tr_inputs.shape)\n",
    "\n",
    "val_predictions = torch.cat(tuple(val_preds), dim=0)\n",
    "val_inputs = torch.cat(tuple(val_inps), dim=0)\n",
    "val_groundtruth = torch.cat(tuple(val_labls), dim=0)\n",
    "val_length = torch.cat(tuple(val_lens), dim=0)\n",
    "val_inp_scale, val_out_scale = torch.cat(tuple(val_iscale), dim=0), torch.cat(tuple(val_oscale), dim=0)\n",
    "val_mx, val_my, val_mz = torch.cat(tuple(val_momx), dim=0), torch.cat(tuple(val_momy), dim=0), torch.cat(tuple(val_momz), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = torch.zeros((138), device='cuda:0')\n",
    "\n",
    "for i in range(len(tr_predictions)):\n",
    "    count=0\n",
    "    for frame in tr_predictions[i, :int(tr_lengths[i])]:\n",
    "        if torch.all(frame.eq(zeros)):\n",
    "            count +=1\n",
    "    print(f'{count}/{int(tr_lengths[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = torch.zeros((138), device='cuda:0')\n",
    "\n",
    "for i in range(len(val_predictions)):\n",
    "    count=0\n",
    "    for frame in tr_predictions[i, :int(val_length[i])]:\n",
    "        if torch.all(frame.eq(zeros)):\n",
    "            count +=1\n",
    "    print(f'{count}/{int(val_length[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'./{name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'./{name}.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "After the training, we shall proceed with the performance test. This will go through the test batches and perform the inference, then it will show the test loss, as well as the performance metric. In this case, as we are working with human body keypoints, we will use the Mean Per Joint Position Error (MPJPE) metric, which outputs the mean euclidean distance between the joints (keypoints) positions estimated and the ones in the groundtruth.\n",
    "\n",
    "The formula for MPJPE is the following:\n",
    "\n",
    "$\\text{MPJPE} = \\frac1T\\frac1N\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{i=1}^{N}\\|(J_{i}^{(t)}-J_{root}^{(t)})-(Ĵ_{i}^{(t)}-Ĵ_{root}^{(t)})\\|$\n",
    "\n",
    "Where N is the number of joints, and T the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the formula above, we need to align the root joints of the labels and the network output. In order to do that, I have defined a function (`substract_root_PJPE`) that substracts the root joint of each keypoint set (face, hands, body) in the corresponding keypoint set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "MPJPE = []\n",
    "h = model.init_hidden(batch_size)\n",
    "preds, inps, labls, lengs = [], [], [], []\n",
    "iscal, oscal = [], []\n",
    "mx, my, mz = [], [], []\n",
    "test_CK = [0,0]\n",
    "\n",
    "model.eval()\n",
    "for inputs_test, labels_test, lengths_test, is_test, os_test, mx_test, my_test, mz_test in test_loader:\n",
    "    \n",
    "    h = tuple([each.data for each in h])\n",
    "    inputs_test, labels_test, lengths_test = inputs_test.to(device), labels_test.to(device), lengths_test.to(device)\n",
    "    \n",
    "    output_test, h = model(inputs_test, h, lengths_test)\n",
    "    \n",
    "    e = [preds, inps, labls, lengs, iscal, oscal, mx, my, mz]\n",
    "    b = [output_test, inputs_test, labels_test, lengths_test, is_test, os_test, mx_test, my_test, mz_test]\n",
    "    for k in range(len(e)):\n",
    "        e[k].append(b[k])      \n",
    "    \n",
    "    test_loss = loss_function(nn.utils.rnn.pack_padded_sequence(output_test, lengths=lengths_test, batch_first=True,\n",
    "                                                               enforce_sorted=False).data,\n",
    "                             nn.utils.rnn.pack_padded_sequence(labels_test, lengths=lengths_test, batch_first=True,\n",
    "                                                               enforce_sorted=False).data)\n",
    "    test_losses.append(test_loss.item())\n",
    "    rooted_o, rooted_l = substract_root_PJPE(output_test), substract_root_PJPE(labels_test)\n",
    "    MPJPE.append(mpjpe(rooted_o, rooted_l, lengths_test))\n",
    "    test_CK[0] += correct_keypoints(output_test, labels_test, lengths_test)\n",
    "    test_CK[1] += lengths_test.sum()*labels_test.shape[2]\n",
    "\n",
    "test_predictions = torch.cat(tuple(preds), dim=0)\n",
    "test_inputs = torch.cat(tuple(inps), dim=0)\n",
    "test_groundtruth = torch.cat(tuple(labls), dim=0)\n",
    "test_inp_scale, test_out_scale = torch.cat(tuple(iscal), dim=0), torch.cat(tuple(oscal), dim=0)\n",
    "test_mx, test_my, test_mz = torch.cat(tuple(mx), dim=0), torch.cat(tuple(my), dim=0), torch.cat(tuple(mz), dim=0)\n",
    "MPJPE_total = np.mean(MPJPE)\n",
    "test_PCK = test_CK[0]/test_CK[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.4158 \n",
      "MPJPE: 1.3015 \n",
      "Test PCK: 22.307\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test loss: {np.mean(test_losses):.4f}\", f\"\\nMPJPE: {MPJPE_total:.4f}\",  f\"\\nTest PCK: {test_PCK*100:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results into a json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'train':{'inputs':tr_inputs.tolist(), 'predictions':tr_predictions.tolist(), 'labels':tr_groundtruth.tolist(), 'lengths':tr_lengths.tolist()},\n",
    "          'validation':{'inputs':val_inputs.tolist(), 'predictions':val_predictions.tolist(), 'labels':val_groundtruth.tolist(), 'lengths':val_length.tolist()},\n",
    "          'test':{'inputs':test_inputs.tolist(), 'predictions':test_predictions.tolist(), 'labels':test_groundtruth.tolist(), 'lengths':test_lengths.tolist()}}\n",
    "with open('../../results/small_hands.json', 'w') as fp:\n",
    "    json.dump(results, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load results from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../results/small_hands.json', 'r') as j:\n",
    "    jd = json.load(j)\n",
    "    tr, val, test = jd['train'], jd['validation'], jd['test']\n",
    "    tr_inputs, tr_predictions, tr_groundtruth, tr_lengths = tuple(torch.tensor(tr[n]) for n in ['inputs', 'predictions',\n",
    "                                                                                              'labels', 'lengths'])\n",
    "    val_inputs, val_predictions, val_groundtruth, val_length = tuple(torch.tensor(val[n]) for n in ['inputs', 'predictions',\n",
    "                                                                                              'labels', 'lengths'])\n",
    "    test_inputs, test_predictions, test_groundtruth, test_lengths = tuple(torch.tensor(test[n]) for n in ['inputs', 'predictions',\n",
    "                                                                                              'labels', 'lengths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 8752, 84]), torch.Size([8, 8752, 42]), torch.Size([8]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_inputs.shape, val_predictions.shape, test_lengths.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to better understanding of the results, I will plot some of the frames from the last batches on the training and validation, and also from testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_rotate(c_inputs, c_z, frames, frame):\n",
    "    c_inputs[:,::2].mul_(mom_x[1])\n",
    "    c_inputs[:,1::2].mul_(mom_y[1])\n",
    "    c_z.mul_(stdz)\n",
    "\n",
    "    handsXY = torch.chunk(c_inputs[frames, :], len(frames), dim=0)\n",
    "    handsZ = torch.chunk(c_z[frames, :], len(frames), dim=0)\n",
    "    \n",
    "    x = handsXY[frame].squeeze()[::2]\n",
    "    y = handsXY[frame].squeeze()[1::2]\n",
    "    z = handsZ[frame].squeeze()\n",
    "\n",
    "    rh0 = [[c[i] for i in [0, 1, 2, 3, 4]] for c in [x,y,z]]\n",
    "    rh1 = [[c[i] for i in [0, 5, 6, 7, 8]] for c in [x,y,z]]\n",
    "    rh2 = [[c[i] for i in [0, 9, 10, 11, 12]] for c in [x,y,z]]\n",
    "    rh3 = [[c[i] for i in [0, 13, 14, 15, 16]] for c in [x,y,z]]\n",
    "    rh4 = [[c[i] for i in [0, 17, 18, 19, 20]] for c in [x,y,z]]\n",
    "    lh0 = [[c[i+21] for i in [0, 1, 2, 3, 4]] for c in [x,y,z]]\n",
    "    lh1 = [[c[i+21] for i in [0, 5, 6, 7, 8]] for c in [x,y,z]]\n",
    "    lh2 = [[c[i+21] for i in [0, 9, 10, 11, 12]] for c in [x,y,z]]\n",
    "    lh3 = [[c[i+21] for i in [0, 13, 14, 15, 16]] for c in [x,y,z]]\n",
    "    lh4 = [[c[i+21] for i in [0, 17, 18, 19, 20]] for c in [x,y,z]]\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    def init():\n",
    "        ax.plot(rh0[0], rh0[2], rh0[1])\n",
    "        ax.plot(rh1[0], rh1[2], rh1[1])\n",
    "        ax.plot(rh2[0], rh2[2], rh2[1])\n",
    "        ax.plot(rh3[0], rh3[2], rh3[1])\n",
    "        ax.plot(rh4[0], rh4[2], rh4[1])\n",
    "        \n",
    "        ax.plot(lh0[0], lh0[2], lh0[1])\n",
    "        ax.plot(lh1[0], lh1[2], lh1[1])\n",
    "        ax.plot(lh2[0], lh2[2], lh2[1])\n",
    "        ax.plot(lh3[0], lh3[2], lh3[1])\n",
    "        ax.plot(lh4[0], lh4[2], lh4[1])\n",
    "        \n",
    "        lims = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()\n",
    "        spans = lims[0][1]-lims[0][0], lims[1][1]-lims[1][0], lims[2][1]-lims[2][0]\n",
    "        span = max(spans)\n",
    "        margins = [(span-s)/2 for  s in spans]\n",
    "        ax.set_xlim(lims[0][0]-margins[0], lims[0][1]+margins[0])\n",
    "        ax.set_ylim(lims[1][0]-margins[1], lims[1][1]+margins[1])\n",
    "        ax.set_zlim(lims[2][0]-margins[2], lims[2][1]+margins[2])\n",
    "\n",
    "        return fig,\n",
    "\n",
    "    def animate(i):\n",
    "        ax.view_init(elev=220., azim=3.6*i)\n",
    "        return fig,\n",
    "\n",
    "    # Animate\n",
    "    ani = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=100, interval=100, blit=True)    \n",
    "\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frames(predictions, groundtruth, inputs, video_n, frames, rot, train=False):\n",
    "    \n",
    "    inp = inputs.clone()\n",
    "    preds = predictions.clone()\n",
    "    grtr = groundtruth.clone()\n",
    "    \n",
    "    handsXY = torch.chunk(inp[video_n, frames, :], len(frames), dim=0)\n",
    "    pred_handsZ = torch.chunk(preds[video_n, frames, :], len(frames), dim=0)\n",
    "    true_handsZ = torch.chunk(grtr[video_n, frames, :], len(frames), dim=0)\n",
    "   \n",
    "    nrows = np.ceil(len(frames)/2)\n",
    "    fig = plt.figure(figsize=(15, 6*nrows))\n",
    "    fig2 = plt.figure(figsize=(15, 6*nrows))\n",
    "    for frame in range(len(frames)):\n",
    "        x = handsXY[frame].squeeze()[::2].tolist()\n",
    "        y = handsXY[frame].squeeze()[1::2].tolist()\n",
    "        pred_z = pred_handsZ[frame].squeeze().tolist()\n",
    "        true_z = true_handsZ[frame].squeeze().tolist()\n",
    "        \n",
    "        r = R.from_euler('y', rot, degrees=True)\n",
    "        \n",
    "        xyz1, xyz2 = np.asarray([c for c in zip(x, y, pred_z)]), np.asarray([c for c in zip(x, y, true_z)])\n",
    "        xyz1, xyz2 = r.apply(xyz1), r.apply(xyz2)\n",
    "        x1, x2 = xyz1[:,0], xyz2[:,0]\n",
    "        y1, y2 = xyz1[:,1], xyz2[:,1]\n",
    "        pred_z, true_z = xyz1[:,2], xyz2[:,2]\n",
    "        if not train:\n",
    "            print((x2.max()-x2.min())/(x1.max()-x1.min()))\n",
    "            x1 = x1*((x2.max()-x2.min())/(x1.max()-x1.min()))\n",
    "        \n",
    "        rh0 = tuple([[c[i] for i in [0, 1, 2, 3, 4]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        rh1 = tuple([[c[i] for i in [0, 5, 6, 7, 8]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        rh2 = tuple([[c[i] for i in [0, 9, 10, 11, 12]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        rh3 = tuple([[c[i] for i in [0, 13, 14, 15, 16]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        rh4 = tuple([[c[i] for i in [0, 17, 18, 19, 20]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        lh0 = tuple([[c[i+21] for i in [0, 1, 2, 3, 4]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        lh1 = tuple([[c[i+21] for i in [0, 5, 6, 7, 8]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        lh2 = tuple([[c[i+21] for i in [0, 9, 10, 11, 12]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        lh3 = tuple([[c[i+21] for i in [0, 13, 14, 15, 16]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        lh4 = tuple([[c[i+21] for i in [0, 17, 18, 19, 20]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        \n",
    "        ax = fig.add_subplot(nrows, 2, frame+1, projection='3d')\n",
    "\n",
    "        ax.plot(rh0[0][0], rh0[0][2], rh0[0][1])\n",
    "        ax.plot(rh1[0][0], rh1[0][2], rh1[0][1])\n",
    "        ax.plot(rh2[0][0], rh2[0][2], rh2[0][1])\n",
    "        ax.plot(rh3[0][0], rh3[0][2], rh3[0][1])\n",
    "        ax.plot(rh4[0][0], rh4[0][2], rh4[0][1])\n",
    "        \n",
    "        ax.plot(lh0[0][0], lh0[0][2], lh0[0][1])\n",
    "        ax.plot(lh1[0][0], lh1[0][2], lh1[0][1])\n",
    "        ax.plot(lh2[0][0], lh2[0][2], lh2[0][1])\n",
    "        ax.plot(lh3[0][0], lh3[0][2], lh3[0][1])\n",
    "        ax.plot(lh4[0][0], lh4[0][2], lh4[0][1])\n",
    "        \n",
    "        ax2 = fig2.add_subplot(nrows, 2, frame+1, projection='3d')\n",
    "        ax2.plot(rh0[1][0], rh0[1][2], rh0[1][1])\n",
    "        ax2.plot(rh1[1][0], rh1[1][2], rh1[1][1])\n",
    "        ax2.plot(rh2[1][0], rh2[1][2], rh2[1][1])\n",
    "        ax2.plot(rh3[1][0], rh3[1][2], rh3[1][1])\n",
    "        ax2.plot(rh4[1][0], rh4[1][2], rh4[1][1])\n",
    "        \n",
    "        ax2.plot(lh0[1][0], lh0[1][2], lh0[1][1])\n",
    "        ax2.plot(lh1[1][0], lh1[1][2], lh1[1][1])\n",
    "        ax2.plot(lh2[1][0], lh2[1][2], lh2[1][1])\n",
    "        ax2.plot(lh3[1][0], lh3[1][2], lh3[1][1])\n",
    "        ax2.plot(lh4[1][0], lh4[1][2], lh4[1][1])\n",
    "        \n",
    "        lims = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()\n",
    "        spans = lims[0][1]-lims[0][0], lims[1][1]-lims[1][0], lims[2][1]-lims[2][0]\n",
    "        span = max(spans)\n",
    "        margins = [(span-s)/2 for  s in spans]\n",
    "        ax.set_xlim(lims[0][0]-margins[0], lims[0][1]+margins[0])\n",
    "        ax.set_ylim(lims[1][0]-margins[1], lims[1][1]+margins[1])\n",
    "        ax.set_zlim(lims[2][0]-margins[2], lims[2][1]+margins[2])\n",
    "        \n",
    "        lims2 = ax2.get_xlim(), ax2.get_ylim(), ax2.get_zlim()\n",
    "        spans2 = lims2[0][1]-lims2[0][0], lims2[1][1]-lims2[1][0], lims2[2][1]-lims2[2][0]\n",
    "        span2 = max(spans2)\n",
    "        margins2 = [(span2-s)/2 for  s in spans2]\n",
    "        ax2.set_xlim(lims2[0][0]-margins2[0], lims2[0][1]+margins2[0])\n",
    "        ax2.set_ylim(lims2[1][0]-margins2[1], lims2[1][1]+margins2[1])\n",
    "        ax2.set_zlim(lims2[2][0]-margins2[2], lims2[2][1]+margins2[2])\n",
    "\n",
    "        ax.view_init(elev=-65., azim=-90.)\n",
    "        ax2.view_init(elev=-65., azim=-90.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single frame\n",
    "On the first cell you can select which frames you want to plot and from which video of the batch. On the second you select which frame of the previosly selected you want to plot, specifying its index on the declared \"frames\" list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last batches of training -output, inputs, labels-.\n",
    "vid = 1\n",
    "frames = [100]\n",
    "\n",
    "c_inputs = training_kp[vid].clone()\n",
    "c_labels = training_lbl[vid].clone()\n",
    "c_inputs[:,::2].mul_(training_inpscale[vid, 0])\n",
    "c_inputs[:,1::2].mul_(training_inpscale[vid, 1])\n",
    "c_labels.mul_(training_outscale[vid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(plot_and_rotate(c_inputs, c_output, frames, 0).to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_inputs = tr_inputs[vid].clone()\n",
    "HTML(plot_and_rotate(c_inputs, c_labels, frames, 0).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the same process for the last test batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice of frames\n",
    "Now let's plot a sequence of frames of the selected video. We will plot both the groundtruth and the predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [i for i in range(1,9,2)]\n",
    "video_n = 53\n",
    "\n",
    "c_inputs = tr_inputs.clone()\n",
    "c_output = tr_predictions.clone()\n",
    "c_labels = tr_groundtruth.clone()\n",
    "\n",
    "for vid in range(c_labels.shape[0]): \n",
    "    c_inputs[vid,:,::2].mul_(tr_inp_scale[vid, 0])\n",
    "    c_inputs[vid,:,1::2].mul_(tr_inp_scale[vid, 1])\n",
    "    c_output[vid].mul_(tr_out_scale[vid])\n",
    "    c_labels[vid].mul_(tr_out_scale[vid])\n",
    "    \n",
    "    c_inputs[vid,:,::2].mul_(tr_mx[vid, 1])\n",
    "    c_inputs[vid,:,1::2].mul_(tr_my[vid, 1])\n",
    "    c_output[vid].mul_(tr_mz[vid])\n",
    "    c_labels[vid].mul_(tr_mz[vid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeeb812479244acda1d5d04ff64998f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f294c086624f52975565bb8a9208af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_frames(c_output, c_labels, c_inputs, video_n, frames, -90, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [i for i in range(1,9,2)]\n",
    "video_n = 29\n",
    "\n",
    "c_inputs = test_inputs.clone()\n",
    "c_output = test_predictions.clone()\n",
    "c_labels = test_groundtruth.clone()\n",
    "\n",
    "for vid in range(c_labels.shape[0]): \n",
    "    c_inputs[vid,:,::2].mul_(test_inp_scale[vid, 0])\n",
    "    c_inputs[vid,:,1::2].mul_(test_inp_scale[vid, 1])\n",
    "    c_output[vid].mul_(test_out_scale[vid,0])\n",
    "    c_labels[vid].mul_(test_out_scale[vid,1])\n",
    "    \n",
    "    c_inputs[vid,:,::2].mul_(test_mx[vid, 1])\n",
    "    c_inputs[:,:,1::2].mul_(test_my[vid, 1])\n",
    "    c_output[vid].mul_(test_mz[vid,0])\n",
    "    c_labels[vid].mul_(test_mz[vid,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91353f59e4244b28af8d8e2de5727afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64e08a5075c47cf9f2f7a43e81ad739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.34926480190806\n",
      "25.03486995461438\n",
      "17.952318952030883\n",
      "22.69701240194187\n"
     ]
    }
   ],
   "source": [
    "plot_frames(c_output, c_labels, c_inputs, video_n, frames, -90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [i for i in range(1,9,2)]\n",
    "video_n = 17\n",
    "\n",
    "c_inputs = val_inputs.clone()\n",
    "c_output = val_predictions.clone()\n",
    "c_labels = val_groundtruth.clone()\n",
    "\n",
    "for vid in range(c_labels.shape[0]): \n",
    "    c_inputs[vid,:,::2].mul_(val_inp_scale[vid, 0])\n",
    "    c_inputs[vid,:,1::2].mul_(val_inp_scale[vid, 1])\n",
    "    c_output[vid].mul_(val_out_scale[vid])\n",
    "    c_labels[vid].mul_(val_out_scale[vid])\n",
    "    \n",
    "    c_inputs[vid,:,::2].mul_(val_mom_x[vid, 1])\n",
    "    c_inputs[:,:,1::2].mul_(val_mom_y[vid, 1])\n",
    "    c_output[vid].mul_(val_mom_z[vid])\n",
    "    c_labels[vid].mul_(val_mom_z[vid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3d97c68a8b4afb91c9f19a1565a9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adca4f266a3b4f1e8745c4ae18086975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_frames(c_output, c_labels, c_inputs, video_n, frames, -90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
