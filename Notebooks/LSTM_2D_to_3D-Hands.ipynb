{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D to 3D LSTM\n",
    "\n",
    "This is the first approach to try to estimate 3D points coordinates from 2D keypoints extracted with Openpose. Here I will build a simple LSTM to perform the task over the Panoptic Studio dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch utilities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Plotting utilities\n",
    "%matplotlib widget\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from timeit import default_timer as timer\n",
    "import pyprind\n",
    "\n",
    "# Directory and file utilities\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = { 0:'one video', 1:'one signer', 2:'all signers'}\n",
    "MODE =  modes[1]\n",
    "video = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition\n",
    "Now I will define some functions in order to parse and organise the data, and later convert it to pytorch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is structured as follows: in the dataset directory there are several folders, each folder corresponds to a recording; each of these folders contains a folder with the audio, folders with face, hands and body keypoints estimations for each frame, and a folder with the video recorded from different views.\n",
    "\n",
    "In this first approach I will be using the keypoints estimations. Every keypoint folder (face, hands or body) is organized the same way: it contains a json per frame of the video, which includes the 3D keypoints estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_keypoints` will go through each folder in the dataset directory and retrieve the face keypoints, the hands keypoints and the body keypoints. It will separate them into input (2D coordinates per joint per frame) and grountruth (third coordinate to estimate for each input 2D keypoint). \n",
    "The input will be of shape $([n videos, seq len, input size])$, where *seq_len* = number of frames, and *input_size* = face + hands + body keypoints, that is (70+(21+21)+26)x2 -multiplied by 2 because there are x and y coordinates-. The groundtruth (label) data will be of the same shape, except that the last dimension size will not be multiplied by 2 (there's only one coordinate to estimate).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints(data_path):\n",
    "    dataset = []\n",
    "    groundtruth = []\n",
    "    # Look over just the folders inside the directory\n",
    "    just_folders = filter(lambda x: isdir(join(data_path, x)), listdir(data_path))\n",
    "    for p in list(map(lambda x: join(data_path, x), just_folders)): \n",
    "        # Gets 2 list of n_frames lists, one for the 2D coordinates and one for the third coordinate.\n",
    "        # Each list of the n_frames lists contains, either the (x and y) or the z of each keypoint for the face(first line), hands(second), body(third).\n",
    "        # e.g. the first line will result in [[x1,y1,x2,y2...x70,y70]sub1...[x1,y1...x70,y70]subN], [[z1,z2...z70]sub1...[z1..z70]subN]\n",
    "        # Actually, as there will be two of each list above because there are two people en each video.\n",
    "        hands_2d, hands_3d = get_hands(p)\n",
    "        \n",
    "        # Concatenates the coordinates for the face, hands and body on the last dimension, for each person.\n",
    "        vid_input_p1, vid_input_p2 = hands_2d\n",
    "        vid_labels_p1, vid_labels_p2 = hands_3d\n",
    "        \n",
    "        dataset.append(vid_input_p1)\n",
    "        dataset.append(vid_input_p2)\n",
    "        groundtruth.append(vid_labels_p1)\n",
    "        groundtruth.append(vid_labels_p2)\n",
    "        print(f'Completed folder {p}')\n",
    "    return dataset, groundtruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hands json contains the *landmarks* field inside both *left_hand* and *right_hand* field. As there are some frames that may not have one of the hands estimated, I've had to put some exception handling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hands(path):\n",
    "    hand_2D_seq = ([], [])\n",
    "    hand_3D_seq = ([], [])\n",
    "    paths = map(lambda x: join(path, 'hdHand3d', x), sorted(listdir(join(path, 'hdHand3d'))))\n",
    "    files = list(filter(lambda x: isfile(x), paths))\n",
    "    for f in files: # The first and the last frames of these folders are blank.\n",
    "        with open(f, 'r') as j:\n",
    "            json_array = json.load(j)\n",
    "            i = 0\n",
    "            for person in json_array['people']:\n",
    "                if person['id'] != -1:\n",
    "                    try:\n",
    "                        # Separate x,y from z\n",
    "                        hands= [[person[hand]['landmarks'][c] \n",
    "                                 for c in range(len(person['right_hand']['landmarks'])) if (c+1)%3!=0] \n",
    "                                 for hand in ['left_hand', 'right_hand']]\n",
    "                        hand_2D_seq[person['id']].append(hands[0]+hands[1])\n",
    "                        \n",
    "                        hands_3d = [person[hand]['landmarks'][2::3] \n",
    "                                    for hand in ['left_hand', 'right_hand']]\n",
    "                        hand_3D_seq[person['id']].append(hands_3d[0]+hands_3d[1])\n",
    "                    \n",
    "                    except Exception as e: # In case left_hand or right_hand keys don't exist.\n",
    "                        if 'left_hand' in str(e): \n",
    "                            try: # Just put a 0., 0., 0. estimation for each keypoint of the left_hand\n",
    "                                hands = hand_2D_seq[person['id']][-1][:42]+[person['right_hand']['landmarks'][c] \n",
    "                                                                 for c in range(len(person['right_hand']['landmarks'])) if (c+1)%3!=0]\n",
    "                                hands_3d = hand_3D_seq[person['id']][-1][:21]+person['right_hand']['landmarks'][2::3]\n",
    "                            except: # In case neither left_hand nor right_hand exist\n",
    "                                hands = hand_2D_seq[person['id']][-1]\n",
    "                                hands_3d = hand_3D_seq[person['id']][-1]\n",
    "                        elif 'right_hand' in str(e): # Just put a 0., 0., 0. estimation for each keypoint of the right_hand\n",
    "                            hands = [person['left_hand']['landmarks'][c] \n",
    "                                     for c in range(len(person['left_hand']['landmarks'])) if (c+1)%3!=0]+hand_2D_seq[person['id']][-1][42:]\n",
    "                            hands_3d = person['left_hand']['landmarks'][2::3]+hand_3D_seq[person['id']][-1][21:]\n",
    "\n",
    "                        hand_2D_seq[person['id']].append(hands)\n",
    "                        hand_3D_seq[person['id']].append(hands_3d)\n",
    "                    i+=1\n",
    "                    pid = person['id']\n",
    "            if i<2:\n",
    "                hand_2D_seq[1-pid].append(hand_2D_seq[1-pid][-1])\n",
    "                hand_3D_seq[1-pid].append(hand_3D_seq[1-pid][-1])\n",
    "    print('Hands completed.')\n",
    "    return hand_2D_seq, hand_3D_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190419_asl2\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190419_asl4\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190419_asl5\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl1\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl2\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl3\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl5\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl7\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl9\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl91\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl1\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl2\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl3\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl4\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl5\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl6\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl7\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl8\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl910\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl911\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl912\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl913\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../../data/DB keypoints'\n",
    "dataset, groundtruth = get_keypoints(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_seq(dataset):\n",
    "    max_seq = max([len(x) for x in dataset])\n",
    "    seq_lengths = []\n",
    "    for seq in dataset:\n",
    "        seq_lengths.append(len(seq))\n",
    "        for i in range(max_seq-len(seq)):\n",
    "            seq.append([np.nan for j in range(len(seq[0]))])\n",
    "        \n",
    "    return max_seq, seq_lengths\n",
    "\n",
    "max_seq, seq_lengths = padding_seq(dataset)\n",
    "_, _ = padding_seq(groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 8752, 84) (44, 8752, 42) (44,)\n"
     ]
    }
   ],
   "source": [
    "# From python lists to numpy ndarray.\n",
    "dataset = np.asarray(dataset)\n",
    "groundtruth = np.asarray(groundtruth)\n",
    "lengths = np.asarray(seq_lengths)\n",
    "print(dataset.shape, groundtruth.shape, lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../pickles/hands_data.npy', dataset)\n",
    "np.save('../../pickles/hands_ground.npy', groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../pickles/hands_lengths.npy', lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset structuring\n",
    "Now let's convert the lists obtained to Pytorch tensors and organise them in train, validation and test datasets. \n",
    "First, I will define a padding function in order to make all the sequences of video frames the same length, so I can train the LSTM in batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8752\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load('../../pickles/hands_data.npy', allow_pickle=True)\n",
    "groundtruth = np.load('../../pickles/hands_ground.npy', allow_pickle=True)\n",
    "lengths = np.load('../../pickles/hands_lengths.npy', allow_pickle=True)\n",
    "max_seq = dataset.shape[1]\n",
    "print(max_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8045 7836 7326 7141 8751 5155 8751 8752 8751 8751 8751 7205]\n",
      "[8045 7836 7326 7141 8750 5155 8750 8750 8750 8750 8750 7205]\n",
      "(420, 250, 84) (420, 250, 42) (420,)\n"
     ]
    }
   ],
   "source": [
    "if MODE == 'one video':\n",
    "    dataset, groundtruth = dataset[video], groundtruth[video]\n",
    "    chunks_d = np.split(dataset, 547, axis=0)[:lengths[video]//16]\n",
    "    chunks_g = np.split(groundtruth, 547, axis=0)[:lengths[video]//16]\n",
    "    chunks_d = tuple(np.expand_dims(c, axis=0) for c in chunks_d)\n",
    "    chunks_g = tuple(np.expand_dims(c, axis=0) for c in chunks_g)\n",
    "    \n",
    "    dataset = np.concatenate(chunks_d, axis=0)\n",
    "    groundtruth = np.concatenate(chunks_g, axis=0)\n",
    "    lengths = np.asarray([16 for i in range(lengths[video]//16)])\n",
    "    print(lengths.shape)\n",
    "\n",
    "elif MODE == 'one signer':\n",
    "    dataset, groundtruth, lengths = dataset[20::2, :-2], groundtruth[20::2, :-2], lengths[20::2]\n",
    "    print(lengths)\n",
    "    lengths = np.asarray([min(8750, x) for x in lengths])\n",
    "    print(lengths)\n",
    "    chunks_d, chunks_g = np.split(dataset, 35, axis=1), np.split(groundtruth, 35, axis=1)\n",
    "    \n",
    "    dataset, groundtruth = np.concatenate(chunks_d, axis=0), np.concatenate(chunks_g, axis=0)\n",
    "    lengths = np.concatenate(tuple([250 if j*250<=lengths[i] \n",
    "                                    else (lengths[i]%250 if (j-1)*250<lengths[i] \n",
    "                                          else 0) for i in range(12)] for j in range(1,36)), axis=0)\n",
    "    print(dataset.shape, groundtruth.shape, lengths.shape)\n",
    "    \n",
    "elif MODE == 'all signers':\n",
    "    dataset, groundtruth = dataset[:, :-2], groundtruth[:, :-2]\n",
    "    lengths = np.asarray([min(8750, x) for x in lengths])\n",
    "    chunks_d, chunks_g = np.split(dataset, 25, axis=1), np.split(groundtruth, 25, axis=1)\n",
    "    \n",
    "    dataset, groundtruth = np.concatenate(chunks_d, axis=0), np.concatenate(chunks_g, axis=0)\n",
    "    lengths = np.concatenate(tuple([350 if j*350<=lengths[i]\n",
    "                                    else (lengths[i]%350 if (j-1)*350<lengths[i] \n",
    "                                          else 0) for i in range(44)] for j in range(1,26)), axis=0)\n",
    "    print(dataset.shape, groundtruth.shape, lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 250, 84) (384, 250, 42) (384,)\n"
     ]
    }
   ],
   "source": [
    "# Clean all NaN videos\n",
    "dataset = np.delete(dataset, np.where(lengths==0), axis=0)\n",
    "groundtruth = np.delete(groundtruth, np.where(lengths==0), axis=0)\n",
    "lengths = np.delete(lengths, np.where(lengths==0), axis=0)\n",
    "\n",
    "print(dataset.shape, groundtruth.shape, lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 250, 84) (384, 250, 42) (384,)\n"
     ]
    }
   ],
   "source": [
    "def align(tensor, coordinates=1):\n",
    "    for n_vid in range(tensor.shape[0]):\n",
    "        max_value = [np.nanmax(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        min_value = [np.nanmin(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        center = [(max_value[i]+min_value[i])/2 for i in range(coordinates)]\n",
    "        for j in range(coordinates):\n",
    "            subtensor = tensor[n_vid, :, j::coordinates]\n",
    "            subtensor[:] = np.subtract(subtensor, center[j])\n",
    "\n",
    "align(dataset,2)\n",
    "align(groundtruth)\n",
    "print(dataset.shape, groundtruth.shape, lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 250, 84) (384, 250, 42)\n"
     ]
    }
   ],
   "source": [
    "r = R.from_euler('y', 110, degrees=True)\n",
    "shapes = dataset.shape\n",
    "dataset = dataset.reshape(-1, 2)\n",
    "groundtruth = groundtruth.reshape(-1,1)\n",
    "xyz = np.concatenate((dataset, groundtruth), axis=1)\n",
    "xyz = r.apply(xyz)\n",
    "dataset, groundtruth = xyz[:, :2].reshape(shapes), xyz[:,2].reshape(shapes[0], shapes[1],42)\n",
    "\n",
    "print(dataset.shape, groundtruth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: All-NaN slice encountered\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 2)\n"
     ]
    }
   ],
   "source": [
    "def norm_uniform(tensor, coordinates=1, factor=None):\n",
    "    scale = []\n",
    "    mean_ranges = []\n",
    "    for n_vid in range(tensor.shape[0]):\n",
    "        coord_scale = []\n",
    "        max_value = [np.nanmax(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        min_value = [np.nanmin(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        center = [(max_value[i]+min_value[i])/2 for i in range(coordinates)]\n",
    "        ranges = np.ndarray((tensor.shape[1],coordinates))\n",
    "        for n_frame in range(tensor.shape[1]):\n",
    "            rang = [np.nanmax(tensor[n_vid, n_frame,i::coordinates])-np.nanmin(tensor[n_vid, n_frame,i::coordinates]) for i in range(coordinates)]\n",
    "            ranges[n_frame] = np.asarray(rang)\n",
    "        mean_range = [np.nanmean(ranges[:,i]) for i in range(coordinates)]\n",
    "        for j in range(coordinates):\n",
    "            subtensor = tensor[n_vid, :, j::coordinates]\n",
    "            subtensor[:] = np.subtract(subtensor, center[j])\n",
    "            if factor is not None:\n",
    "                subtensor[:] = np.divide(subtensor, factor[n_vid])\n",
    "            else:\n",
    "                subtensor[:] = np.divide(subtensor, max_value[j]-center[j])\n",
    "            coord_scale.append((max_value[j]-center[j] if factor is None else factor[n_vid]))\n",
    "        scale.append(coord_scale)\n",
    "        mean_ranges.append(mean_range)\n",
    "    return mean_ranges\n",
    "input_scale = np.asarray(norm_uniform(dataset,2)).squeeze()\n",
    "print(input_scale.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: RuntimeWarning: Mean of empty slice\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 2) (384, 2)\n"
     ]
    }
   ],
   "source": [
    "def normalize(tensor, coordinates=1, std=None):\n",
    "    moments = []\n",
    "    std_centroids = []\n",
    "    for n_vid in range(tensor.shape[0]):\n",
    "        coord_moments = []\n",
    "        mean_value = [np.nanmean(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        std_value = [np.nanstd(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        centroids = np.ndarray((tensor.shape[1],coordinates))\n",
    "        for n_frame in range(tensor.shape[1]):\n",
    "            centroid = [np.nanmean(tensor[n_vid, n_frame, i::coordinates]) for i in range(coordinates)]\n",
    "            centroids[n_frame] = np.asarray(centroid)\n",
    "        std_centroid = [np.nanstd(centroids[:,i]) for i in range(coordinates)]\n",
    "        if std is not None:\n",
    "            std_value = [std[n_vid]]\n",
    "        for j in range(coordinates):\n",
    "            subtensor = tensor[:, :, j::coordinates]\n",
    "            subtensor[:] = np.subtract(subtensor, mean_value[j])\n",
    "            subtensor[:] = np.divide(subtensor, std_value[j])\n",
    "            coord_moments.append((mean_value[j], std_value[j]))\n",
    "        moments.append(coord_moments)\n",
    "        std_centroids.append(std_centroid)\n",
    "    return moments, std_centroids\n",
    "\n",
    "moments, std_centroids = normalize(dataset, 2)\n",
    "mom_x, mom_y = np.asarray(moments)[:, 0], np.asarray(moments)[:, 1]\n",
    "std_centroids = np.asarray(std_centroids)\n",
    "print(mom_x.shape, mom_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 250, 84) (384, 250, 42) (384,)\n",
      "(384, 2) (384, 2) (384, 2)\n"
     ]
    }
   ],
   "source": [
    "# Randomly shuffle videos\n",
    "permutation = np.random.permutation(dataset.shape[0])\n",
    "dataset, groundtruth, lengths = dataset[permutation], groundtruth[permutation], lengths[permutation]\n",
    "input_scale, mom_x, mom_y = input_scale[permutation], mom_x[permutation], mom_y[permutation]\n",
    "std_centroids = std_centroids[permutation]\n",
    "\n",
    "print(dataset.shape, groundtruth.shape, lengths.shape)\n",
    "print(input_scale.shape, mom_x.shape, mom_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307, 250, 84) (39, 250, 84) (38, 250, 84)\n",
      "(307, 250, 42) (39, 250, 42) (38, 250, 42)\n",
      "(307,) (39,) (38,)\n"
     ]
    }
   ],
   "source": [
    "l1, l2 = len(dataset), len(groundtruth)\n",
    "p1, p2 = 0.8, 0.9\n",
    "# Split in train, validation and test\n",
    "training_kp, val_kp, test_kp = dataset[:round(p1*l1)], dataset[round(p1*l1):round(p2*l1)], dataset[round(p2*l1):]\n",
    "training_lbl, val_lbl, test_lbl = groundtruth[:round(p1*l2)], groundtruth[round(p1*l2):round(p2*l2)], groundtruth[round(p2*l2):]\n",
    "training_lengths, val_lengths, test_lengths = lengths[:round(p1*l1)], lengths[round(p1*l1):round(p2*l1)], lengths[round(p2*l1):]\n",
    "training_inpscale, val_inpscale, test_inpscale = input_scale[:round(p1*l1)], input_scale[round(p1*l1):round(p2*l1)], input_scale[round(p2*l1):]\n",
    "training_mom_x, val_mom_x, test_mom_x = mom_x[:round(p1*l1)], mom_x[round(p1*l1):round(p2*l1)], mom_x[round(p2*l1):]\n",
    "training_mom_y, val_mom_y, test_mom_y = mom_y[:round(p1*l1)], mom_y[round(p1*l1):round(p2*l1)], mom_y[round(p2*l1):]\n",
    "\n",
    "training_std_centroids, val_std_centroids, test_std_centroids = std_centroids[:round(p1*l1)], std_centroids[round(p1*l1):round(p2*l1)], std_centroids[round(p2*l1):]\n",
    "\n",
    "print(training_kp.shape, val_kp.shape, test_kp.shape)\n",
    "print(training_lbl.shape, val_lbl.shape, test_lbl.shape)\n",
    "print(training_lengths.shape, val_lengths.shape, test_lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: All-NaN slice encountered\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307,) (39,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: RuntimeWarning: Mean of empty slice\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307,) (39,)\n"
     ]
    }
   ],
   "source": [
    "training_outscale = np.asarray(norm_uniform(training_lbl)).squeeze()\n",
    "val_outscale_t = np.asarray(norm_uniform(val_lbl)).squeeze()\n",
    "test_outscale_t = np.asarray(norm_uniform(test_lbl)).squeeze()\n",
    "print(training_outscale.shape, val_outscale_t.shape)\n",
    "\n",
    "training_mom_z = np.asarray(normalize(training_lbl)[0])[:, 0, 1]\n",
    "val_mom_z_t = np.asarray(normalize(val_lbl)[0])[:, 0, 1]\n",
    "test_mom_z_t = np.asarray(normalize(test_lbl)[0])[:, 0, 1]\n",
    "print(training_mom_z.shape, val_mom_z_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39,) (38,)\n",
      "(39,) (38,)\n"
     ]
    }
   ],
   "source": [
    "alpha = LinearRegression(fit_intercept=True)#make_pipeline(PolynomialFeatures(9), LinearRegression(fit_intercept=True))\n",
    "alpha.fit(training_inpscale, training_outscale)\n",
    "val_outscale = alpha.predict(val_inpscale)\n",
    "test_outscale = alpha.predict(test_inpscale)\n",
    "print(val_outscale.shape, test_outscale.shape)\n",
    "\n",
    "sigma = LinearRegression(fit_intercept=True)#make_pipeline(PolynomialFeatures(9), LinearRegression(fit_intercept=True))\n",
    "train_X = np.concatenate((training_mom_x[:,1, np.newaxis], training_mom_y[:,1, np.newaxis]), axis=1)\n",
    "sigma.fit(train_X, training_mom_z)\n",
    "val_X = np.concatenate((val_mom_x[:,1,np.newaxis], val_mom_y[:,1,np.newaxis]), axis=1)\n",
    "test_X = np.concatenate((test_mom_x[:,1,np.newaxis], test_mom_y[:,1,np.newaxis]), axis=1)\n",
    "val_mom_z = sigma.predict(val_X)\n",
    "test_mom_z = sigma.predict(test_X)\n",
    "print(val_mom_z.shape, test_mom_z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0876d05a084a2981aaebdf0b56b8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7efffa534e90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(training_inpscale[:,0], training_inpscale[:,1], training_outscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712a0bc900404111a604a04e9af61bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7efffa49bfd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(training_std_centroids[:,0], training_std_centroids[:,1], training_mom_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d83a69d2ea46c99008a1848fc0a8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f84c56c7d90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(training_std_centroids[:,0], training_mom_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc6da98ef7a4ef3ad306b1086c6b678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f84c563f750>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(training_std_centroids[:,1], training_mom_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18.2906965  16.82517962 19.57005748 22.52438205 17.87114275 11.99424387\n",
      " 16.31027071 16.35032729 19.23204757 15.32006594 19.75141615 16.29158461\n",
      " 23.05646335 10.10385934 25.09532207 18.09517013 17.19481992 18.81210565\n",
      " 23.20661397 20.1816844  15.76375996 16.47858531 20.78641058 17.15650844\n",
      " 19.31872192 17.14999796 15.22281605 14.21528394 16.68384578 15.91857705\n",
      " 18.09480804 17.12435906 15.59362474 17.13569245 15.83816033 16.13209973\n",
      " 20.49356196 18.63693859 16.06012377] [17.05990854 17.06409997 19.25830967  8.39479894 14.22167657 20.44188234\n",
      " 26.31123757 15.95854941 14.92147178 18.67199752 18.36479764 18.18536949\n",
      " 20.76252158 13.42502815 17.51867502 20.57902718 13.56698354 12.66891902\n",
      " 11.55972816 11.3547611  13.55901344 13.20416469 18.09152243 15.26261763\n",
      " 16.90963431 19.29833464 14.54353774 21.21486303 18.4105562  16.74867481\n",
      " 19.98191801 16.20374108 14.32702299 18.33056613 52.69586727 13.82608953\n",
      " 20.53225323 16.89716669]\n",
      "[0.33154718 0.95228282 0.98557872 2.12295204 0.55708201 0.99922327\n",
      " 0.78396307 0.93017435 1.1030453  1.22476538 0.77972073 1.2137724\n",
      " 1.17328473 0.34572935 2.61229058 0.82440535 1.09263669 1.21562654\n",
      " 1.3461239  0.55031391 1.03799167 0.8836913  1.18530148 1.12585985\n",
      " 1.0044583  1.10332965 0.71049131 1.31181577 0.99693112 0.80435143\n",
      " 1.2416067  0.70008507 0.94583579 1.12797808 1.42629602 0.60481953\n",
      " 1.38644953 1.14789005 1.01528209] [0.25391511 0.97124273 1.23786374 1.39723889 0.69044666 1.13499092\n",
      " 1.08763417 0.76308363 1.18305786 1.02148466 1.04129104 1.06212161\n",
      " 0.84683679 0.99794243 0.96577908 1.06029608 0.63506433 1.75225617\n",
      " 0.73839942 1.08856907 0.62868723 2.12448458 0.82448489 0.79312724\n",
      " 1.5059543  1.0318593  0.7568495  1.18473046 1.05412078 0.78406032\n",
      " 1.13183565 0.97955028 1.14843212 0.56948848 2.37065354 0.61181261\n",
      " 1.2391294  0.97022326]\n",
      "7.1425736362597965 42.250788514170175\n",
      "0.1550631162232058 0.1520626898254534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(val_outscale_t, test_outscale_t)\n",
    "print(val_mom_z_t, test_mom_z_t)\n",
    "print(mean_squared_error(val_outscale, val_outscale_t), mean_squared_error(test_outscale, test_outscale_t))\n",
    "print(mean_squared_error(val_mom_z, val_mom_z_t), mean_squared_error(test_mom_z, test_mom_z_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 2) (38, 2)\n"
     ]
    }
   ],
   "source": [
    "val_outscale = np.concatenate((val_outscale[:,np.newaxis], val_outscale_t[:,np.newaxis]), axis=1)\n",
    "test_outscale = np.concatenate((test_outscale[:,np.newaxis], test_outscale_t[:,np.newaxis]), axis=1)\n",
    "\n",
    "val_mom_z = np.concatenate((val_mom_z[:,np.newaxis], val_mom_z_t[:,np.newaxis]), axis=1)\n",
    "test_mom_z = np.concatenate((test_mom_z[:,np.newaxis], test_mom_z_t[:,np.newaxis]), axis=1)\n",
    "print(val_outscale.shape, test_mom_z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([307, 250, 84]) torch.Size([39, 250, 42]) torch.Size([38])\n",
      "torch.Size([307, 2]) torch.Size([307])\n"
     ]
    }
   ],
   "source": [
    "# From python lists to pytorch tensors.\n",
    "training_kp, val_kp, test_kp = torch.tensor(np.nan_to_num(training_kp), dtype=torch.float32), torch.tensor(np.nan_to_num(val_kp), dtype=torch.float32), torch.tensor(np.nan_to_num(test_kp), dtype=torch.float32)\n",
    "training_lbl, val_lbl, test_lbl = torch.tensor(np.nan_to_num(training_lbl), dtype=torch.float32), torch.tensor(np.nan_to_num(val_lbl), dtype=torch.float32), torch.tensor(np.nan_to_num(test_lbl), dtype=torch.float32)\n",
    "training_lengths, val_lengths, test_lengths = torch.tensor(np.nan_to_num(training_lengths), dtype=torch.float32), torch.tensor(np.nan_to_num(val_lengths), dtype=torch.float32), torch.tensor(np.nan_to_num(test_lengths), dtype=torch.float32)\n",
    "training_inpscale, val_inpscale, test_inpscale = torch.tensor(np.nan_to_num(training_inpscale), dtype=torch.float32), torch.tensor(np.nan_to_num(val_inpscale), dtype=torch.float32), torch.tensor(np.nan_to_num(test_inpscale), dtype=torch.float32)\n",
    "training_outscale, val_outscale, test_outscale = torch.tensor(np.nan_to_num(training_outscale), dtype=torch.float32), torch.tensor(np.nan_to_num(val_outscale), dtype=torch.float32), torch.tensor(np.nan_to_num(test_outscale), dtype=torch.float32)\n",
    "training_mom_x, val_mom_x, test_mom_x = torch.tensor(np.nan_to_num(training_mom_x), dtype=torch.float32), torch.tensor(np.nan_to_num(val_mom_x), dtype=torch.float32), torch.tensor(np.nan_to_num(test_mom_x), dtype=torch.float32)\n",
    "training_mom_y, val_mom_y, test_mom_y = torch.tensor(np.nan_to_num(training_mom_y), dtype=torch.float32), torch.tensor(np.nan_to_num(val_mom_y), dtype=torch.float32), torch.tensor(np.nan_to_num(test_mom_y), dtype=torch.float32)\n",
    "training_mom_z, val_mom_z, test_mom_z = torch.tensor(np.nan_to_num(training_mom_z), dtype=torch.float32), torch.tensor(np.nan_to_num(val_mom_z), dtype=torch.float32), torch.tensor(np.nan_to_num(test_mom_z), dtype=torch.float32)\n",
    "\n",
    "print(training_kp.shape, val_lbl.shape, test_lengths.shape)\n",
    "print(training_inpscale.shape, training_outscale.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we define the batch_size and put the datasets in DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f0012714f10>\n"
     ]
    }
   ],
   "source": [
    "train_data = TensorDataset(training_kp, training_lbl, training_lengths,\n",
    "                          training_inpscale, training_outscale,\n",
    "                          training_mom_x, training_mom_y, training_mom_z)\n",
    "val_data = TensorDataset(val_kp, val_lbl, val_lengths,\n",
    "                        val_inpscale, val_outscale,\n",
    "                        val_mom_x, val_mom_y, val_mom_z)\n",
    "test_data = TensorDataset(test_kp, test_lbl, test_lengths,\n",
    "                         test_inpscale, test_outscale,\n",
    "                         test_mom_x, test_mom_y, test_mom_z)\n",
    "\n",
    "batch_size = 25\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a GPU available we set our device to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print some examples to see whether it is loaded correctly or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 250, 84]) torch.Size([25, 250, 42]) torch.Size([25]) torch.Size([25, 2]) torch.Size([25])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y, sample_len, iscale, oscale, momx, momy, momz = dataiter.next()\n",
    "\n",
    "print(sample_x.shape, sample_y.shape, sample_len.shape, iscale.shape, momz.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building\n",
    "It is time to build the model for this approach. It will consist on a single/double layer LSTM followed by a Linear layer with output size the number of keypoints we want to estimate. I also define a method to initialize the hidden_state of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_2D3D(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, bidirectional, dropout=0.):\n",
    "        super().__init__()\n",
    "        # Save the model parameters\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bi = bidirectional\n",
    "        \n",
    "        # Define the architecture\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*(2 if self.bi else 1), output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, state, lengths):\n",
    "        # Describe the forward step\n",
    "        batch_size, seq_len = x.size(0), x.size(1) # We save the batch size and the (maximum) sequence length\n",
    "        \n",
    "        # Need to pack a tensor containing padded sequences of variable length\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths=lengths, batch_first=True, enforce_sorted=False)\n",
    "        ht, hidden_state = self.lstm(packed, state) # ht will be a PackedSequence\n",
    "\n",
    "        # Need to flatten and reshape the output to feed it to the Linear layer\n",
    "        ht = ht.data.contiguous() # ht will be of shape [sum(lengths), hidden_dim]\n",
    "        ot = self.fc(ht) # ot will be of shape [sum(lengths), ouput_size]\n",
    "\n",
    "        l_ot = [ot[:int(length)] for length in lengths] # list of batch elements, each shape [lengths[i], output_size]\n",
    "        packed_ot = nn.utils.rnn.pack_sequence(l_ot, enforce_sorted=False) # PackedSequence\n",
    "        # Finally return to shape [batch_size, seq_len, output_size]\n",
    "        ot, _ = nn.utils.rnn.pad_packed_sequence(packed_ot, batch_first=True, total_length=seq_len)\n",
    "        \n",
    "        return ot, hidden_state\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers*(2 if self.bi else 1), batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers*(2 if self.bi else 1), batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_2D3D(\n",
      "  (lstm): LSTM(84, 512, num_layers=2, batch_first=True)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=42, bias=True)\n",
      "  )\n",
      ")\n",
      "3347498\n"
     ]
    }
   ],
   "source": [
    "# Define some model parameters\n",
    "INPUT_SIZE = sample_x.size(2)\n",
    "OUTPUT_SIZE = sample_y.size(2)\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = False\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTM_2D3D(INPUT_SIZE, OUTPUT_SIZE, HIDDEN_DIM, N_LAYERS, BIDIRECTIONAL, dropout=0.)\n",
    "model.to(device)\n",
    "print(model)\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now we will proceed with the training. The first cell will define the learning rate, the loss function and the selected optimizer for the training process. Then we will proceed with a training over a number of epochs in which we will print it's training loss and validation loss. I also will be using Tensorboard to have a much nicer view of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substract_root_PJPE(output):\n",
    "    jhl, jhr = torch.chunk(output[:, :, :21], max_seq, dim=1), torch.chunk(output[:, :, 21:], max_seq, dim=1)\n",
    "    joints_merged = []\n",
    "    roots = [0, 0]\n",
    "    for i, joints in enumerate((jhl, jhr)):\n",
    "        n_joints = []\n",
    "        for chunk in joints:\n",
    "            n_joints.append(chunk.sub(chunk[:,:,roots[i]].unsqueeze(2)))\n",
    "        joints_merged.append(torch.cat(tuple(n_joints), dim=1))\n",
    "    joints_merged = torch.cat(tuple(joints_merged), dim=2)\n",
    "    return joints_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpjpe(rooted_o, rooted_l, seq_lens):\n",
    "    MPJPE = []\n",
    "    for i in range(len(seq_lens)):\n",
    "        MPJPE.append(rooted_o[i,:int(seq_lens[i])].sub(rooted_l[i,:int(seq_lens[i])]).abs().mean().item())\n",
    "    \n",
    "    return np.mean(MPJPE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_keypoints(output, labels, seq_lens, alpha=0.1):\n",
    "    total_correct = 0\n",
    "    for i in range(len(seq_lens)):\n",
    "        for j in range(int(seq_lens[i])):\n",
    "            rang = (labels[i,j].max()-labels[i,j].min())\n",
    "            for o, l in zip(output[i,j], labels[i,j]):\n",
    "                if o < l+alpha*rang and o > l-alpha*rang:\n",
    "                    total_correct += 1\n",
    "    return total_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 60\n",
    "lr = 4.5e-6\n",
    "loss_function = nn.MSELoss()\n",
    "one_cycle = True\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "if one_cycle:\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, \n",
    "                                                  steps_per_epoch=len(train_loader), epochs=NUM_EPOCHS,\n",
    "                                                  div_factor=25.0, final_div_factor=10000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "name = 'hands_whole'\n",
    "writer = SummaryWriter(log_dir=f'/deeplearning/logs/{name}{datetime.now()}_lr-{lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 1/60 in 138.18s.\n",
      " Loss: 1.0517  Val Loss: 1.7549\n",
      " Train MPJPE: 0.5601  Val MPJPE: 0.5208\n",
      "Finished epoch 2/60 in 137.78s.\n",
      " Loss: 1.0579  Val Loss: 1.9323\n",
      " Train MPJPE: 0.5610  Val MPJPE: 0.5311\n",
      "Finished epoch 3/60 in 143.34s.\n",
      " Loss: 1.0561  Val Loss: 1.7752\n",
      " Train MPJPE: 0.5592  Val MPJPE: 0.5030\n",
      "Finished epoch 4/60 in 144.02s.\n",
      " Loss: 1.0577  Val Loss: 1.8825\n",
      " Train MPJPE: 0.5590  Val MPJPE: 0.5475\n",
      "Finished epoch 5/60 in 143.71s.\n",
      " Loss: 1.0532  Val Loss: 1.8427\n",
      " Train MPJPE: 0.5539  Val MPJPE: 0.5027\n",
      "Finished epoch 6/60 in 142.70s.\n",
      " Loss: 1.0591  Val Loss: 1.8109\n",
      " Train MPJPE: 0.5554  Val MPJPE: 0.5273\n",
      "Finished epoch 7/60 in 138.90s.\n",
      " Loss: 1.0592  Val Loss: 1.6984\n",
      " Train MPJPE: 0.5566  Val MPJPE: 0.5373\n",
      "Finished epoch 8/60 in 137.84s.\n",
      " Loss: 1.0594  Val Loss: 1.8313\n",
      " Train MPJPE: 0.5550  Val MPJPE: 0.5502\n",
      "Finished epoch 9/60 in 139.12s.\n",
      " Loss: 1.0562  Val Loss: 1.8053\n",
      " Train MPJPE: 0.5512  Val MPJPE: 0.5301\n",
      "Finished epoch 10/60 in 142.56s.\n",
      " Loss: 1.0547  Val Loss: 1.8774\n",
      " Train MPJPE: 0.5534  Val MPJPE: 0.5577\n",
      "Finished epoch 11/60 in 140.37s.\n",
      " Loss: 1.0493  Val Loss: 1.9071\n",
      " Train MPJPE: 0.5545  Val MPJPE: 0.5678\n",
      "Finished epoch 12/60 in 139.55s.\n",
      " Loss: 1.0498  Val Loss: 1.9632\n",
      " Train MPJPE: 0.5478  Val MPJPE: 0.5166\n",
      "Finished epoch 13/60 in 143.61s.\n",
      " Loss: 1.0350  Val Loss: 1.8682\n",
      " Train MPJPE: 0.5469  Val MPJPE: 0.4780\n",
      "Finished epoch 14/60 in 142.26s.\n",
      " Loss: 1.0472  Val Loss: 1.7909\n",
      " Train MPJPE: 0.5445  Val MPJPE: 0.5240\n",
      "Finished epoch 15/60 in 143.75s.\n",
      " Loss: 1.0394  Val Loss: 1.8715\n",
      " Train MPJPE: 0.5369  Val MPJPE: 0.4964\n",
      "Finished epoch 16/60 in 138.22s.\n",
      " Loss: 1.0351  Val Loss: 1.6502\n",
      " Train MPJPE: 0.5347  Val MPJPE: 0.4902\n",
      "Finished epoch 17/60 in 138.22s.\n",
      " Loss: 1.0444  Val Loss: 1.8088\n",
      " Train MPJPE: 0.5360  Val MPJPE: 0.4864\n",
      "Finished epoch 18/60 in 142.25s.\n",
      " Loss: 1.0353  Val Loss: 1.8352\n",
      " Train MPJPE: 0.5293  Val MPJPE: 0.5019\n",
      "Finished epoch 19/60 in 138.97s.\n",
      " Loss: 1.0404  Val Loss: 1.9727\n",
      " Train MPJPE: 0.5258  Val MPJPE: 0.5100\n",
      "Finished epoch 20/60 in 141.16s.\n",
      " Loss: 1.0218  Val Loss: 1.7720\n",
      " Train MPJPE: 0.5180  Val MPJPE: 0.4895\n",
      "Finished epoch 21/60 in 140.88s.\n",
      " Loss: 1.0227  Val Loss: 1.8486\n",
      " Train MPJPE: 0.5137  Val MPJPE: 0.5204\n",
      "Finished epoch 22/60 in 139.81s.\n",
      " Loss: 1.0283  Val Loss: 1.6461\n",
      " Train MPJPE: 0.5125  Val MPJPE: 0.4684\n",
      "Finished epoch 23/60 in 136.99s.\n",
      " Loss: 1.0262  Val Loss: 1.8327\n",
      " Train MPJPE: 0.5036  Val MPJPE: 0.4911\n",
      "Finished epoch 24/60 in 140.50s.\n",
      " Loss: 1.0219  Val Loss: 1.8491\n",
      " Train MPJPE: 0.4999  Val MPJPE: 0.4987\n",
      "Finished epoch 25/60 in 141.44s.\n",
      " Loss: 1.0172  Val Loss: 1.6179\n",
      " Train MPJPE: 0.4916  Val MPJPE: 0.4692\n",
      "Finished epoch 26/60 in 140.57s.\n",
      " Loss: 1.0207  Val Loss: 1.6689\n",
      " Train MPJPE: 0.4888  Val MPJPE: 0.4204\n",
      "Finished epoch 27/60 in 140.17s.\n",
      " Loss: 1.0106  Val Loss: 1.8702\n",
      " Train MPJPE: 0.4834  Val MPJPE: 0.4878\n",
      "Finished epoch 28/60 in 139.66s.\n",
      " Loss: 1.0081  Val Loss: 1.8087\n",
      " Train MPJPE: 0.4818  Val MPJPE: 0.4748\n",
      "Finished epoch 29/60 in 138.36s.\n",
      " Loss: 1.0154  Val Loss: 1.7704\n",
      " Train MPJPE: 0.4815  Val MPJPE: 0.4575\n",
      "Finished epoch 30/60 in 138.85s.\n",
      " Loss: 1.0156  Val Loss: 1.7353\n",
      " Train MPJPE: 0.4775  Val MPJPE: 0.4484\n",
      "Finished epoch 31/60 in 150.45s.\n",
      " Loss: 1.0105  Val Loss: 1.7078\n",
      " Train MPJPE: 0.4726  Val MPJPE: 0.4495\n",
      "Finished epoch 32/60 in 138.92s.\n",
      " Loss: 1.0052  Val Loss: 1.4408\n",
      " Train MPJPE: 0.4746  Val MPJPE: 0.4416\n",
      "Finished epoch 33/60 in 154.30s.\n",
      " Loss: 1.0086  Val Loss: 1.5832\n",
      " Train MPJPE: 0.4700  Val MPJPE: 0.4749\n",
      "Finished epoch 34/60 in 139.23s.\n",
      " Loss: 1.0075  Val Loss: 1.5820\n",
      " Train MPJPE: 0.4702  Val MPJPE: 0.4634\n",
      "Finished epoch 35/60 in 141.50s.\n",
      " Loss: 1.0068  Val Loss: 1.7047\n",
      " Train MPJPE: 0.4700  Val MPJPE: 0.4187\n",
      "Finished epoch 36/60 in 148.54s.\n",
      " Loss: 1.0016  Val Loss: 1.8669\n",
      " Train MPJPE: 0.4676  Val MPJPE: 0.4768\n",
      "Finished epoch 37/60 in 147.25s.\n",
      " Loss: 1.0052  Val Loss: 1.8215\n",
      " Train MPJPE: 0.4680  Val MPJPE: 0.4696\n",
      "Finished epoch 38/60 in 143.48s.\n",
      " Loss: 0.9973  Val Loss: 1.6718\n",
      " Train MPJPE: 0.4681  Val MPJPE: 0.4601\n",
      "Finished epoch 39/60 in 143.50s.\n",
      " Loss: 1.0075  Val Loss: 1.4841\n",
      " Train MPJPE: 0.4657  Val MPJPE: 0.4397\n",
      "Finished epoch 40/60 in 144.80s.\n",
      " Loss: 1.0066  Val Loss: 1.4752\n",
      " Train MPJPE: 0.4648  Val MPJPE: 0.4435\n",
      "Finished epoch 41/60 in 144.03s.\n",
      " Loss: 0.9996  Val Loss: 1.6125\n",
      " Train MPJPE: 0.4625  Val MPJPE: 0.4736\n",
      "Finished epoch 42/60 in 143.21s.\n",
      " Loss: 0.9967  Val Loss: 1.9057\n",
      " Train MPJPE: 0.4648  Val MPJPE: 0.4640\n",
      "Finished epoch 43/60 in 149.41s.\n",
      " Loss: 0.9893  Val Loss: 1.5868\n",
      " Train MPJPE: 0.4633  Val MPJPE: 0.4277\n",
      "Finished epoch 44/60 in 147.74s.\n",
      " Loss: 0.9983  Val Loss: 1.7588\n",
      " Train MPJPE: 0.4632  Val MPJPE: 0.4660\n",
      "Finished epoch 45/60 in 150.81s.\n",
      " Loss: 1.0030  Val Loss: 1.7672\n",
      " Train MPJPE: 0.4601  Val MPJPE: 0.5102\n",
      "Finished epoch 46/60 in 141.97s.\n",
      " Loss: 1.0025  Val Loss: 1.7182\n",
      " Train MPJPE: 0.4627  Val MPJPE: 0.4892\n",
      "Finished epoch 47/60 in 167.79s.\n",
      " Loss: 1.0054  Val Loss: 1.6807\n",
      " Train MPJPE: 0.4580  Val MPJPE: 0.4808\n",
      "Finished epoch 48/60 in 150.66s.\n",
      " Loss: 1.0013  Val Loss: 1.7090\n",
      " Train MPJPE: 0.4628  Val MPJPE: 0.4769\n",
      "Finished epoch 49/60 in 186.64s.\n",
      " Loss: 1.0047  Val Loss: 1.4535\n",
      " Train MPJPE: 0.4610  Val MPJPE: 0.4132\n",
      "Finished epoch 50/60 in 149.41s.\n",
      " Loss: 1.0043  Val Loss: 1.5257\n",
      " Train MPJPE: 0.4614  Val MPJPE: 0.4325\n",
      "Finished epoch 51/60 in 149.34s.\n",
      " Loss: 1.0011  Val Loss: 1.6480\n",
      " Train MPJPE: 0.4580  Val MPJPE: 0.4131\n",
      "Finished epoch 52/60 in 161.87s.\n",
      " Loss: 0.9944  Val Loss: 1.7688\n",
      " Train MPJPE: 0.4624  Val MPJPE: 0.4789\n",
      "Finished epoch 53/60 in 164.60s.\n",
      " Loss: 1.0052  Val Loss: 1.7405\n",
      " Train MPJPE: 0.4611  Val MPJPE: 0.4720\n",
      "Finished epoch 54/60 in 175.63s.\n",
      " Loss: 0.9996  Val Loss: 1.5651\n",
      " Train MPJPE: 0.4580  Val MPJPE: 0.4588\n",
      "Finished epoch 55/60 in 171.28s.\n",
      " Loss: 1.0041  Val Loss: 1.5471\n",
      " Train MPJPE: 0.4609  Val MPJPE: 0.4332\n",
      "Finished epoch 56/60 in 154.28s.\n",
      " Loss: 0.9983  Val Loss: 1.7740\n",
      " Train MPJPE: 0.4592  Val MPJPE: 0.4700\n",
      "Finished epoch 57/60 in 160.27s.\n",
      " Loss: 1.0001  Val Loss: 1.5697\n",
      " Train MPJPE: 0.4606  Val MPJPE: 0.4331\n",
      "Finished epoch 58/60 in 178.60s.\n",
      " Loss: 0.9998  Val Loss: 1.6346\n",
      " Train MPJPE: 0.4607  Val MPJPE: 0.4519\n",
      "Finished epoch 59/60 in 150.23s.\n",
      " Loss: 1.0019  Val Loss: 1.8916\n",
      " Train MPJPE: 0.4597  Val MPJPE: 0.5033\n",
      "Finished epoch 60/60 in 144.93s.\n",
      " Loss: 0.9996  Val Loss: 1.5936\n",
      " Train MPJPE: 0.4607  Val MPJPE: 0.4394\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da09a5d3b9e14ea0b10ad0c6050f2992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f0012754650>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer_beg = timer()\n",
    "\n",
    "tr_losses = []\n",
    "val_losses = []\n",
    "\n",
    "model.train()\n",
    "for i in range(NUM_EPOCHS):\n",
    "    # Init the hidden state (ht, ct)\n",
    "    h = model.init_hidden(batch_size)\n",
    "    batch_losses = []\n",
    "    train_MPJPE = []\n",
    "    \n",
    "    if i+1 == NUM_EPOCHS:\n",
    "        preds, inps, labls, lens = [], [], [], []\n",
    "        val_preds, val_inps, val_labls, val_lens = [], [], [], []\n",
    "        iscale, oscale, val_iscale, val_oscale = [], [], [], []\n",
    "        momx, momy, momz = [], [], []\n",
    "        val_momx, val_momy, val_momz = [], [], []\n",
    "        \n",
    "    for inputs, labels, lengths, i_s, o_s, mx, my, mz in train_loader:\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels, lengths = inputs.to(device), labels.to(device), lengths.to(device)\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward step\n",
    "        output, h = model(inputs, h, lengths)\n",
    "        \n",
    "        if i+1 == NUM_EPOCHS:\n",
    "            e = [preds, inps, labls, lens, iscale, oscale, momx, momy, momz]\n",
    "            b = [output, inputs, labels, lengths, i_s, o_s, mx, my, mz]\n",
    "            for k in range(len(e)):\n",
    "                e[k].append(b[k])            \n",
    "\n",
    "        # Loss calculation and backward step\n",
    "        loss = loss_function(nn.utils.rnn.pack_padded_sequence(output, lengths=lengths, batch_first=True, \n",
    "                                                               enforce_sorted=False).data,\n",
    "                             nn.utils.rnn.pack_padded_sequence(labels, lengths=lengths, batch_first=True,\n",
    "                                                               enforce_sorted=False).data)\n",
    "        loss.backward()\n",
    "        # Weight update\n",
    "        optimizer.step()\n",
    "        # One cycle policy step\n",
    "        if one_cycle:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Output data collection for showing\n",
    "        batch_losses.append(loss.item())\n",
    "        rooted_o, rooted_l = substract_root_PJPE(output), substract_root_PJPE(labels)\n",
    "        train_MPJPE.append(mpjpe(rooted_o, rooted_l, lengths))\n",
    "    \n",
    "    timer_end = timer()\n",
    "    tr_losses.append(np.mean(batch_losses))\n",
    "    writer.add_scalar('Loss/train', tr_losses[-1], i)   \n",
    "    train_MPJPE_total = np.mean(train_MPJPE)\n",
    "    \n",
    "    # Validation at the end of an epoch\n",
    "    val_h = model.init_hidden(batch_size)\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    val_MPJPE = []\n",
    "    \n",
    "    for inp, lab, lns, vis, vos, vmx, vmy, vmz in val_loader:\n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "        inp, lab, lns = inp.to(device), lab.to(device), lns.to(device)\n",
    "        out, val_h = model(inp, val_h, lns)\n",
    "        \n",
    "        if i+1 == NUM_EPOCHS:\n",
    "            e = [val_preds, val_inps, val_labls, val_lens, val_iscale, val_oscale, val_momx, val_momy, val_momz]\n",
    "            b = [out, inp, lab, lns, vis, vos, vmx, vmy, vmz]\n",
    "            for k in range(len(e)):\n",
    "                e[k].append(b[k])  \n",
    "        \n",
    "        loss = loss_function(nn.utils.rnn.pack_padded_sequence(out, lengths=lns, batch_first=True,\n",
    "                                                               enforce_sorted=False).data,\n",
    "                             nn.utils.rnn.pack_padded_sequence(lab, lengths=lns, batch_first=True,\n",
    "                                                               enforce_sorted=False).data)\n",
    "        val_loss.append(loss.item())\n",
    "        rooted_o, rooted_l = substract_root_PJPE(out), substract_root_PJPE(lab)\n",
    "        val_MPJPE.append(mpjpe(rooted_o, rooted_l, lns))\n",
    "        \n",
    "    val_losses.append(np.mean(val_loss))\n",
    "    writer.add_scalar('Loss/validation', val_losses[-1], i)  \n",
    "    val_MPJPE_total = np.mean(val_MPJPE)\n",
    "    model.train()\n",
    "    \n",
    "    # Output loss and training time.\n",
    "    print(f\"Finished epoch {i+1}/{NUM_EPOCHS} in {(timer_end-timer_beg):.2f}s.\\n\",\n",
    "             f\"Loss: {np.mean(tr_losses[-1]):.4f}\", f\" Val Loss: {val_losses[-1]:.4f}\\n\",\n",
    "             f\"Train MPJPE: {train_MPJPE_total:.4f}\", f\" Val MPJPE: {val_MPJPE_total:.4f}\")\n",
    "    timer_beg = timer()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(tr_losses, label='train')\n",
    "plt.plot(val_losses, label='validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSELoss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the predictions for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 2]) torch.Size([300, 250, 84])\n"
     ]
    }
   ],
   "source": [
    "tr_predictions = torch.cat(tuple(preds), dim=0)\n",
    "tr_inputs = torch.cat(tuple(inps), dim=0)\n",
    "tr_groundtruth = torch.cat(tuple(labls), dim=0)\n",
    "tr_lengths = torch.cat(tuple(lens), dim=0)\n",
    "tr_inp_scale, tr_out_scale = torch.cat(tuple(iscale), dim=0), torch.cat(tuple(oscale), dim=0)\n",
    "tr_mx, tr_my, tr_mz = torch.cat(tuple(momx), dim=0), torch.cat(tuple(momy), dim=0), torch.cat(tuple(momz), dim=0)\n",
    "print(tr_inp_scale.shape, tr_inputs.shape)\n",
    "\n",
    "val_predictions = torch.cat(tuple(val_preds), dim=0)\n",
    "val_inputs = torch.cat(tuple(val_inps), dim=0)\n",
    "val_groundtruth = torch.cat(tuple(val_labls), dim=0)\n",
    "val_length = torch.cat(tuple(val_lens), dim=0)\n",
    "val_inp_scale, val_out_scale = torch.cat(tuple(val_iscale), dim=0), torch.cat(tuple(val_oscale), dim=0)\n",
    "val_mx, val_my, val_mz = torch.cat(tuple(val_momx), dim=0), torch.cat(tuple(val_momy), dim=0), torch.cat(tuple(val_momz), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PCK: 13.764 Validation PCK: 6.767\n"
     ]
    }
   ],
   "source": [
    "c_output = tr_predictions.clone()\n",
    "c_labels = tr_groundtruth.clone()\n",
    "for vid in range(c_labels.shape[0]): \n",
    "    c_output[vid].mul_(tr_mz[vid])\n",
    "    c_labels[vid].mul_(tr_mz[vid])\n",
    "\n",
    "tr_CK = correct_keypoints(c_output, c_labels, tr_lengths)\n",
    "tr_K = tr_lengths.sum()*tr_groundtruth.shape[2]\n",
    "\n",
    "c_output = val_predictions.clone()\n",
    "c_labels = val_groundtruth.clone()\n",
    "for vid in range(c_labels.shape[0]): \n",
    "    c_output[vid].mul_(val_mz[vid,0])\n",
    "    c_labels[vid].mul_(val_mz[vid,1])\n",
    "    \n",
    "val_CK = correct_keypoints(c_output, c_labels, val_length)\n",
    "val_K = val_lengths.sum()*val_groundtruth.shape[2]\n",
    "\n",
    "tr_PCK = tr_CK/ tr_K\n",
    "val_PCK = val_CK/ val_K\n",
    "print(f\"Training PCK: {tr_PCK*100:.3f} Validation PCK: {val_PCK*100:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = torch.zeros((138), device='cuda:0')\n",
    "\n",
    "for i in range(len(tr_predictions)):\n",
    "    count=0\n",
    "    for frame in tr_predictions[i, :int(tr_lengths[i])]:\n",
    "        if torch.all(frame.eq(zeros)):\n",
    "            count +=1\n",
    "    print(f'{count}/{int(tr_lengths[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = torch.zeros((138), device='cuda:0')\n",
    "\n",
    "for i in range(len(val_predictions)):\n",
    "    count=0\n",
    "    for frame in tr_predictions[i, :int(val_length[i])]:\n",
    "        if torch.all(frame.eq(zeros)):\n",
    "            count +=1\n",
    "    print(f'{count}/{int(val_length[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'./{name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'./{name}.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "After the training, we shall proceed with the performance test. This will go through the test batches and perform the inference, then it will show the test loss, as well as the performance metric. In this case, as we are working with human body keypoints, we will use the Mean Per Joint Position Error (MPJPE) metric, which outputs the mean euclidean distance between the joints (keypoints) positions estimated and the ones in the groundtruth.\n",
    "\n",
    "The formula for MPJPE is the following:\n",
    "\n",
    "$\\text{MPJPE} = \\frac1T\\frac1N\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{i=1}^{N}\\|(J_{i}^{(t)}-J_{root}^{(t)})-(Ĵ_{i}^{(t)}-Ĵ_{root}^{(t)})\\|$\n",
    "\n",
    "Where N is the number of joints, and T the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the formula above, we need to align the root joints of the labels and the network output. In order to do that, I have defined a function (`substract_root_PJPE`) that substracts the root joint of each keypoint set (face, hands, body) in the corresponding keypoint set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "MPJPE = []\n",
    "h = model.init_hidden(batch_size)\n",
    "preds, inps, labls, lengs = [], [], [], []\n",
    "iscal, oscal = [], []\n",
    "mx, my, mz = [], [], []\n",
    "\n",
    "model.eval()\n",
    "for inputs_test, labels_test, lengths_test, is_test, os_test, mx_test, my_test, mz_test in test_loader:\n",
    "    \n",
    "    h = tuple([each.data for each in h])\n",
    "    inputs_test, labels_test, lengths_test = inputs_test.to(device), labels_test.to(device), lengths_test.to(device)\n",
    "    \n",
    "    output_test, h = model(inputs_test, h, lengths_test)\n",
    "    \n",
    "    e = [preds, inps, labls, lengs, iscal, oscal, mx, my, mz]\n",
    "    b = [output_test, inputs_test, labels_test, lengths_test, is_test, os_test, mx_test, my_test, mz_test]\n",
    "    for k in range(len(e)):\n",
    "        e[k].append(b[k])      \n",
    "    \n",
    "    test_loss = loss_function(nn.utils.rnn.pack_padded_sequence(output_test, lengths=lengths_test, batch_first=True,\n",
    "                                                               enforce_sorted=False).data,\n",
    "                             nn.utils.rnn.pack_padded_sequence(labels_test, lengths=lengths_test, batch_first=True,\n",
    "                                                               enforce_sorted=False).data)\n",
    "    test_losses.append(test_loss.item())\n",
    "    rooted_o, rooted_l = substract_root_PJPE(output_test), substract_root_PJPE(labels_test)\n",
    "    MPJPE.append(mpjpe(rooted_o, rooted_l, lengths_test))\n",
    "\n",
    "\n",
    "test_predictions = torch.cat(tuple(preds), dim=0)\n",
    "test_inputs = torch.cat(tuple(inps), dim=0)\n",
    "test_groundtruth = torch.cat(tuple(labls), dim=0)\n",
    "test_lengths = torch.cat(tuple(lengs), dim=0)\n",
    "test_inp_scale, test_out_scale = torch.cat(tuple(iscal), dim=0), torch.cat(tuple(oscal), dim=0)\n",
    "test_mx, test_my, test_mz = torch.cat(tuple(mx), dim=0), torch.cat(tuple(my), dim=0), torch.cat(tuple(mz), dim=0)\n",
    "MPJPE_total = np.mean(MPJPE)\n",
    "\n",
    "c_output = test_predictions.clone()\n",
    "c_labels = test_groundtruth.clone()\n",
    "for vid in range(c_labels.shape[0]): \n",
    "    c_output[vid].mul_(test_mz[vid,0])\n",
    "    c_labels[vid].mul_(test_mz[vid,1])\n",
    "    \n",
    "test_CK = correct_keypoints(c_output, c_labels, test_lengths)\n",
    "test_K = test_lengths.sum()*test_groundtruth.shape[2]\n",
    "\n",
    "test_PCK = test_CK/ test_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.2145 \n",
      "MPJPE: 0.4661 \n",
      "Test PCK: 12.489\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test loss: {np.mean(test_losses):.4f}\", f\"\\nMPJPE: {MPJPE_total:.4f}\",  f\"\\nTest PCK: {test_PCK*100:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results into a json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tr_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ce7cbdf59ac8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m results = {'train':{'inputs':tr_inputs.tolist(), 'predictions':tr_predictions.tolist(), 'labels':tr_groundtruth.tolist(), 'lengths':tr_lengths.tolist(), \n\u001b[0m\u001b[1;32m      2\u001b[0m                     'is':tr_inp_scale.tolist(), 'os':tr_out_scale.tolist(), 'mx':tr_mx.tolist(), 'my':tr_my.tolist(), 'mz':tr_mz.tolist()},\n\u001b[1;32m      3\u001b[0m           'validation':{'inputs':val_inputs.tolist(), 'predictions':val_predictions.tolist(), 'labels':val_groundtruth.tolist(), 'lengths':val_length.tolist(),\n\u001b[1;32m      4\u001b[0m                        'is':val_inp_scale.tolist(), 'os':val_out_scale.tolist(), 'mx':val_mx.tolist(), 'my':val_my.tolist(), 'mz':val_mz.tolist()},\n\u001b[1;32m      5\u001b[0m           'test':{'inputs':test_inputs.tolist(), 'predictions':test_predictions.tolist(), 'labels':test_groundtruth.tolist(), 'lengths':test_lengths.tolist(),\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tr_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "results = {'train':{'inputs':tr_inputs.tolist(), 'predictions':tr_predictions.tolist(), 'labels':tr_groundtruth.tolist(), 'lengths':tr_lengths.tolist(), \n",
    "                    'is':tr_inp_scale.tolist(), 'os':tr_out_scale.tolist(), 'mx':tr_mx.tolist(), 'my':tr_my.tolist(), 'mz':tr_mz.tolist()},\n",
    "          'validation':{'inputs':val_inputs.tolist(), 'predictions':val_predictions.tolist(), 'labels':val_groundtruth.tolist(), 'lengths':val_length.tolist(),\n",
    "                       'is':val_inp_scale.tolist(), 'os':val_out_scale.tolist(), 'mx':val_mx.tolist(), 'my':val_my.tolist(), 'mz':val_mz.tolist()},\n",
    "          'test':{'inputs':test_inputs.tolist(), 'predictions':test_predictions.tolist(), 'labels':test_groundtruth.tolist(), 'lengths':test_lengths.tolist(),\n",
    "                 'is':test_inp_scale.tolist(), 'os':test_out_scale.tolist(), 'mx':test_mx.tolist(), 'my':test_my.tolist(), 'mz':test_mz.tolist()}}\n",
    "with open('../../results/small_hands.json', 'w') as fp:\n",
    "    json.dump(results, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load results from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../results/small_hands_A.json', 'r') as j:\n",
    "    jd = json.load(j)\n",
    "    tr, val, test = jd['train'], jd['validation'], jd['test']\n",
    "    tr_inputs, tr_predictions, tr_groundtruth, tr_lengths, tr_inp_scale, tr_out_scale, tr_mx, tr_my, tr_mz = tuple(torch.tensor(tr[n]) for n in ['inputs', 'predictions',\n",
    "                                                                                              'labels', 'lengths',\n",
    "                                                                                               'is', 'os', 'mx', 'my', 'mz'])\n",
    "    val_inputs, val_predictions, val_groundtruth, val_length, val_inp_scale, val_out_scale, val_mx, val_my, val_mz = tuple(torch.tensor(val[n]) for n in ['inputs', 'predictions',\n",
    "                                                                                                  'labels', 'lengths',\n",
    "                                                                                                  'is', 'os', 'mx', 'my', 'mz'])\n",
    "    test_inputs, test_predictions, test_groundtruth, test_lengths, test_inp_scale, test_out_scale, test_mx, test_my, test_mz = tuple(torch.tensor(test[n]) for n in ['inputs', 'predictions',\n",
    "                                                                                                         'labels', 'lengths',\n",
    "                                                                                                         'is', 'os', 'mx', 'my', 'mz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768, 350, 84]), torch.Size([96, 350, 42]), torch.Size([96]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_inputs.shape, val_predictions.shape, test_lengths.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to better understanding of the results, I will plot some of the frames from the last batches on the training and validation, and also from testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_rotate(c_inputs, c_z, frames, frame):\n",
    "    c_inputs[:,::2].mul_(mom_x[1])\n",
    "    c_inputs[:,1::2].mul_(mom_y[1])\n",
    "    c_z.mul_(stdz)\n",
    "\n",
    "    handsXY = torch.chunk(c_inputs[frames, :], len(frames), dim=0)\n",
    "    handsZ = torch.chunk(c_z[frames, :], len(frames), dim=0)\n",
    "    \n",
    "    x = handsXY[frame].squeeze()[::2]\n",
    "    y = handsXY[frame].squeeze()[1::2]\n",
    "    z = handsZ[frame].squeeze()\n",
    "\n",
    "    rh0 = [[c[i] for i in [0, 1, 2, 3, 4]] for c in [x,y,z]]\n",
    "    rh1 = [[c[i] for i in [0, 5, 6, 7, 8]] for c in [x,y,z]]\n",
    "    rh2 = [[c[i] for i in [0, 9, 10, 11, 12]] for c in [x,y,z]]\n",
    "    rh3 = [[c[i] for i in [0, 13, 14, 15, 16]] for c in [x,y,z]]\n",
    "    rh4 = [[c[i] for i in [0, 17, 18, 19, 20]] for c in [x,y,z]]\n",
    "    lh0 = [[c[i+21] for i in [0, 1, 2, 3, 4]] for c in [x,y,z]]\n",
    "    lh1 = [[c[i+21] for i in [0, 5, 6, 7, 8]] for c in [x,y,z]]\n",
    "    lh2 = [[c[i+21] for i in [0, 9, 10, 11, 12]] for c in [x,y,z]]\n",
    "    lh3 = [[c[i+21] for i in [0, 13, 14, 15, 16]] for c in [x,y,z]]\n",
    "    lh4 = [[c[i+21] for i in [0, 17, 18, 19, 20]] for c in [x,y,z]]\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    def init():\n",
    "        ax.plot(rh0[0], rh0[2], rh0[1])\n",
    "        ax.plot(rh1[0], rh1[2], rh1[1])\n",
    "        ax.plot(rh2[0], rh2[2], rh2[1])\n",
    "        ax.plot(rh3[0], rh3[2], rh3[1])\n",
    "        ax.plot(rh4[0], rh4[2], rh4[1])\n",
    "        \n",
    "        ax.plot(lh0[0], lh0[2], lh0[1])\n",
    "        ax.plot(lh1[0], lh1[2], lh1[1])\n",
    "        ax.plot(lh2[0], lh2[2], lh2[1])\n",
    "        ax.plot(lh3[0], lh3[2], lh3[1])\n",
    "        ax.plot(lh4[0], lh4[2], lh4[1])\n",
    "        \n",
    "        lims = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()\n",
    "        spans = lims[0][1]-lims[0][0], lims[1][1]-lims[1][0], lims[2][1]-lims[2][0]\n",
    "        span = max(spans)\n",
    "        margins = [(span-s)/2 for  s in spans]\n",
    "        ax.set_xlim(lims[0][0]-margins[0], lims[0][1]+margins[0])\n",
    "        ax.set_ylim(lims[1][0]-margins[1], lims[1][1]+margins[1])\n",
    "        ax.set_zlim(lims[2][0]-margins[2], lims[2][1]+margins[2])\n",
    "\n",
    "        return fig,\n",
    "\n",
    "    def animate(i):\n",
    "        ax.view_init(elev=220., azim=3.6*i)\n",
    "        return fig,\n",
    "\n",
    "    # Animate\n",
    "    ani = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=100, interval=100, blit=True)    \n",
    "\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frames(predictions, groundtruth, inputs, video_n, frames, rot, train=False):\n",
    "    \n",
    "    inp = inputs.clone()\n",
    "    preds = predictions.clone()\n",
    "    grtr = groundtruth.clone()\n",
    "    \n",
    "    handsXY = torch.chunk(inp[video_n, frames, :], len(frames), dim=0)\n",
    "    pred_handsZ = torch.chunk(preds[video_n, frames, :], len(frames), dim=0)\n",
    "    true_handsZ = torch.chunk(grtr[video_n, frames, :], len(frames), dim=0)\n",
    "   \n",
    "    nrows = np.ceil(len(frames)/2)\n",
    "    fig = plt.figure(figsize=(15, 6*nrows))\n",
    "    fig2 = plt.figure(figsize=(15, 6*nrows))\n",
    "    for frame in range(len(frames)):\n",
    "        x = handsXY[frame].squeeze()[::2].tolist()\n",
    "        y = handsXY[frame].squeeze()[1::2].tolist()\n",
    "        pred_z = pred_handsZ[frame].squeeze().tolist()\n",
    "        true_z = true_handsZ[frame].squeeze().tolist()\n",
    "        \n",
    "        r = R.from_euler('y', rot, degrees=True)\n",
    "        \n",
    "        xyz1, xyz2 = np.asarray([c for c in zip(x, y, pred_z)]), np.asarray([c for c in zip(x, y, true_z)])\n",
    "        xyz1, xyz2 = r.apply(xyz1), r.apply(xyz2)\n",
    "        x1, x2 = xyz1[:,0], xyz2[:,0]\n",
    "        y1, y2 = xyz1[:,1], xyz2[:,1]\n",
    "        pred_z, true_z = xyz1[:,2], xyz2[:,2]\n",
    "        if not train:\n",
    "            print((x2.max()-x2.min())/(x1.max()-x1.min()))\n",
    "            x1 = x1*((x2.max()-x2.min())/(x1.max()-x1.min()))\n",
    "        \n",
    "        rh0 = tuple([[c[i] for i in [0, 1, 2, 3, 4]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        rh1 = tuple([[c[i] for i in [0, 5, 6, 7, 8]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        rh2 = tuple([[c[i] for i in [0, 9, 10, 11, 12]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        rh3 = tuple([[c[i] for i in [0, 13, 14, 15, 16]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        rh4 = tuple([[c[i] for i in [0, 17, 18, 19, 20]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        lh0 = tuple([[c[i+21] for i in [0, 1, 2, 3, 4]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        lh1 = tuple([[c[i+21] for i in [0, 5, 6, 7, 8]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        lh2 = tuple([[c[i+21] for i in [0, 9, 10, 11, 12]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        lh3 = tuple([[c[i+21] for i in [0, 13, 14, 15, 16]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        lh4 = tuple([[c[i+21] for i in [0, 17, 18, 19, 20]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        \n",
    "        ax = fig.add_subplot(nrows, 2, frame+1, projection='3d')\n",
    "\n",
    "        ax.plot(rh0[0][0], rh0[0][1], rh0[0][2])\n",
    "        ax.plot(rh1[0][0], rh1[0][1], rh1[0][2])\n",
    "        ax.plot(rh2[0][0], rh2[0][1], rh2[0][2])\n",
    "        ax.plot(rh3[0][0], rh3[0][1], rh3[0][2])\n",
    "        ax.plot(rh4[0][0], rh4[0][1], rh4[0][2])\n",
    "        \n",
    "        ax.plot(lh0[0][0], lh0[0][1], lh0[0][2])\n",
    "        ax.plot(lh1[0][0], lh1[0][1], lh1[0][2])\n",
    "        ax.plot(lh2[0][0], lh2[0][1], lh2[0][2])\n",
    "        ax.plot(lh3[0][0], lh3[0][1], lh3[0][2])\n",
    "        ax.plot(lh4[0][0], lh4[0][1], lh4[0][2])\n",
    "        \n",
    "        ax2 = fig2.add_subplot(nrows, 2, frame+1, projection='3d')\n",
    "        ax2.plot(rh0[1][0], rh0[1][1], rh0[1][2])\n",
    "        ax2.plot(rh1[1][0], rh1[1][1], rh1[1][2])\n",
    "        ax2.plot(rh2[1][0], rh2[1][1], rh2[1][2])\n",
    "        ax2.plot(rh3[1][0], rh3[1][1], rh3[1][2])\n",
    "        ax2.plot(rh4[1][0], rh4[1][1], rh4[1][2])\n",
    "        \n",
    "        ax2.plot(lh0[1][0], lh0[1][1], lh0[1][2])\n",
    "        ax2.plot(lh1[1][0], lh1[1][1], lh1[1][2])\n",
    "        ax2.plot(lh2[1][0], lh2[1][1], lh2[1][2])\n",
    "        ax2.plot(lh3[1][0], lh3[1][1], lh3[1][2])\n",
    "        ax2.plot(lh4[1][0], lh4[1][1], lh4[1][2])\n",
    "        \n",
    "        lims = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()\n",
    "        spans = lims[0][1]-lims[0][0], lims[1][1]-lims[1][0], lims[2][1]-lims[2][0]\n",
    "        span = max(spans)\n",
    "        margins = [(span-s)/2 for  s in spans]\n",
    "        ax.set_xlim(lims[0][0]-margins[0], lims[0][1]+margins[0])\n",
    "        ax.set_ylim(lims[1][0]-margins[1], lims[1][1]+margins[1])\n",
    "        ax.set_zlim(lims[2][0]-margins[2], lims[2][1]+margins[2])\n",
    "        \n",
    "        lims2 = ax2.get_xlim(), ax2.get_ylim(), ax2.get_zlim()\n",
    "        spans2 = lims2[0][1]-lims2[0][0], lims2[1][1]-lims2[1][0], lims2[2][1]-lims2[2][0]\n",
    "        span2 = max(spans2)\n",
    "        margins2 = [(span2-s)/2 for  s in spans2]\n",
    "        ax2.set_xlim(lims2[0][0]-margins2[0], lims2[0][1]+margins2[0])\n",
    "        ax2.set_ylim(lims2[1][0]-margins2[1], lims2[1][1]+margins2[1])\n",
    "        ax2.set_zlim(lims2[2][0]-margins2[2], lims2[2][1]+margins2[2])\n",
    "\n",
    "        ax.view_init(elev=-65., azim=-90.)\n",
    "        ax2.view_init(elev=-65., azim=-90.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single frame\n",
    "On the first cell you can select which frames you want to plot and from which video of the batch. On the second you select which frame of the previosly selected you want to plot, specifying its index on the declared \"frames\" list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last batches of training -output, inputs, labels-.\n",
    "vid = 1\n",
    "frames = [100]\n",
    "\n",
    "c_inputs = training_kp[vid].clone()\n",
    "c_labels = training_lbl[vid].clone()\n",
    "c_inputs[:,::2].mul_(training_inpscale[vid, 0])\n",
    "c_inputs[:,1::2].mul_(training_inpscale[vid, 1])\n",
    "c_labels.mul_(training_outscale[vid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(plot_and_rotate(c_inputs, c_output, frames, 0).to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_inputs = tr_inputs[vid].clone()\n",
    "HTML(plot_and_rotate(c_inputs, c_labels, frames, 0).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the same process for the last test batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice of frames\n",
    "Now let's plot a sequence of frames of the selected video. We will plot both the groundtruth and the predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [i for i in range(1,9,2)]\n",
    "video_n = 121\n",
    "\n",
    "c_inputs = tr_inputs.clone()\n",
    "c_output = tr_predictions.clone()\n",
    "c_labels = tr_groundtruth.clone()\n",
    "\n",
    "for vid in range(c_labels.shape[0]): \n",
    "    c_inputs[vid,:,::2].mul_(tr_inp_scale[vid, 0])\n",
    "    c_inputs[vid,:,1::2].mul_(tr_inp_scale[vid, 1])\n",
    "    c_output[vid].mul_(tr_out_scale[vid])\n",
    "    c_labels[vid].mul_(tr_out_scale[vid])\n",
    "    \n",
    "    c_inputs[vid,:,::2].mul_(tr_mx[vid, 1])\n",
    "    c_inputs[vid,:,1::2].mul_(tr_my[vid, 1])\n",
    "    c_output[vid].mul_(tr_mz[vid])\n",
    "    c_labels[vid].mul_(tr_mz[vid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bbbb394eb84cb28bd7c4ccad2105e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba18efa39514d98a7a990e491e6aa93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_frames(c_output, c_labels, c_inputs, video_n, frames, -90, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [i for i in range(100,180,20)]\n",
    "video_n = 35\n",
    "\n",
    "c_inputs = test_inputs.clone()\n",
    "c_output = test_predictions.clone()\n",
    "c_labels = test_groundtruth.clone()\n",
    "\n",
    "for vid in range(c_labels.shape[0]): \n",
    "    c_inputs[vid,:,::2].mul_(test_inp_scale[vid, 0])\n",
    "    c_inputs[vid,:,1::2].mul_(test_inp_scale[vid, 1])\n",
    "    c_output[vid].mul_(test_out_scale[vid,0])\n",
    "    c_labels[vid].mul_(test_out_scale[vid,1])\n",
    "    \n",
    "    c_inputs[vid,:,::2].mul_(test_mx[vid, 1])\n",
    "    c_inputs[:,:,1::2].mul_(test_my[vid, 1])\n",
    "    c_output[vid].mul_(test_mz[vid,0])\n",
    "    c_labels[vid].mul_(test_mz[vid,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9737a7a0a5a440a0a83457b4bab18537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935b6d677ba1427e91af4507c0d30086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_frames(c_output, c_labels, c_inputs, video_n, frames, -90, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [i for i in range(1,9,2)]\n",
    "video_n = 151\n",
    "\n",
    "c_inputs = torch.tensor(dataset)\n",
    "c_output = torch.tensor(groundtruth)\n",
    "c_labels = torch.tensor(groundtruth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9854879eeaf24afd8f50cc337f6dfb6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88aa9a7337684c4891d8ba5f7a180fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "plot_frames(c_output, c_labels, c_inputs, video_n, frames, -90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [i for i in range(1,9,2)]\n",
    "video_n = 17\n",
    "\n",
    "c_inputs = val_inputs.clone()\n",
    "c_output = val_predictions.clone()\n",
    "c_labels = val_groundtruth.clone()\n",
    "\n",
    "for vid in range(c_labels.shape[0]): \n",
    "    c_inputs[vid,:,::2].mul_(val_inp_scale[vid, 0])\n",
    "    c_inputs[vid,:,1::2].mul_(val_inp_scale[vid, 1])\n",
    "    c_output[vid].mul_(val_out_scale[vid])\n",
    "    c_labels[vid].mul_(val_out_scale[vid])\n",
    "    \n",
    "    c_inputs[vid,:,::2].mul_(val_mom_x[vid, 1])\n",
    "    c_inputs[:,:,1::2].mul_(val_mom_y[vid, 1])\n",
    "    c_output[vid].mul_(val_mom_z[vid])\n",
    "    c_labels[vid].mul_(val_mom_z[vid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3d97c68a8b4afb91c9f19a1565a9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adca4f266a3b4f1e8745c4ae18086975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_frames(c_output, c_labels, c_inputs, video_n, frames, -90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
