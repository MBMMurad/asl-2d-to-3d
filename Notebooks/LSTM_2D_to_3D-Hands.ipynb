{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D to 3D LSTM\n",
    "\n",
    "This is the first approach to try to estimate 3D points coordinates from 2D keypoints extracted with Openpose. Here I will build a simple LSTM to perform the task over the Panoptic Studio dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch utilities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Plotting utilities\n",
    "%matplotlib widget\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from timeit import default_timer as timer\n",
    "import pyprind\n",
    "\n",
    "# Directory and file utilities\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = { 0:'one video', 1:'one signer', 2:'all signers'}\n",
    "MODE =  modes[0]\n",
    "video = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition\n",
    "Now I will define some functions in order to parse and organise the data, and later convert it to pytorch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is structured as follows: in the dataset directory there are several folders, each folder corresponds to a recording; each of these folders contains a folder with the audio, folders with face, hands and body keypoints estimations for each frame, and a folder with the video recorded from different views.\n",
    "\n",
    "In this first approach I will be using the keypoints estimations. Every keypoint folder (face, hands or body) is organized the same way: it contains a json per frame of the video, which includes the 3D keypoints estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_keypoints` will go through each folder in the dataset directory and retrieve the face keypoints, the hands keypoints and the body keypoints. It will separate them into input (2D coordinates per joint per frame) and grountruth (third coordinate to estimate for each input 2D keypoint). \n",
    "The input will be of shape $([n videos, seq len, input size])$, where *seq_len* = number of frames, and *input_size* = face + hands + body keypoints, that is (70+(21+21)+26)x2 -multiplied by 2 because there are x and y coordinates-. The groundtruth (label) data will be of the same shape, except that the last dimension size will not be multiplied by 2 (there's only one coordinate to estimate).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints(data_path):\n",
    "    dataset = []\n",
    "    groundtruth = []\n",
    "    # Look over just the folders inside the directory\n",
    "    just_folders = filter(lambda x: isdir(join(data_path, x)), listdir(data_path))\n",
    "    for p in list(map(lambda x: join(data_path, x), just_folders)): \n",
    "        # Gets 2 list of n_frames lists, one for the 2D coordinates and one for the third coordinate.\n",
    "        # Each list of the n_frames lists contains, either the (x and y) or the z of each keypoint for the face(first line), hands(second), body(third).\n",
    "        # e.g. the first line will result in [[x1,y1,x2,y2...x70,y70]sub1...[x1,y1...x70,y70]subN], [[z1,z2...z70]sub1...[z1..z70]subN]\n",
    "        # Actually, as there will be two of each list above because there are two people en each video.\n",
    "        hands_2d, hands_3d = get_hands(p)\n",
    "        \n",
    "        # Concatenates the coordinates for the face, hands and body on the last dimension, for each person.\n",
    "        vid_input_p1, vid_input_p2 = hands_2d\n",
    "        vid_labels_p1, vid_labels_p2 = hands_3d\n",
    "        \n",
    "        dataset.append(vid_input_p1)\n",
    "        dataset.append(vid_input_p2)\n",
    "        groundtruth.append(vid_labels_p1)\n",
    "        groundtruth.append(vid_labels_p2)\n",
    "        print(f'Completed folder {p}')\n",
    "    return dataset, groundtruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hands json contains the *landmarks* field inside both *left_hand* and *right_hand* field. As there are some frames that may not have one of the hands estimated, I've had to put some exception handling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hands(path):\n",
    "    hand_2D_seq = ([], [])\n",
    "    hand_3D_seq = ([], [])\n",
    "    paths = map(lambda x: join(path, 'hdHand3d', x), sorted(listdir(join(path, 'hdHand3d'))))\n",
    "    files = list(filter(lambda x: isfile(x), paths))\n",
    "    for f in files: # The first and the last frames of these folders are blank.\n",
    "        with open(f, 'r') as j:\n",
    "            json_array = json.load(j)\n",
    "            i = 0\n",
    "            for person in json_array['people']:\n",
    "                if person['id'] != -1:\n",
    "                    try:\n",
    "                        # Separate x,y from z\n",
    "                        hands= [[person[hand]['landmarks'][c] \n",
    "                                 for c in range(len(person['right_hand']['landmarks'])) if (c+1)%3!=0] \n",
    "                                 for hand in ['left_hand', 'right_hand']]\n",
    "                        hand_2D_seq[person['id']].append(hands[0]+hands[1])\n",
    "                        \n",
    "                        hands_3d = [person[hand]['landmarks'][2::3] \n",
    "                                    for hand in ['left_hand', 'right_hand']]\n",
    "                        hand_3D_seq[person['id']].append(hands_3d[0]+hands_3d[1])\n",
    "                    \n",
    "                    except Exception as e: # In case left_hand or right_hand keys don't exist.\n",
    "                        if 'left_hand' in str(e): \n",
    "                            try: # Just put a 0., 0., 0. estimation for each keypoint of the left_hand\n",
    "                                hands = hand_2D_seq[person['id']][-1][:42]+[person['right_hand']['landmarks'][c] \n",
    "                                                                 for c in range(len(person['right_hand']['landmarks'])) if (c+1)%3!=0]\n",
    "                                hands_3d = hand_3D_seq[person['id']][-1][:21]+person['right_hand']['landmarks'][2::3]\n",
    "                            except: # In case neither left_hand nor right_hand exist\n",
    "                                hands = hand_2D_seq[person['id']][-1]\n",
    "                                hands_3d = hand_3D_seq[person['id']][-1]\n",
    "                        elif 'right_hand' in str(e): # Just put a 0., 0., 0. estimation for each keypoint of the right_hand\n",
    "                            hands = [person['left_hand']['landmarks'][c] \n",
    "                                     for c in range(len(person['left_hand']['landmarks'])) if (c+1)%3!=0]+hand_2D_seq[person['id']][-1][42:]\n",
    "                            hands_3d = person['left_hand']['landmarks'][2::3]+hand_3D_seq[person['id']][-1][21:]\n",
    "\n",
    "                        hand_2D_seq[person['id']].append(hands)\n",
    "                        hand_3D_seq[person['id']].append(hands_3d)\n",
    "                    i+=1\n",
    "                    pid = person['id']\n",
    "            if i<2:\n",
    "                hand_2D_seq[1-pid].append(hand_2D_seq[1-pid][-1])\n",
    "                hand_3D_seq[1-pid].append(hand_3D_seq[1-pid][-1])\n",
    "    print('Hands completed.')\n",
    "    return hand_2D_seq, hand_3D_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190419_asl2\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190419_asl4\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190419_asl5\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl1\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl2\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl3\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl5\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl7\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl9\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190425_asl91\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl1\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl2\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl3\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl4\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl5\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl6\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl7\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl8\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl910\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl911\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl912\n",
      "Hands completed.\n",
      "Completed folder ../../../data/DB keypoints/190611_asl913\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../../data/DB keypoints'\n",
    "dataset, groundtruth = get_keypoints(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_seq(dataset):\n",
    "    max_seq = max([len(x) for x in dataset])\n",
    "    seq_lengths = []\n",
    "    for seq in dataset:\n",
    "        seq_lengths.append(len(seq))\n",
    "        for i in range(max_seq-len(seq)):\n",
    "            seq.append([np.nan for j in range(len(seq[0]))])\n",
    "        \n",
    "    return max_seq, seq_lengths\n",
    "\n",
    "max_seq, seq_lengths = padding_seq(dataset)\n",
    "_, _ = padding_seq(groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 8752, 84) (44, 8752, 42) (44,)\n"
     ]
    }
   ],
   "source": [
    "# From python lists to numpy ndarray.\n",
    "dataset = np.asarray(dataset)\n",
    "groundtruth = np.asarray(groundtruth)\n",
    "lengths = np.asarray(seq_lengths)\n",
    "print(dataset.shape, groundtruth.shape, lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../pickles/hands_data.npy', dataset)\n",
    "np.save('../../pickles/hands_ground.npy', groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../pickles/hands_lengths.npy', lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset structuring\n",
    "Now let's convert the lists obtained to Pytorch tensors and organise them in train, validation and test datasets. \n",
    "First, I will define a padding function in order to make all the sequences of video frames the same length, so I can train the LSTM in batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8752\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load('../../pickles/hands_data.npy', allow_pickle=True)\n",
    "groundtruth = np.load('../../pickles/hands_ground.npy', allow_pickle=True)\n",
    "lengths = np.load('../../pickles/hands_lengths.npy', allow_pickle=True)\n",
    "max_seq = dataset.shape[1]\n",
    "print(max_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(516,)\n"
     ]
    }
   ],
   "source": [
    "if MODE == 'one video':\n",
    "    dataset, groundtruth = dataset[video], groundtruth[video]\n",
    "    chunks_d = np.split(dataset, 547, axis=0)[:lengths[video]//16]\n",
    "    chunks_g = np.split(groundtruth, 547, axis=0)[:lengths[video]//16]\n",
    "    chunks_d = tuple(np.expand_dims(c, axis=0) for c in chunks_d)\n",
    "    chunks_g = tuple(np.expand_dims(c, axis=0) for c in chunks_g)\n",
    "    \n",
    "    dataset = np.concatenate(chunks_d, axis=0)\n",
    "    groundtruth = np.concatenate(chunks_g, axis=0)\n",
    "    lengths = np.asarray([16 for i in range(lengths[video]//16)])\n",
    "    print(lengths.shape)\n",
    "\n",
    "elif MODE == 'one signer':\n",
    "    dataset, groundtruth, lengths = dataset[20::2, :-2], groundtruth[20::2, :-2], lengths[20::2]\n",
    "    print(lengths)\n",
    "    lengths = np.asarray([min(8750, x) for x in lengths])\n",
    "    print(lengths)\n",
    "    chunks_d, chunks_g = np.split(dataset, 35, axis=1), np.split(groundtruth, 35, axis=1)\n",
    "    \n",
    "    dataset, groundtruth = np.concatenate(chunks_d, axis=0), np.concatenate(chunks_g, axis=0)\n",
    "    lengths = np.concatenate(tuple([250 if j*250<=lengths[i] \n",
    "                                    else (lengths[i]%250 if (j-1)*250<lengths[i] \n",
    "                                          else 0) for i in range(12)] for j in range(1,36)), axis=0)\n",
    "    print(dataset.shape, groundtruth.shape, lengths.shape)\n",
    "    \n",
    "elif MODE == 'all signers':\n",
    "    dataset, groundtruth = dataset[:, :-2], groundtruth[:, :-2]\n",
    "    lengths = np.asarray([min(8750, x) for x in lengths])\n",
    "    chunks_d, chunks_g = np.split(dataset, 25, axis=1), np.split(groundtruth, 25, axis=1)\n",
    "    \n",
    "    dataset, groundtruth = np.concatenate(chunks_d, axis=0), np.concatenate(chunks_g, axis=0)\n",
    "    lengths = np.concatenate(tuple([350 if j*350<=lengths[i]\n",
    "                                    else (lengths[i]%350 if (j-1)*350<lengths[i] \n",
    "                                          else 0) for i in range(44)] for j in range(1,26)), axis=0)\n",
    "    print(dataset.shape, groundtruth.shape, lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(516, 16, 84) (516, 16, 42) (516,)\n"
     ]
    }
   ],
   "source": [
    "# Clean all NaN videos\n",
    "dataset = np.delete(dataset, np.where(lengths==0), axis=0)\n",
    "groundtruth = np.delete(groundtruth, np.where(lengths==0), axis=0)\n",
    "lengths = np.delete(lengths, np.where(lengths==0), axis=0)\n",
    "\n",
    "print(dataset.shape, groundtruth.shape, lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(516, 16, 84) (516, 16, 42) (516,)\n"
     ]
    }
   ],
   "source": [
    "def align(tensor, coordinates=1):\n",
    "    for n_vid in range(tensor.shape[0]):\n",
    "        max_value = [np.nanmax(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        min_value = [np.nanmin(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        center = [(max_value[i]+min_value[i])/2 for i in range(coordinates)]\n",
    "        for j in range(coordinates):\n",
    "            subtensor = tensor[n_vid, :, j::coordinates]\n",
    "            subtensor[:] = np.subtract(subtensor, center[j])\n",
    "\n",
    "align(dataset,2)\n",
    "align(groundtruth)\n",
    "print(dataset.shape, groundtruth.shape, lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(516, 16, 84) (516, 16, 42)\n"
     ]
    }
   ],
   "source": [
    "r = R.from_euler('y', 110, degrees=True)\n",
    "shapes = dataset.shape\n",
    "dataset = dataset.reshape(-1, 2)\n",
    "groundtruth = groundtruth.reshape(-1,1)\n",
    "xyz = np.concatenate((dataset, groundtruth), axis=1)\n",
    "xyz = r.apply(xyz)\n",
    "dataset, groundtruth = xyz[:, :2].reshape(shapes), xyz[:,2].reshape(shapes[0], shapes[1],42)\n",
    "\n",
    "print(dataset.shape, groundtruth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(516, 2)\n"
     ]
    }
   ],
   "source": [
    "def norm_uniform(tensor, coordinates=1, factor=None):\n",
    "    scale = []\n",
    "    mean_ranges = []\n",
    "    for n_vid in range(tensor.shape[0]):\n",
    "        coord_scale = []\n",
    "        max_value = [np.nanmax(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        min_value = [np.nanmin(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        center = [(max_value[i]+min_value[i])/2 for i in range(coordinates)]\n",
    "        ranges = np.ndarray((tensor.shape[1],coordinates))\n",
    "        for n_frame in range(tensor.shape[1]):\n",
    "            rang = [np.nanmax(tensor[n_vid, n_frame,i::coordinates])-np.nanmin(tensor[n_vid, n_frame,i::coordinates]) for i in range(coordinates)]\n",
    "            ranges[n_frame] = np.asarray(rang)\n",
    "        mean_range = [np.nanmean(ranges[:,i]) for i in range(coordinates)]\n",
    "        for j in range(coordinates):\n",
    "            subtensor = tensor[n_vid, :, j::coordinates]\n",
    "            subtensor[:] = np.subtract(subtensor, center[j])\n",
    "            if factor is not None:\n",
    "                subtensor[:] = np.divide(subtensor, factor[n_vid])\n",
    "            else:\n",
    "                subtensor[:] = np.divide(subtensor, max_value[j]-center[j])\n",
    "            coord_scale.append((max_value[j]-center[j] if factor is None else factor[n_vid]))\n",
    "        scale.append(coord_scale)\n",
    "        mean_ranges.append(mean_range)\n",
    "    return mean_ranges\n",
    "input_scale = np.asarray(norm_uniform(dataset,2)).squeeze()\n",
    "print(input_scale.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(516, 2) (516, 2)\n"
     ]
    }
   ],
   "source": [
    "def normalize(tensor, coordinates=1, std=None):\n",
    "    moments = []\n",
    "    std_centroids = []\n",
    "    for n_vid in range(tensor.shape[0]):\n",
    "        coord_moments = []\n",
    "        mean_value = [np.nanmean(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        std_value = [np.nanstd(tensor[n_vid, :,i::coordinates]) for i in range(coordinates)]\n",
    "        centroids = np.ndarray((tensor.shape[1],coordinates))\n",
    "        for n_frame in range(tensor.shape[1]):\n",
    "            centroid = [np.nanmean(tensor[n_vid, n_frame, i::coordinates]) for i in range(coordinates)]\n",
    "            centroids[n_frame] = np.asarray(centroid)\n",
    "        std_centroid = [np.nanstd(centroids[:,i]) for i in range(coordinates)]\n",
    "        if std is not None:\n",
    "            std_value = [std[n_vid]]\n",
    "        for j in range(coordinates):\n",
    "            subtensor = tensor[:, :, j::coordinates]\n",
    "            subtensor[:] = np.subtract(subtensor, mean_value[j])\n",
    "            subtensor[:] = np.divide(subtensor, std_value[j])\n",
    "            coord_moments.append((mean_value[j], std_value[j]))\n",
    "        moments.append(coord_moments)\n",
    "        std_centroids.append(std_centroid)\n",
    "    return moments, std_centroids\n",
    "\n",
    "moments, std_centroids = normalize(dataset, 2)\n",
    "mom_x, mom_y = np.asarray(moments)[:, 0], np.asarray(moments)[:, 1]\n",
    "std_centroids = np.asarray(std_centroids)\n",
    "print(mom_x.shape, mom_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(516, 16, 84) (516, 16, 42) (516,)\n",
      "(516, 2) (516, 2) (516, 2)\n"
     ]
    }
   ],
   "source": [
    "# Randomly shuffle videos\n",
    "permutation = np.random.permutation(dataset.shape[0])\n",
    "dataset, groundtruth, lengths = dataset[permutation], groundtruth[permutation], lengths[permutation]\n",
    "input_scale, mom_x, mom_y = input_scale[permutation], mom_x[permutation], mom_y[permutation]\n",
    "std_centroids = std_centroids[permutation]\n",
    "\n",
    "print(dataset.shape, groundtruth.shape, lengths.shape)\n",
    "print(input_scale.shape, mom_x.shape, mom_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(413, 16, 84) (51, 16, 84) (52, 16, 84)\n",
      "(413, 16, 42) (51, 16, 42) (52, 16, 42)\n",
      "(413,) (51,) (52,)\n"
     ]
    }
   ],
   "source": [
    "l1, l2 = len(dataset), len(groundtruth)\n",
    "p1, p2 = 0.8, 0.9\n",
    "# Split in train, validation and test\n",
    "training_kp, val_kp, test_kp = dataset[:round(p1*l1)], dataset[round(p1*l1):round(p2*l1)], dataset[round(p2*l1):]\n",
    "training_lbl, val_lbl, test_lbl = groundtruth[:round(p1*l2)], groundtruth[round(p1*l2):round(p2*l2)], groundtruth[round(p2*l2):]\n",
    "training_lengths, val_lengths, test_lengths = lengths[:round(p1*l1)], lengths[round(p1*l1):round(p2*l1)], lengths[round(p2*l1):]\n",
    "training_inpscale, val_inpscale, test_inpscale = input_scale[:round(p1*l1)], input_scale[round(p1*l1):round(p2*l1)], input_scale[round(p2*l1):]\n",
    "training_mom_x, val_mom_x, test_mom_x = mom_x[:round(p1*l1)], mom_x[round(p1*l1):round(p2*l1)], mom_x[round(p2*l1):]\n",
    "training_mom_y, val_mom_y, test_mom_y = mom_y[:round(p1*l1)], mom_y[round(p1*l1):round(p2*l1)], mom_y[round(p2*l1):]\n",
    "\n",
    "training_std_centroids, val_std_centroids, test_std_centroids = std_centroids[:round(p1*l1)], std_centroids[round(p1*l1):round(p2*l1)], std_centroids[round(p2*l1):]\n",
    "\n",
    "print(training_kp.shape, val_kp.shape, test_kp.shape)\n",
    "print(training_lbl.shape, val_lbl.shape, test_lbl.shape)\n",
    "print(training_lengths.shape, val_lengths.shape, test_lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(413,) (51,)\n",
      "(413,) (51,)\n"
     ]
    }
   ],
   "source": [
    "training_outscale = np.asarray(norm_uniform(training_lbl)).squeeze()\n",
    "val_outscale_t = np.asarray(norm_uniform(val_lbl)).squeeze()\n",
    "test_outscale_t = np.asarray(norm_uniform(test_lbl)).squeeze()\n",
    "print(training_outscale.shape, val_outscale_t.shape)\n",
    "\n",
    "training_mom_z = np.asarray(normalize(training_lbl)[0])[:, 0, 1]\n",
    "val_mom_z_t = np.asarray(normalize(val_lbl)[0])[:, 0, 1]\n",
    "test_mom_z_t = np.asarray(normalize(test_lbl)[0])[:, 0, 1]\n",
    "print(training_mom_z.shape, val_mom_z_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51,) (52,)\n",
      "(51,) (52,)\n"
     ]
    }
   ],
   "source": [
    "alpha = LinearRegression(fit_intercept=True)#make_pipeline(PolynomialFeatures(9), LinearRegression(fit_intercept=True))\n",
    "alpha.fit(training_inpscale, training_outscale)\n",
    "val_outscale = alpha.predict(val_inpscale)\n",
    "test_outscale = alpha.predict(test_inpscale)\n",
    "print(val_outscale.shape, test_outscale.shape)\n",
    "\n",
    "sigma = LinearRegression(fit_intercept=True)#make_pipeline(PolynomialFeatures(9), LinearRegression(fit_intercept=True))\n",
    "train_X = np.concatenate((training_mom_x[:,1, np.newaxis], training_mom_y[:,1, np.newaxis]), axis=1)\n",
    "sigma.fit(train_X, training_mom_z)\n",
    "val_X = np.concatenate((val_mom_x[:,1,np.newaxis], val_mom_y[:,1,np.newaxis]), axis=1)\n",
    "test_X = np.concatenate((test_mom_x[:,1,np.newaxis], test_mom_y[:,1,np.newaxis]), axis=1)\n",
    "val_mom_z = sigma.predict(val_X)\n",
    "test_mom_z = sigma.predict(test_X)\n",
    "print(val_mom_z.shape, test_mom_z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356b1826ba8e4e929d06bcddf3508533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f84c30ab450>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(training_inpscale[:,0], training_inpscale[:,1], training_outscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abf1fbc1e5043a99017c7e1b2510701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f84c582b4d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(training_std_centroids[:,0], training_std_centroids[:,1], training_mom_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d83a69d2ea46c99008a1848fc0a8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f84c56c7d90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(training_std_centroids[:,0], training_mom_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc6da98ef7a4ef3ad306b1086c6b678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f84c563f750>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(training_std_centroids[:,1], training_mom_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.28879856 10.60725876 11.25517763 20.01651073 21.7298516  12.30238393\n",
      " 21.807535   10.77036985 28.03584354 16.94330454  9.17330178 18.49562579\n",
      " 19.43042391 16.77428076 16.77787181 20.61507273 19.45553063 12.58631626\n",
      " 18.7659651  16.14926278 10.20822807 10.7267783   7.32520864 11.27962412\n",
      " 13.14271877 13.97670675 11.62229522 15.6417069  56.0330848  21.01964659\n",
      " 16.33170024 10.65520121 22.80618024 16.26552062 14.83890187 17.09533569\n",
      " 27.46729224 33.36211366 15.05250687 10.2603747  15.22201936 12.16820421\n",
      " 11.2091201  13.35645059 15.05095632 12.25212931  9.56907763 12.70788033\n",
      " 11.11017518 12.37227989 21.78384677] [15.41856983 17.81377227 15.97721053 20.22363501 17.82733117 13.67843542\n",
      " 15.01052285 33.21937553 16.65156694 16.0070928  17.4915489  13.88598386\n",
      " 16.06578406  8.89477119 11.02447018 19.63415222 35.01345454 13.84103702\n",
      " 14.39612781 13.99369401 20.20369175 14.43100453 27.52914449 19.23487302\n",
      " 15.91382927 14.20209722 11.4295382  14.53999664 20.79500842 14.68029544\n",
      " 11.37819306 10.47025472 19.70529362 11.18218621 31.41171309 22.11924542\n",
      " 12.43955021 20.12798401 10.27492785 16.34049791  7.36796841 22.76806907\n",
      " 23.83351557 19.10037378 21.15002892 26.2642927  12.37460304 30.56975807\n",
      " 11.80508443 10.50506399 10.85274195 16.14129554]\n",
      "[0.63765883 0.6118175  0.93461989 1.08236342 1.65152959 0.59051013\n",
      " 1.48773663 0.69080244 1.92270334 0.60314797 0.95538529 0.88966102\n",
      " 1.29898967 0.85074701 0.96252393 1.19803037 0.99965036 0.71835515\n",
      " 0.92087815 1.4984464  0.82409471 0.75341462 1.26693668 0.96148284\n",
      " 1.45497094 0.76691995 1.12952892 0.82498628 2.15804133 0.52509004\n",
      " 0.94342856 1.00293656 1.44834746 0.5974226  1.16442934 1.07623929\n",
      " 1.42961441 0.96445133 0.7354814  0.8981692  1.0880913  1.015185\n",
      " 0.81636868 0.43751285 3.41803365 0.73926873 0.84790282 1.18560842\n",
      " 0.92113634 1.30687312 1.1476794 ] [0.39191789 1.41275826 0.90835549 0.79448306 1.24409655 0.63371267\n",
      " 1.72368141 1.16619866 0.39472048 1.73188958 0.98810787 1.18931243\n",
      " 0.99256543 0.93819016 1.00443184 1.06469789 1.29956628 0.61459806\n",
      " 0.84313368 1.41668534 0.92538793 0.98065567 1.36927902 0.72402594\n",
      " 1.05874994 0.93385274 1.26688478 0.97053889 1.08519877 0.72122977\n",
      " 0.84931002 1.11090629 1.21619369 0.8182433  1.78268123 0.98231013\n",
      " 0.66868196 0.88177377 0.98687814 1.13568934 1.0026234  1.12433012\n",
      " 1.40940193 0.48466381 1.38321993 1.33732575 0.69141146 1.3448376\n",
      " 0.67869136 1.01938945 0.70396274 1.63220083]\n",
      "54.352753474048065 34.60587807326301\n",
      "0.23167952525336874 0.10303585314308565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(val_outscale_t, test_outscale_t)\n",
    "print(val_mom_z_t, test_mom_z_t)\n",
    "print(mean_squared_error(val_outscale, val_outscale_t), mean_squared_error(test_outscale, test_outscale_t))\n",
    "print(mean_squared_error(val_mom_z, val_mom_z_t), mean_squared_error(test_mom_z, test_mom_z_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 2) (52, 2)\n"
     ]
    }
   ],
   "source": [
    "val_outscale = np.concatenate((val_outscale[:,np.newaxis], val_outscale_t[:,np.newaxis]), axis=1)\n",
    "test_outscale = np.concatenate((test_outscale[:,np.newaxis], test_outscale_t[:,np.newaxis]), axis=1)\n",
    "\n",
    "val_mom_z = np.concatenate((val_mom_z[:,np.newaxis], val_mom_z_t[:,np.newaxis]), axis=1)\n",
    "test_mom_z = np.concatenate((test_mom_z[:,np.newaxis], test_mom_z_t[:,np.newaxis]), axis=1)\n",
    "print(val_outscale.shape, test_mom_z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([413, 16, 84]) torch.Size([51, 16, 42]) torch.Size([52])\n",
      "torch.Size([413, 2]) torch.Size([413])\n"
     ]
    }
   ],
   "source": [
    "# From python lists to pytorch tensors.\n",
    "training_kp, val_kp, test_kp = torch.tensor(np.nan_to_num(training_kp), dtype=torch.float32), torch.tensor(np.nan_to_num(val_kp), dtype=torch.float32), torch.tensor(np.nan_to_num(test_kp), dtype=torch.float32)\n",
    "training_lbl, val_lbl, test_lbl = torch.tensor(np.nan_to_num(training_lbl), dtype=torch.float32), torch.tensor(np.nan_to_num(val_lbl), dtype=torch.float32), torch.tensor(np.nan_to_num(test_lbl), dtype=torch.float32)\n",
    "training_lengths, val_lengths, test_lengths = torch.tensor(np.nan_to_num(training_lengths), dtype=torch.float32), torch.tensor(np.nan_to_num(val_lengths), dtype=torch.float32), torch.tensor(np.nan_to_num(test_lengths), dtype=torch.float32)\n",
    "training_inpscale, val_inpscale, test_inpscale = torch.tensor(np.nan_to_num(training_inpscale), dtype=torch.float32), torch.tensor(np.nan_to_num(val_inpscale), dtype=torch.float32), torch.tensor(np.nan_to_num(test_inpscale), dtype=torch.float32)\n",
    "training_outscale, val_outscale, test_outscale = torch.tensor(np.nan_to_num(training_outscale), dtype=torch.float32), torch.tensor(np.nan_to_num(val_outscale), dtype=torch.float32), torch.tensor(np.nan_to_num(test_outscale), dtype=torch.float32)\n",
    "training_mom_x, val_mom_x, test_mom_x = torch.tensor(np.nan_to_num(training_mom_x), dtype=torch.float32), torch.tensor(np.nan_to_num(val_mom_x), dtype=torch.float32), torch.tensor(np.nan_to_num(test_mom_x), dtype=torch.float32)\n",
    "training_mom_y, val_mom_y, test_mom_y = torch.tensor(np.nan_to_num(training_mom_y), dtype=torch.float32), torch.tensor(np.nan_to_num(val_mom_y), dtype=torch.float32), torch.tensor(np.nan_to_num(test_mom_y), dtype=torch.float32)\n",
    "training_mom_z, val_mom_z, test_mom_z = torch.tensor(np.nan_to_num(training_mom_z), dtype=torch.float32), torch.tensor(np.nan_to_num(val_mom_z), dtype=torch.float32), torch.tensor(np.nan_to_num(test_mom_z), dtype=torch.float32)\n",
    "\n",
    "print(training_kp.shape, val_lbl.shape, test_lengths.shape)\n",
    "print(training_inpscale.shape, training_outscale.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we define the batch_size and put the datasets in DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f13e86b7550>\n"
     ]
    }
   ],
   "source": [
    "train_data = TensorDataset(training_kp, training_lbl, training_lengths,\n",
    "                          training_inpscale, training_outscale,\n",
    "                          training_mom_x, training_mom_y, training_mom_z)\n",
    "val_data = TensorDataset(val_kp, val_lbl, val_lengths,\n",
    "                        val_inpscale, val_outscale,\n",
    "                        val_mom_x, val_mom_y, val_mom_z)\n",
    "test_data = TensorDataset(test_kp, test_lbl, test_lengths,\n",
    "                         test_inpscale, test_outscale,\n",
    "                         test_mom_x, test_mom_y, test_mom_z)\n",
    "\n",
    "batch_size = 25\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a GPU available we set our device to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print some examples to see whether it is loaded correctly or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 16, 84]) torch.Size([25, 16, 42]) torch.Size([25]) torch.Size([25, 2]) torch.Size([25])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y, sample_len, iscale, oscale, momx, momy, momz = dataiter.next()\n",
    "\n",
    "print(sample_x.shape, sample_y.shape, sample_len.shape, iscale.shape, momz.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building\n",
    "It is time to build the model for this approach. It will consist on a single/double layer LSTM followed by a Linear layer with output size the number of keypoints we want to estimate. I also define a method to initialize the hidden_state of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_2D3D(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, bidirectional, dropout=0.):\n",
    "        super().__init__()\n",
    "        # Save the model parameters\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bi = bidirectional\n",
    "        \n",
    "        # Define the architecture\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*(2 if self.bi else 1), output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, state, lengths):\n",
    "        # Describe the forward step\n",
    "        batch_size, seq_len = x.size(0), x.size(1) # We save the batch size and the (maximum) sequence length\n",
    "        \n",
    "        # Need to pack a tensor containing padded sequences of variable length\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths=lengths, batch_first=True, enforce_sorted=False)\n",
    "        ht, hidden_state = self.lstm(packed, state) # ht will be a PackedSequence\n",
    "\n",
    "        # Need to flatten and reshape the output to feed it to the Linear layer\n",
    "        ht = ht.data.contiguous() # ht will be of shape [sum(lengths), hidden_dim]\n",
    "        ot = self.fc(ht) # ot will be of shape [sum(lengths), ouput_size]\n",
    "\n",
    "        l_ot = [ot[:int(length)] for length in lengths] # list of batch elements, each shape [lengths[i], output_size]\n",
    "        packed_ot = nn.utils.rnn.pack_sequence(l_ot, enforce_sorted=False) # PackedSequence\n",
    "        # Finally return to shape [batch_size, seq_len, output_size]\n",
    "        ot, _ = nn.utils.rnn.pad_packed_sequence(packed_ot, batch_first=True, total_length=seq_len)\n",
    "        \n",
    "        return ot, hidden_state\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers*(2 if self.bi else 1), batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers*(2 if self.bi else 1), batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_2D3D(\n",
      "  (lstm): LSTM(84, 512, num_layers=2, batch_first=True)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=42, bias=True)\n",
      "  )\n",
      ")\n",
      "3347498\n"
     ]
    }
   ],
   "source": [
    "# Define some model parameters\n",
    "INPUT_SIZE = sample_x.size(2)\n",
    "OUTPUT_SIZE = sample_y.size(2)\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = False\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTM_2D3D(INPUT_SIZE, OUTPUT_SIZE, HIDDEN_DIM, N_LAYERS, BIDIRECTIONAL, dropout=0.)\n",
    "model.to(device)\n",
    "print(model)\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now we will proceed with the training. The first cell will define the learning rate, the loss function and the selected optimizer for the training process. Then we will proceed with a training over a number of epochs in which we will print it's training loss and validation loss. I also will be using Tensorboard to have a much nicer view of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substract_root_PJPE(output):\n",
    "    jhl, jhr = torch.chunk(output[:, :, :21], max_seq, dim=1), torch.chunk(output[:, :, 21:], max_seq, dim=1)\n",
    "    joints_merged = []\n",
    "    roots = [0, 0]\n",
    "    for i, joints in enumerate((jhl, jhr)):\n",
    "        n_joints = []\n",
    "        for chunk in joints:\n",
    "            n_joints.append(chunk.sub(chunk[:,:,roots[i]].unsqueeze(2)))\n",
    "        joints_merged.append(torch.cat(tuple(n_joints), dim=1))\n",
    "    joints_merged = torch.cat(tuple(joints_merged), dim=2)\n",
    "    return joints_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpjpe(rooted_o, rooted_l, seq_lens):\n",
    "    MPJPE = []\n",
    "    for i in range(len(seq_lens)):\n",
    "        MPJPE.append(rooted_o[i,:int(seq_lens[i])].sub(rooted_l[i,:int(seq_lens[i])]).abs().mean().item())\n",
    "    \n",
    "    return np.mean(MPJPE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_keypoints(output, labels, seq_lens, alpha=0.1):\n",
    "    total_correct = 0\n",
    "    for i in range(len(seq_lens)):\n",
    "        for j in range(int(seq_lens[i])):\n",
    "            rang = (labels[i,j].max()-labels[i,j].min())\n",
    "            for o, l in zip(output[i,j], labels[i,j]):\n",
    "                if o < l+alpha*rang and o > l-alpha*rang:\n",
    "                    total_correct += 1\n",
    "    return total_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "lr = 5e-6\n",
    "loss_function = nn.MSELoss()\n",
    "one_cycle = True\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "if one_cycle:\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, \n",
    "                                                  steps_per_epoch=len(train_loader), epochs=NUM_EPOCHS,\n",
    "                                                  div_factor=25.0, final_div_factor=10000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "name = 'hands_whole'\n",
    "writer = SummaryWriter(log_dir=f'/deeplearning/logs/{name}{datetime.now()}_lr-{lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 1/50 in 24.74s.\n",
      " Loss: 0.7272  Val Loss: 0.7235\n",
      " Train MPJPE: 0.6691  Val MPJPE: 0.6686\n",
      " Train PCK: 17.763  Val PCK: 19.908\n",
      "Finished epoch 2/50 in 24.52s.\n",
      " Loss: 0.7288  Val Loss: 0.7336\n",
      " Train MPJPE: 0.6689  Val MPJPE: 0.6654\n",
      " Train PCK: 17.685  Val PCK: 19.565\n",
      "Finished epoch 3/50 in 24.56s.\n",
      " Loss: 0.7209  Val Loss: 0.7383\n",
      " Train MPJPE: 0.6708  Val MPJPE: 0.6659\n",
      " Train PCK: 17.882  Val PCK: 19.583\n",
      "Finished epoch 4/50 in 24.49s.\n",
      " Loss: 0.7236  Val Loss: 0.7370\n",
      " Train MPJPE: 0.6686  Val MPJPE: 0.6644\n",
      " Train PCK: 17.866  Val PCK: 19.411\n",
      "Finished epoch 5/50 in 24.50s.\n",
      " Loss: 0.7160  Val Loss: 0.7332\n",
      " Train MPJPE: 0.6709  Val MPJPE: 0.6645\n",
      " Train PCK: 18.013  Val PCK: 19.446\n",
      "Finished epoch 6/50 in 24.55s.\n",
      " Loss: 0.7158  Val Loss: 0.7362\n",
      " Train MPJPE: 0.6608  Val MPJPE: 0.6539\n",
      " Train PCK: 17.807  Val PCK: 19.420\n",
      "Finished epoch 7/50 in 24.51s.\n",
      " Loss: 0.7091  Val Loss: 0.7300\n",
      " Train MPJPE: 0.6582  Val MPJPE: 0.6466\n",
      " Train PCK: 17.964  Val PCK: 19.265\n",
      "Finished epoch 8/50 in 24.50s.\n",
      " Loss: 0.7082  Val Loss: 0.7342\n",
      " Train MPJPE: 0.6582  Val MPJPE: 0.6584\n",
      " Train PCK: 17.954  Val PCK: 19.708\n",
      "Finished epoch 9/50 in 24.55s.\n",
      " Loss: 0.6966  Val Loss: 0.7191\n",
      " Train MPJPE: 0.6490  Val MPJPE: 0.6447\n",
      " Train PCK: 18.061  Val PCK: 19.744\n",
      "Finished epoch 10/50 in 24.56s.\n",
      " Loss: 0.6838  Val Loss: 0.7181\n",
      " Train MPJPE: 0.6447  Val MPJPE: 0.6350\n",
      " Train PCK: 18.427  Val PCK: 19.589\n",
      "Finished epoch 11/50 in 24.84s.\n",
      " Loss: 0.6748  Val Loss: 0.7116\n",
      " Train MPJPE: 0.6349  Val MPJPE: 0.6401\n",
      " Train PCK: 18.446  Val PCK: 19.786\n",
      "Finished epoch 12/50 in 24.83s.\n",
      " Loss: 0.6650  Val Loss: 0.7013\n",
      " Train MPJPE: 0.6234  Val MPJPE: 0.6344\n",
      " Train PCK: 18.446  Val PCK: 19.765\n",
      "Finished epoch 13/50 in 24.91s.\n",
      " Loss: 0.6503  Val Loss: 0.7041\n",
      " Train MPJPE: 0.6152  Val MPJPE: 0.6201\n",
      " Train PCK: 18.744  Val PCK: 19.557\n",
      "Finished epoch 14/50 in 24.96s.\n",
      " Loss: 0.6376  Val Loss: 0.6904\n",
      " Train MPJPE: 0.5990  Val MPJPE: 0.6213\n",
      " Train PCK: 18.831  Val PCK: 20.182\n",
      "Finished epoch 15/50 in 25.07s.\n",
      " Loss: 0.6194  Val Loss: 0.6882\n",
      " Train MPJPE: 0.5869  Val MPJPE: 0.6164\n",
      " Train PCK: 19.185  Val PCK: 20.411\n",
      "Finished epoch 16/50 in 25.09s.\n",
      " Loss: 0.6039  Val Loss: 0.6781\n",
      " Train MPJPE: 0.5713  Val MPJPE: 0.5924\n",
      " Train PCK: 19.378  Val PCK: 20.033\n",
      "Finished epoch 17/50 in 25.44s.\n",
      " Loss: 0.5844  Val Loss: 0.6753\n",
      " Train MPJPE: 0.5527  Val MPJPE: 0.6043\n",
      " Train PCK: 19.795  Val PCK: 20.095\n",
      "Finished epoch 18/50 in 25.47s.\n",
      " Loss: 0.5682  Val Loss: 0.6730\n",
      " Train MPJPE: 0.5389  Val MPJPE: 0.5660\n",
      " Train PCK: 20.327  Val PCK: 20.289\n",
      "Finished epoch 19/50 in 26.17s.\n",
      " Loss: 0.5570  Val Loss: 0.6738\n",
      " Train MPJPE: 0.5234  Val MPJPE: 0.5750\n",
      " Train PCK: 20.660  Val PCK: 20.527\n",
      "Finished epoch 20/50 in 25.65s.\n",
      " Loss: 0.5479  Val Loss: 0.6837\n",
      " Train MPJPE: 0.5103  Val MPJPE: 0.5618\n",
      " Train PCK: 20.702  Val PCK: 20.324\n",
      "Finished epoch 21/50 in 25.88s.\n",
      " Loss: 0.5469  Val Loss: 0.6933\n",
      " Train MPJPE: 0.5054  Val MPJPE: 0.5559\n",
      " Train PCK: 21.173  Val PCK: 20.170\n",
      "Finished epoch 22/50 in 25.94s.\n",
      " Loss: 0.5467  Val Loss: 0.6649\n",
      " Train MPJPE: 0.5010  Val MPJPE: 0.5277\n",
      " Train PCK: 21.390  Val PCK: 20.307\n",
      "Finished epoch 23/50 in 26.08s.\n",
      " Loss: 0.5342  Val Loss: 0.6506\n",
      " Train MPJPE: 0.4909  Val MPJPE: 0.5630\n",
      " Train PCK: 21.916  Val PCK: 20.854\n",
      "Finished epoch 24/50 in 26.08s.\n",
      " Loss: 0.5357  Val Loss: 0.6653\n",
      " Train MPJPE: 0.4975  Val MPJPE: 0.5587\n",
      " Train PCK: 21.807  Val PCK: 20.235\n",
      "Finished epoch 25/50 in 26.13s.\n",
      " Loss: 0.5309  Val Loss: 0.6323\n",
      " Train MPJPE: 0.4968  Val MPJPE: 0.5184\n",
      " Train PCK: 21.968  Val PCK: 21.467\n",
      "Finished epoch 26/50 in 26.14s.\n",
      " Loss: 0.5360  Val Loss: 0.6551\n",
      " Train MPJPE: 0.4908  Val MPJPE: 0.5718\n",
      " Train PCK: 21.851  Val PCK: 21.676\n",
      "Finished epoch 27/50 in 26.13s.\n",
      " Loss: 0.5320  Val Loss: 0.6792\n",
      " Train MPJPE: 0.4957  Val MPJPE: 0.5475\n",
      " Train PCK: 22.188  Val PCK: 20.372\n",
      "Finished epoch 28/50 in 26.17s.\n",
      " Loss: 0.5362  Val Loss: 0.6567\n",
      " Train MPJPE: 0.4889  Val MPJPE: 0.5628\n",
      " Train PCK: 21.867  Val PCK: 21.842\n",
      "Finished epoch 29/50 in 26.24s.\n",
      " Loss: 0.5353  Val Loss: 0.6586\n",
      " Train MPJPE: 0.4962  Val MPJPE: 0.5568\n",
      " Train PCK: 22.338  Val PCK: 20.452\n",
      "Finished epoch 30/50 in 26.47s.\n",
      " Loss: 0.5357  Val Loss: 0.6829\n",
      " Train MPJPE: 0.4893  Val MPJPE: 0.5462\n",
      " Train PCK: 21.914  Val PCK: 20.253\n",
      "Finished epoch 31/50 in 26.22s.\n",
      " Loss: 0.5340  Val Loss: 0.6638\n",
      " Train MPJPE: 0.4899  Val MPJPE: 0.5673\n",
      " Train PCK: 21.908  Val PCK: 20.414\n",
      "Finished epoch 32/50 in 26.29s.\n",
      " Loss: 0.5351  Val Loss: 0.6663\n",
      " Train MPJPE: 0.4894  Val MPJPE: 0.5539\n",
      " Train PCK: 21.984  Val PCK: 21.036\n",
      "Finished epoch 33/50 in 26.26s.\n",
      " Loss: 0.5348  Val Loss: 0.6644\n",
      " Train MPJPE: 0.4927  Val MPJPE: 0.5414\n",
      " Train PCK: 21.968  Val PCK: 20.363\n",
      "Finished epoch 34/50 in 26.30s.\n",
      " Loss: 0.5367  Val Loss: 0.6510\n",
      " Train MPJPE: 0.4939  Val MPJPE: 0.5528\n",
      " Train PCK: 22.074  Val PCK: 21.726\n",
      "Finished epoch 35/50 in 26.35s.\n",
      " Loss: 0.5308  Val Loss: 0.6611\n",
      " Train MPJPE: 0.4834  Val MPJPE: 0.5574\n",
      " Train PCK: 22.208  Val PCK: 20.598\n",
      "Finished epoch 36/50 in 26.73s.\n",
      " Loss: 0.5390  Val Loss: 0.6818\n",
      " Train MPJPE: 0.4922  Val MPJPE: 0.5666\n",
      " Train PCK: 21.977  Val PCK: 20.143\n",
      "Finished epoch 37/50 in 26.12s.\n",
      " Loss: 0.5382  Val Loss: 0.6860\n",
      " Train MPJPE: 0.4863  Val MPJPE: 0.5697\n",
      " Train PCK: 21.791  Val PCK: 20.449\n",
      "Finished epoch 38/50 in 26.14s.\n",
      " Loss: 0.5355  Val Loss: 0.6693\n",
      " Train MPJPE: 0.4927  Val MPJPE: 0.5402\n",
      " Train PCK: 22.111  Val PCK: 20.655\n",
      "Finished epoch 39/50 in 26.05s.\n",
      " Loss: 0.5332  Val Loss: 0.6786\n",
      " Train MPJPE: 0.4928  Val MPJPE: 0.5649\n",
      " Train PCK: 22.233  Val PCK: 21.310\n",
      "Finished epoch 40/50 in 26.09s.\n",
      " Loss: 0.5299  Val Loss: 0.6464\n",
      " Train MPJPE: 0.4883  Val MPJPE: 0.5312\n",
      " Train PCK: 22.281  Val PCK: 21.384\n",
      "Finished epoch 41/50 in 26.09s.\n",
      " Loss: 0.5257  Val Loss: 0.6541\n",
      " Train MPJPE: 0.4835  Val MPJPE: 0.5466\n",
      " Train PCK: 22.332  Val PCK: 20.366\n",
      "Finished epoch 42/50 in 26.13s.\n",
      " Loss: 0.5314  Val Loss: 0.6895\n",
      " Train MPJPE: 0.4839  Val MPJPE: 0.5466\n",
      " Train PCK: 22.057  Val PCK: 20.399\n",
      "Finished epoch 43/50 in 26.19s.\n",
      " Loss: 0.5276  Val Loss: 0.6748\n",
      " Train MPJPE: 0.4904  Val MPJPE: 0.5401\n",
      " Train PCK: 22.194  Val PCK: 20.140\n",
      "Finished epoch 44/50 in 26.17s.\n",
      " Loss: 0.5335  Val Loss: 0.6526\n",
      " Train MPJPE: 0.4917  Val MPJPE: 0.5356\n",
      " Train PCK: 22.107  Val PCK: 20.497\n",
      "Finished epoch 45/50 in 26.24s.\n",
      " Loss: 0.5357  Val Loss: 0.6513\n",
      " Train MPJPE: 0.4892  Val MPJPE: 0.5523\n",
      " Train PCK: 21.900  Val PCK: 20.917\n",
      "Finished epoch 46/50 in 26.36s.\n",
      " Loss: 0.5351  Val Loss: 0.6543\n",
      " Train MPJPE: 0.4859  Val MPJPE: 0.5403\n",
      " Train PCK: 22.207  Val PCK: 21.149\n",
      "Finished epoch 47/50 in 26.36s.\n",
      " Loss: 0.5401  Val Loss: 0.6458\n",
      " Train MPJPE: 0.4912  Val MPJPE: 0.5903\n",
      " Train PCK: 21.901  Val PCK: 20.812\n",
      "Finished epoch 48/50 in 26.19s.\n",
      " Loss: 0.5384  Val Loss: 0.6485\n",
      " Train MPJPE: 0.4873  Val MPJPE: 0.5447\n",
      " Train PCK: 22.031  Val PCK: 21.128\n",
      "Finished epoch 49/50 in 26.15s.\n",
      " Loss: 0.5320  Val Loss: 0.6584\n",
      " Train MPJPE: 0.4884  Val MPJPE: 0.5435\n",
      " Train PCK: 21.991  Val PCK: 21.690\n",
      "Finished epoch 50/50 in 26.25s.\n",
      " Loss: 0.5352  Val Loss: 0.6747\n",
      " Train MPJPE: 0.4896  Val MPJPE: 0.5325\n",
      " Train PCK: 22.097  Val PCK: 21.188\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7849ded4d5014669b7fdeb3745162dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f13cc3d0090>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer_beg = timer()\n",
    "\n",
    "tr_losses = []\n",
    "val_losses = []\n",
    "\n",
    "model.train()\n",
    "for i in range(NUM_EPOCHS):\n",
    "    # Init the hidden state (ht, ct)\n",
    "    h = model.init_hidden(batch_size)\n",
    "    batch_losses = []\n",
    "    train_MPJPE = []\n",
    "    train_CK = [0, 0]\n",
    "    \n",
    "    if i+1 == NUM_EPOCHS:\n",
    "        preds, inps, labls, lens = [], [], [], []\n",
    "        val_preds, val_inps, val_labls, val_lens = [], [], [], []\n",
    "        iscale, oscale, val_iscale, val_oscale = [], [], [], []\n",
    "        momx, momy, momz = [], [], []\n",
    "        val_momx, val_momy, val_momz = [], [], []\n",
    "        \n",
    "    for inputs, labels, lengths, i_s, o_s, mx, my, mz in train_loader:\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels, lengths = inputs.to(device), labels.to(device), lengths.to(device)\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward step\n",
    "        output, h = model(inputs, h, lengths)\n",
    "        \n",
    "        if i+1 == NUM_EPOCHS:\n",
    "            e = [preds, inps, labls, lens, iscale, oscale, momx, momy, momz]\n",
    "            b = [output, inputs, labels, lengths, i_s, o_s, mx, my, mz]\n",
    "            for k in range(len(e)):\n",
    "                e[k].append(b[k])            \n",
    "\n",
    "        # Loss calculation and backward step\n",
    "        loss = loss_function(nn.utils.rnn.pack_padded_sequence(output, lengths=lengths, batch_first=True, \n",
    "                                                               enforce_sorted=False).data,\n",
    "                             nn.utils.rnn.pack_padded_sequence(labels, lengths=lengths, batch_first=True,\n",
    "                                                               enforce_sorted=False).data)\n",
    "        loss.backward()\n",
    "        # Weight update\n",
    "        optimizer.step()\n",
    "        # One cycle policy step\n",
    "        if one_cycle:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Output data collection for showing\n",
    "        batch_losses.append(loss.item())\n",
    "        rooted_o, rooted_l = substract_root_PJPE(output), substract_root_PJPE(labels)\n",
    "        train_MPJPE.append(mpjpe(rooted_o, rooted_l, lengths))\n",
    "        \n",
    "        train_CK[0] += correct_keypoints(output, labels, lengths)\n",
    "        train_CK[1] += lengths.sum()*labels.shape[2]\n",
    "    \n",
    "    timer_end = timer()\n",
    "    tr_losses.append(np.mean(batch_losses))\n",
    "    writer.add_scalar('Loss/train', tr_losses[-1], i)   \n",
    "    train_MPJPE_total = np.mean(train_MPJPE)\n",
    "    train_PCK = train_CK[0]/train_CK[1]\n",
    "    \n",
    "    # Validation at the end of an epoch\n",
    "    val_h = model.init_hidden(batch_size)\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    val_MPJPE = []\n",
    "    val_CK = [0,0]\n",
    "    \n",
    "    for inp, lab, lns, vis, vos, vmx, vmy, vmz in val_loader:\n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "        inp, lab, lns = inp.to(device), lab.to(device), lns.to(device)\n",
    "        out, val_h = model(inp, val_h, lns)\n",
    "        \n",
    "        if i+1 == NUM_EPOCHS:\n",
    "            e = [val_preds, val_inps, val_labls, val_lens, val_iscale, val_oscale, val_momx, val_momy, val_momz]\n",
    "            b = [out, inp, lab, lns, vis, vos, vmx, vmy, vmz]\n",
    "            for k in range(len(e)):\n",
    "                e[k].append(b[k])  \n",
    "        \n",
    "        loss = loss_function(nn.utils.rnn.pack_padded_sequence(out, lengths=lns, batch_first=True,\n",
    "                                                               enforce_sorted=False).data,\n",
    "                             nn.utils.rnn.pack_padded_sequence(lab, lengths=lns, batch_first=True,\n",
    "                                                               enforce_sorted=False).data)\n",
    "        val_loss.append(loss.item())\n",
    "        rooted_o, rooted_l = substract_root_PJPE(out), substract_root_PJPE(lab)\n",
    "        val_MPJPE.append(mpjpe(rooted_o, rooted_l, lns))\n",
    "        val_CK[0] += correct_keypoints(out, lab, lns)\n",
    "        val_CK[1] += lns.sum()*lab.shape[2]\n",
    "        \n",
    "    val_losses.append(np.mean(val_loss))\n",
    "    writer.add_scalar('Loss/validation', val_losses[-1], i)  \n",
    "    val_MPJPE_total = np.mean(val_MPJPE)\n",
    "    val_PCK = val_CK[0]/val_CK[1] \n",
    "    model.train()\n",
    "    \n",
    "    # Output loss and training time.\n",
    "    print(f\"Finished epoch {i+1}/{NUM_EPOCHS} in {(timer_end-timer_beg):.2f}s.\\n\",\n",
    "             f\"Loss: {np.mean(tr_losses[-1]):.4f}\", f\" Val Loss: {val_losses[-1]:.4f}\\n\",\n",
    "             f\"Train MPJPE: {train_MPJPE_total:.4f}\", f\" Val MPJPE: {val_MPJPE_total:.4f}\\n\",\n",
    "             f\"Train PCK: {train_PCK*100:.3f}\", f\" Val PCK: {val_PCK*100:.3f}\")\n",
    "    timer_beg = timer()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(tr_losses, label='train')\n",
    "plt.plot(val_losses, label='validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSELoss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the predictions for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 2]) torch.Size([400, 16, 84])\n"
     ]
    }
   ],
   "source": [
    "tr_predictions = torch.cat(tuple(preds), dim=0)\n",
    "tr_inputs = torch.cat(tuple(inps), dim=0)\n",
    "tr_groundtruth = torch.cat(tuple(labls), dim=0)\n",
    "tr_lengths = torch.cat(tuple(lens), dim=0)\n",
    "tr_inp_scale, tr_out_scale = torch.cat(tuple(iscale), dim=0), torch.cat(tuple(oscale), dim=0)\n",
    "tr_mx, tr_my, tr_mz = torch.cat(tuple(momx), dim=0), torch.cat(tuple(momy), dim=0), torch.cat(tuple(momz), dim=0)\n",
    "print(tr_inp_scale.shape, tr_inputs.shape)\n",
    "\n",
    "val_predictions = torch.cat(tuple(val_preds), dim=0)\n",
    "val_inputs = torch.cat(tuple(val_inps), dim=0)\n",
    "val_groundtruth = torch.cat(tuple(val_labls), dim=0)\n",
    "val_length = torch.cat(tuple(val_lens), dim=0)\n",
    "val_inp_scale, val_out_scale = torch.cat(tuple(val_iscale), dim=0), torch.cat(tuple(val_oscale), dim=0)\n",
    "val_mx, val_my, val_mz = torch.cat(tuple(val_momx), dim=0), torch.cat(tuple(val_momy), dim=0), torch.cat(tuple(val_momz), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = torch.zeros((138), device='cuda:0')\n",
    "\n",
    "for i in range(len(tr_predictions)):\n",
    "    count=0\n",
    "    for frame in tr_predictions[i, :int(tr_lengths[i])]:\n",
    "        if torch.all(frame.eq(zeros)):\n",
    "            count +=1\n",
    "    print(f'{count}/{int(tr_lengths[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = torch.zeros((138), device='cuda:0')\n",
    "\n",
    "for i in range(len(val_predictions)):\n",
    "    count=0\n",
    "    for frame in tr_predictions[i, :int(val_length[i])]:\n",
    "        if torch.all(frame.eq(zeros)):\n",
    "            count +=1\n",
    "    print(f'{count}/{int(val_length[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'./{name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'./{name}.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "After the training, we shall proceed with the performance test. This will go through the test batches and perform the inference, then it will show the test loss, as well as the performance metric. In this case, as we are working with human body keypoints, we will use the Mean Per Joint Position Error (MPJPE) metric, which outputs the mean euclidean distance between the joints (keypoints) positions estimated and the ones in the groundtruth.\n",
    "\n",
    "The formula for MPJPE is the following:\n",
    "\n",
    "$\\text{MPJPE} = \\frac1T\\frac1N\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{i=1}^{N}\\|(J_{i}^{(t)}-J_{root}^{(t)})-(Ĵ_{i}^{(t)}-Ĵ_{root}^{(t)})\\|$\n",
    "\n",
    "Where N is the number of joints, and T the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the formula above, we need to align the root joints of the labels and the network output. In order to do that, I have defined a function (`substract_root_PJPE`) that substracts the root joint of each keypoint set (face, hands, body) in the corresponding keypoint set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "MPJPE = []\n",
    "h = model.init_hidden(batch_size)\n",
    "preds, inps, labls, lengs = [], [], [], []\n",
    "iscal, oscal = [], []\n",
    "mx, my, mz = [], [], []\n",
    "test_CK = [0,0]\n",
    "\n",
    "model.eval()\n",
    "for inputs_test, labels_test, lengths_test, is_test, os_test, mx_test, my_test, mz_test in test_loader:\n",
    "    \n",
    "    h = tuple([each.data for each in h])\n",
    "    inputs_test, labels_test, lengths_test = inputs_test.to(device), labels_test.to(device), lengths_test.to(device)\n",
    "    \n",
    "    output_test, h = model(inputs_test, h, lengths_test)\n",
    "    \n",
    "    e = [preds, inps, labls, lengs, iscal, oscal, mx, my, mz]\n",
    "    b = [output_test, inputs_test, labels_test, lengths_test, is_test, os_test, mx_test, my_test, mz_test]\n",
    "    for k in range(len(e)):\n",
    "        e[k].append(b[k])      \n",
    "    \n",
    "    test_loss = loss_function(nn.utils.rnn.pack_padded_sequence(output_test, lengths=lengths_test, batch_first=True,\n",
    "                                                               enforce_sorted=False).data,\n",
    "                             nn.utils.rnn.pack_padded_sequence(labels_test, lengths=lengths_test, batch_first=True,\n",
    "                                                               enforce_sorted=False).data)\n",
    "    test_losses.append(test_loss.item())\n",
    "    rooted_o, rooted_l = substract_root_PJPE(output_test), substract_root_PJPE(labels_test)\n",
    "    MPJPE.append(mpjpe(rooted_o, rooted_l, lengths_test))\n",
    "    test_CK[0] += correct_keypoints(output_test, labels_test, lengths_test)\n",
    "    test_CK[1] += lengths_test.sum()*labels_test.shape[2]\n",
    "\n",
    "test_predictions = torch.cat(tuple(preds), dim=0)\n",
    "test_inputs = torch.cat(tuple(inps), dim=0)\n",
    "test_groundtruth = torch.cat(tuple(labls), dim=0)\n",
    "test_inp_scale, test_out_scale = torch.cat(tuple(iscal), dim=0), torch.cat(tuple(oscal), dim=0)\n",
    "test_mx, test_my, test_mz = torch.cat(tuple(mx), dim=0), torch.cat(tuple(my), dim=0), torch.cat(tuple(mz), dim=0)\n",
    "MPJPE_total = np.mean(MPJPE)\n",
    "test_PCK = test_CK[0]/test_CK[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.0700 \n",
      "MPJPE: 0.6878 \n",
      "Test PCK: 20.357\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test loss: {np.mean(test_losses):.4f}\", f\"\\nMPJPE: {MPJPE_total:.4f}\",  f\"\\nTest PCK: {test_PCK*100:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results into a json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'train':{'inputs':tr_inputs.tolist(), 'predictions':tr_predictions.tolist(), 'labels':tr_groundtruth.tolist(), 'lengths':tr_lengths.tolist()},\n",
    "          'validation':{'inputs':val_inputs.tolist(), 'predictions':val_predictions.tolist(), 'labels':val_groundtruth.tolist(), 'lengths':val_length.tolist()},\n",
    "          'test':{'inputs':test_inputs.tolist(), 'predictions':test_predictions.tolist(), 'labels':test_groundtruth.tolist(), 'lengths':test_lengths.tolist()}}\n",
    "with open('../../results/small_hands.json', 'w') as fp:\n",
    "    json.dump(results, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load results from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../results/small_hands.json', 'r') as j:\n",
    "    jd = json.load(j)\n",
    "    tr, val, test = jd['train'], jd['validation'], jd['test']\n",
    "    tr_inputs, tr_predictions, tr_groundtruth, tr_lengths = tuple(torch.tensor(tr[n]) for n in ['inputs', 'predictions',\n",
    "                                                                                              'labels', 'lengths'])\n",
    "    val_inputs, val_predictions, val_groundtruth, val_length = tuple(torch.tensor(val[n]) for n in ['inputs', 'predictions',\n",
    "                                                                                              'labels', 'lengths'])\n",
    "    test_inputs, test_predictions, test_groundtruth, test_lengths = tuple(torch.tensor(test[n]) for n in ['inputs', 'predictions',\n",
    "                                                                                              'labels', 'lengths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 8752, 84]), torch.Size([8, 8752, 42]), torch.Size([8]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_inputs.shape, val_predictions.shape, test_lengths.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to better understanding of the results, I will plot some of the frames from the last batches on the training and validation, and also from testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_rotate(c_inputs, c_z, frames, frame):\n",
    "    c_inputs[:,::2].mul_(mom_x[1])\n",
    "    c_inputs[:,1::2].mul_(mom_y[1])\n",
    "    c_z.mul_(stdz)\n",
    "\n",
    "    handsXY = torch.chunk(c_inputs[frames, :], len(frames), dim=0)\n",
    "    handsZ = torch.chunk(c_z[frames, :], len(frames), dim=0)\n",
    "    \n",
    "    x = handsXY[frame].squeeze()[::2]\n",
    "    y = handsXY[frame].squeeze()[1::2]\n",
    "    z = handsZ[frame].squeeze()\n",
    "\n",
    "    rh0 = [[c[i] for i in [0, 1, 2, 3, 4]] for c in [x,y,z]]\n",
    "    rh1 = [[c[i] for i in [0, 5, 6, 7, 8]] for c in [x,y,z]]\n",
    "    rh2 = [[c[i] for i in [0, 9, 10, 11, 12]] for c in [x,y,z]]\n",
    "    rh3 = [[c[i] for i in [0, 13, 14, 15, 16]] for c in [x,y,z]]\n",
    "    rh4 = [[c[i] for i in [0, 17, 18, 19, 20]] for c in [x,y,z]]\n",
    "    lh0 = [[c[i+21] for i in [0, 1, 2, 3, 4]] for c in [x,y,z]]\n",
    "    lh1 = [[c[i+21] for i in [0, 5, 6, 7, 8]] for c in [x,y,z]]\n",
    "    lh2 = [[c[i+21] for i in [0, 9, 10, 11, 12]] for c in [x,y,z]]\n",
    "    lh3 = [[c[i+21] for i in [0, 13, 14, 15, 16]] for c in [x,y,z]]\n",
    "    lh4 = [[c[i+21] for i in [0, 17, 18, 19, 20]] for c in [x,y,z]]\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    def init():\n",
    "        ax.plot(rh0[0], rh0[2], rh0[1])\n",
    "        ax.plot(rh1[0], rh1[2], rh1[1])\n",
    "        ax.plot(rh2[0], rh2[2], rh2[1])\n",
    "        ax.plot(rh3[0], rh3[2], rh3[1])\n",
    "        ax.plot(rh4[0], rh4[2], rh4[1])\n",
    "        \n",
    "        ax.plot(lh0[0], lh0[2], lh0[1])\n",
    "        ax.plot(lh1[0], lh1[2], lh1[1])\n",
    "        ax.plot(lh2[0], lh2[2], lh2[1])\n",
    "        ax.plot(lh3[0], lh3[2], lh3[1])\n",
    "        ax.plot(lh4[0], lh4[2], lh4[1])\n",
    "        \n",
    "        lims = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()\n",
    "        spans = lims[0][1]-lims[0][0], lims[1][1]-lims[1][0], lims[2][1]-lims[2][0]\n",
    "        span = max(spans)\n",
    "        margins = [(span-s)/2 for  s in spans]\n",
    "        ax.set_xlim(lims[0][0]-margins[0], lims[0][1]+margins[0])\n",
    "        ax.set_ylim(lims[1][0]-margins[1], lims[1][1]+margins[1])\n",
    "        ax.set_zlim(lims[2][0]-margins[2], lims[2][1]+margins[2])\n",
    "\n",
    "        return fig,\n",
    "\n",
    "    def animate(i):\n",
    "        ax.view_init(elev=220., azim=3.6*i)\n",
    "        return fig,\n",
    "\n",
    "    # Animate\n",
    "    ani = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=100, interval=100, blit=True)    \n",
    "\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frames(predictions, groundtruth, inputs, video_n, frames, rot, train=False):\n",
    "    \n",
    "    inp = inputs.clone()\n",
    "    preds = predictions.clone()\n",
    "    grtr = groundtruth.clone()\n",
    "    \n",
    "    handsXY = torch.chunk(inp[video_n, frames, :], len(frames), dim=0)\n",
    "    pred_handsZ = torch.chunk(preds[video_n, frames, :], len(frames), dim=0)\n",
    "    true_handsZ = torch.chunk(grtr[video_n, frames, :], len(frames), dim=0)\n",
    "   \n",
    "    nrows = np.ceil(len(frames)/2)\n",
    "    fig = plt.figure(figsize=(15, 6*nrows))\n",
    "    fig2 = plt.figure(figsize=(15, 6*nrows))\n",
    "    for frame in range(len(frames)):\n",
    "        x = handsXY[frame].squeeze()[::2].tolist()\n",
    "        y = handsXY[frame].squeeze()[1::2].tolist()\n",
    "        pred_z = pred_handsZ[frame].squeeze().tolist()\n",
    "        true_z = true_handsZ[frame].squeeze().tolist()\n",
    "        \n",
    "        r = R.from_euler('y', rot, degrees=True)\n",
    "        \n",
    "        xyz1, xyz2 = np.asarray([c for c in zip(x, y, pred_z)]), np.asarray([c for c in zip(x, y, true_z)])\n",
    "        xyz1, xyz2 = r.apply(xyz1), r.apply(xyz2)\n",
    "        x1, x2 = xyz1[:,0], xyz2[:,0]\n",
    "        y1, y2 = xyz1[:,1], xyz2[:,1]\n",
    "        pred_z, true_z = xyz1[:,2], xyz2[:,2]\n",
    "        if not train:\n",
    "            print((x2.max()-x2.min())/(x1.max()-x1.min()))\n",
    "            x1 = x1*((x2.max()-x2.min())/(x1.max()-x1.min()))\n",
    "        \n",
    "        rh0 = tuple([[c[i] for i in [0, 1, 2, 3, 4]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        rh1 = tuple([[c[i] for i in [0, 5, 6, 7, 8]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        rh2 = tuple([[c[i] for i in [0, 9, 10, 11, 12]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        rh3 = tuple([[c[i] for i in [0, 13, 14, 15, 16]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        rh4 = tuple([[c[i] for i in [0, 17, 18, 19, 20]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        lh0 = tuple([[c[i+21] for i in [0, 1, 2, 3, 4]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        lh1 = tuple([[c[i+21] for i in [0, 5, 6, 7, 8]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        lh2 = tuple([[c[i+21] for i in [0, 9, 10, 11, 12]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        lh3 = tuple([[c[i+21] for i in [0, 13, 14, 15, 16]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        lh4 = tuple([[c[i+21] for i in [0, 17, 18, 19, 20]] for c in l] for l in [[x1, y1, pred_z], [x2, y2, true_z]])\n",
    "        \n",
    "        ax = fig.add_subplot(nrows, 2, frame+1, projection='3d')\n",
    "\n",
    "        ax.plot(rh0[0][0], rh0[0][2], rh0[0][1])\n",
    "        ax.plot(rh1[0][0], rh1[0][2], rh1[0][1])\n",
    "        ax.plot(rh2[0][0], rh2[0][2], rh2[0][1])\n",
    "        ax.plot(rh3[0][0], rh3[0][2], rh3[0][1])\n",
    "        ax.plot(rh4[0][0], rh4[0][2], rh4[0][1])\n",
    "        \n",
    "        ax.plot(lh0[0][0], lh0[0][2], lh0[0][1])\n",
    "        ax.plot(lh1[0][0], lh1[0][2], lh1[0][1])\n",
    "        ax.plot(lh2[0][0], lh2[0][2], lh2[0][1])\n",
    "        ax.plot(lh3[0][0], lh3[0][2], lh3[0][1])\n",
    "        ax.plot(lh4[0][0], lh4[0][2], lh4[0][1])\n",
    "        \n",
    "        ax2 = fig2.add_subplot(nrows, 2, frame+1, projection='3d')\n",
    "        ax2.plot(rh0[1][0], rh0[1][2], rh0[1][1])\n",
    "        ax2.plot(rh1[1][0], rh1[1][2], rh1[1][1])\n",
    "        ax2.plot(rh2[1][0], rh2[1][2], rh2[1][1])\n",
    "        ax2.plot(rh3[1][0], rh3[1][2], rh3[1][1])\n",
    "        ax2.plot(rh4[1][0], rh4[1][2], rh4[1][1])\n",
    "        \n",
    "        ax2.plot(lh0[1][0], lh0[1][2], lh0[1][1])\n",
    "        ax2.plot(lh1[1][0], lh1[1][2], lh1[1][1])\n",
    "        ax2.plot(lh2[1][0], lh2[1][2], lh2[1][1])\n",
    "        ax2.plot(lh3[1][0], lh3[1][2], lh3[1][1])\n",
    "        ax2.plot(lh4[1][0], lh4[1][2], lh4[1][1])\n",
    "        \n",
    "        lims = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()\n",
    "        spans = lims[0][1]-lims[0][0], lims[1][1]-lims[1][0], lims[2][1]-lims[2][0]\n",
    "        span = max(spans)\n",
    "        margins = [(span-s)/2 for  s in spans]\n",
    "        ax.set_xlim(lims[0][0]-margins[0], lims[0][1]+margins[0])\n",
    "        ax.set_ylim(lims[1][0]-margins[1], lims[1][1]+margins[1])\n",
    "        ax.set_zlim(lims[2][0]-margins[2], lims[2][1]+margins[2])\n",
    "        \n",
    "        lims2 = ax2.get_xlim(), ax2.get_ylim(), ax2.get_zlim()\n",
    "        spans2 = lims2[0][1]-lims2[0][0], lims2[1][1]-lims2[1][0], lims2[2][1]-lims2[2][0]\n",
    "        span2 = max(spans2)\n",
    "        margins2 = [(span2-s)/2 for  s in spans2]\n",
    "        ax2.set_xlim(lims2[0][0]-margins2[0], lims2[0][1]+margins2[0])\n",
    "        ax2.set_ylim(lims2[1][0]-margins2[1], lims2[1][1]+margins2[1])\n",
    "        ax2.set_zlim(lims2[2][0]-margins2[2], lims2[2][1]+margins2[2])\n",
    "\n",
    "        ax.view_init(elev=-65., azim=-90.)\n",
    "        ax2.view_init(elev=-65., azim=-90.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single frame\n",
    "On the first cell you can select which frames you want to plot and from which video of the batch. On the second you select which frame of the previosly selected you want to plot, specifying its index on the declared \"frames\" list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last batches of training -output, inputs, labels-.\n",
    "vid = 1\n",
    "frames = [100]\n",
    "\n",
    "c_inputs = training_kp[vid].clone()\n",
    "c_labels = training_lbl[vid].clone()\n",
    "c_inputs[:,::2].mul_(training_inpscale[vid, 0])\n",
    "c_inputs[:,1::2].mul_(training_inpscale[vid, 1])\n",
    "c_labels.mul_(training_outscale[vid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(plot_and_rotate(c_inputs, c_output, frames, 0).to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_inputs = tr_inputs[vid].clone()\n",
    "HTML(plot_and_rotate(c_inputs, c_labels, frames, 0).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the same process for the last test batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice of frames\n",
    "Now let's plot a sequence of frames of the selected video. We will plot both the groundtruth and the predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [i for i in range(1,9,2)]\n",
    "video_n = 53\n",
    "\n",
    "c_inputs = tr_inputs.clone()\n",
    "c_output = tr_predictions.clone()\n",
    "c_labels = tr_groundtruth.clone()\n",
    "\n",
    "for vid in range(c_labels.shape[0]): \n",
    "    c_inputs[vid,:,::2].mul_(tr_inp_scale[vid, 0])\n",
    "    c_inputs[vid,:,1::2].mul_(tr_inp_scale[vid, 1])\n",
    "    c_output[vid].mul_(tr_out_scale[vid])\n",
    "    c_labels[vid].mul_(tr_out_scale[vid])\n",
    "    \n",
    "    c_inputs[vid,:,::2].mul_(tr_mx[vid, 1])\n",
    "    c_inputs[vid,:,1::2].mul_(tr_my[vid, 1])\n",
    "    c_output[vid].mul_(tr_mz[vid])\n",
    "    c_labels[vid].mul_(tr_mz[vid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bed4b5ca994b2fa3163ca7cf76becf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8231f5fabaee427faf110055ed81edcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3564373366855293\n",
      "1.1167376777182294\n",
      "0.8419371475522255\n",
      "1.2652974043808112\n"
     ]
    }
   ],
   "source": [
    "plot_frames(c_output, c_labels, c_inputs, video_n, frames, -90, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [i for i in range(1,9,2)]\n",
    "video_n = 31\n",
    "\n",
    "c_inputs = test_inputs.clone()\n",
    "c_output = test_predictions.clone()\n",
    "c_labels = test_groundtruth.clone()\n",
    "\n",
    "for vid in range(c_labels.shape[0]): \n",
    "    c_inputs[vid,:,::2].mul_(test_inp_scale[vid, 0])\n",
    "    c_inputs[vid,:,1::2].mul_(test_inp_scale[vid, 1])\n",
    "    c_output[vid].mul_(test_out_scale[vid,0])\n",
    "    c_labels[vid].mul_(test_out_scale[vid,1])\n",
    "    \n",
    "    c_inputs[vid,:,::2].mul_(test_mx[vid, 1])\n",
    "    c_inputs[:,:,1::2].mul_(test_my[vid, 1])\n",
    "    c_output[vid].mul_(test_mz[vid,0])\n",
    "    c_labels[vid].mul_(test_mz[vid,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40be72f98264481b671d21e82ea0da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b6b9f9c76e4a1cae99ddc5c3a42f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.769381176025303\n",
      "1.9715554427741366\n",
      "1.9300943597914453\n",
      "2.158001724143077\n"
     ]
    }
   ],
   "source": [
    "plot_frames(c_output, c_labels, c_inputs, video_n, frames, -90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [i for i in range(1,9,2)]\n",
    "video_n = 17\n",
    "\n",
    "c_inputs = val_inputs.clone()\n",
    "c_output = val_predictions.clone()\n",
    "c_labels = val_groundtruth.clone()\n",
    "\n",
    "for vid in range(c_labels.shape[0]): \n",
    "    c_inputs[vid,:,::2].mul_(val_inp_scale[vid, 0])\n",
    "    c_inputs[vid,:,1::2].mul_(val_inp_scale[vid, 1])\n",
    "    c_output[vid].mul_(val_out_scale[vid])\n",
    "    c_labels[vid].mul_(val_out_scale[vid])\n",
    "    \n",
    "    c_inputs[vid,:,::2].mul_(val_mom_x[vid, 1])\n",
    "    c_inputs[:,:,1::2].mul_(val_mom_y[vid, 1])\n",
    "    c_output[vid].mul_(val_mom_z[vid])\n",
    "    c_labels[vid].mul_(val_mom_z[vid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3d97c68a8b4afb91c9f19a1565a9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adca4f266a3b4f1e8745c4ae18086975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_frames(c_output, c_labels, c_inputs, video_n, frames, -90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
